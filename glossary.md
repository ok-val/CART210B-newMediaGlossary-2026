# Glossary of keywords related to new media.

The following list of keywords related to new media is constantly updated by the CART210 community throughout the term.

Each student is responsible for choosing and maintaining a set of three keywords. You will research about, define, and exemplify these terms. Your task involves improving the concepts throughout the term. 

For each keyword, cite at least three references. Ensure that at least two of these are from **academic sources**, such as journal articles, books, or conference papers. Citations must follow the Chicago author-year format.

>[!IMPORTANT]
To make your contributions and suggest changes, follow the instructions in the repository's [README](https://github.com/concordia-dcart/CART210-glossary/blob/main/README.md) file.

---
## Affordance

A possibility or set of possibilities for interaction and the effectuation of change given the current state of a system. The visible field of possibility. Some examples include the possible moves a given chess piece can perform given the rules that govern it and the game-state of the board, and the navigation buttons available on a digital interface.

The term also refers to one of Don Norman's principles of design and interaction:

“Affordances: The perceived and actual properties of the thing, primarily those fundamental properties that determine just how the thing could possibly be used.”[^NormanD2013]


[^NormanD2013]: Norman, D. (2013). The Design of Everyday Things: Revised and Expanded Edition. New York, NY: Basic Books.


## Agency 

Agency seems to have two meanings. On one hand, it refers simply to an organized body of people working for or under a larger form of institution, such as a governmental or political institution. On the other hand is a meaning deep rooted in philosophy and anthropology. Laura Ahearn in the Journal of Linguistic Anthropology explains that;

 “what scholars mean by it can differ considerably from common usages of the word. When I did a keyword search in our university library catalogue for agency [...] the system returned 24,728 matches (among these were result about) travel agencies, the central Intelligence Agency, social service agencies [...] few, if any, of these books use agency in the way scholars do: as a way to talk about the human capacity to act.” [^ahearn99anthropology]

This ‘scholarly’ use of the word, as Ahearn puts it, is most complex as it is often associated with conversations discussing the validity of agency, and whether it is something that humans possess. In fact, the study of history is entirely underpinned by the notion of human agency, which is the ability that humans have to intentionally shape our world.[^nash05nature]

The notion of agency in the scholarly sense is especially important in the fields surrounding the study of technology from an anthropological perspective. Human agency, that of the individual using said technology, is often put into question. The ever-changing cultural significance of social media and developing technologies has ever-changing positive and negative effects on the feeling of agency of the individual. 

For example, the self is targeted in many forms of online media. Even character customization, often featured in many online spaces, games and even social media, targets the representation of the self and, in turn, affects the perception of personal agency the user has. It is explained that; “Agency means that the user feels relevant as an actor. Customization allows the individual user to feel unique and distinct. [...] customization is the most seductive aspect of modern online media because it is always related to an aspect of the self.”[^utz08mediated]

As well, agency often is brought up in anthropological discussions surrounding the internet and the massive flood of customized, constant advertising that comes with it. The discussion spills into the wider societal issue of consumerist culture as is explained by Rebekah Willet in Consumer Citizens Online: Structure, agency, and gender in online participation; “I am seeking to explore the relationships between the structures of consumerism (and wider societal discourses) and the agency (the capacity to think and act freely) of the young consumer/producer.”[^willet08consumer]

![Agency](images/agency-axiuk.png)




[^ahearn99anthropology]: Ahearn, Laura M.  1999. “Agency.” *Journal of Linguistic Anthropology* 9, no. 1/2 12–15. http://www.jstor.org/stable/43102414.

[^nash05nature]: Nash, Linda. 2005. *“The Agency of Nature or the Nature of Agency?” Environmental History* 10, no. 1 67–69. http://www.jstor.org/stable/3985846. 

[^utz08mediated]: Utz, Sonja, Martin Tanis, and Susan B. Barnes. 2008. *Mediated interpersonal communication*. Edited by Elly A. Konijn. Vol. 270. New York, NY: Routledge.

[^willet08consumer]: Willett, Rebekah. 2008. *Consumer citizens online: Structure, agency, and gender in online participation*. MacArthur Foundation Digital Media and Learning Initiative.


## AI 

Artificial Intelligence (AI), encompasses a wide range of techniques that allow machines to mimic human cognitive abilities like learning, reasoning, and problem-solving. Coined by Stanford Professor John McCarthy in 1955, he defined AI as “the science and engineering of making intelligent machines”. [^manning] Previous iterations include Alan Turing’s Turing test, conducted in 1950, followed by the first chatbot ELIZA in the 1960s, the IBM deep blue computer chess game in 1977, Apple’s Siri in 2011 and OpenAI in 2015. [^saha]

Andrew C. Staugaard adds to the definition of AI as being the “mechanization, or duplication, of the human thought process”. [^staugaard] The use of Machine Learning algorithms has helped us expand our toolset to include a series of three learning processes; Supervised Learning (using labelled data to predict diversity), Unsupervised Learning (analyzing unlabelled data to learn what is not initially apparent), and Reinforcement Learning (a feedback dependant process which must be reprimanded when predicting incorrect outputs). [^saha] 

Deep Learning is a concept in which the computer simulates human processes to analyze, think and learn, requiring vast amounts of data to train correctly. [^saha] These two subsets of AI use high performance algorithms and multilayer neural networks to solve complex problems. [^ghosh] These can lead to innovations in medicine, technology, and integration into the social sector. 

In the context of new media, we can see these advancements as productive tools for creativity, enabling automation of processes, customization of dynamic workflows, and advanced data analysis. We can draw connections to how algorithms function on social media platforms, performing tasks adapted to individual preferences, such as curating personalized content based on likes, predicting trends and behaviors to engage the viewer in more sophisticated ways. From this point of view, AI becomes an active participant in the process, optimizing interactions to align with individuals in more meaningful ways.

![AI](images/ai_ouellette.jpg)

[^manning]: Manning, Christopher. 2022. _AI Definitions_. Version 1.2. Stanford, CA: Institute for HAI, Stanford University. https://hai.stanford.edu/sites/default/files/2020-09/AI-Definitions-HAI.pdf.

[^saha]: Saha, Dibbyo. n.d. _"A Brief Introduction to Artificial Intelligence: What is AI and How Is It Going to Shape the Future?"_ Ryerson University. 

[^staugaard]: Staugaard, A. C. 1987. _Robotics and AI: An Introduction to Applied Machine Intelligence_. Englewood Cliffs, NJ: Prentice Hall.

[^ghosh]: Ghosh, Moumita & Arunachalam, Thirugnanam. 2021. _Introduction to Artificial Intelligence_. 10.1007/978-981-16-0415-7_2. 

## Analytics



Analytics refers to “the systematic computational analysis of data or statistics.”[^analyticsoxford] In new media, analytics involves the collection, measurement, and interpretation of digital interactions such as user engagement on websites, social media platforms, and streaming services to understand audience behavior and optimize content strategies.[^dong24study]

Digital platforms rely on analytics to personalize content, optimize marketing campaigns, and improve user engagement. By analyzing trends in user behavior, media companies can refine their strategies to increase audience retention and monetization[^dong24study]. Cultural analytics, as conceptualized by Lev Manovich, extends these principles to the study of digital culture, using computational techniques to analyze large-scale patterns in visual and other forms of media.[^manovich17cultural]

Contemporary analytics practices are deeply integrated into digital media technologies. Data analytics tools track user interactions, providing insights that influence content recommendation systems, targeted advertising, and search engine rankings. Platforms such as Google Analytics and social media insights dashboards help businesses and creators measure the impact of their content and refine their strategies accordingly.[^dong24study] In cultural analytics, computational methods allow researchers to examine trends in digital culture, such as the evolution of user-generated content to influence changes in interaction design across social media platforms.[^manovich17cultural]

Analytics profoundly impacts digital culture by shaping how content is created, distributed, and consumed. Algorithm-driven recommendations influence media consumption habits, raising concerns about data privacy, surveillance, and algorithmic bias.[^dong24study] Additionally, cultural analytics offers new ways to study digital culture, allowing researchers to map the evolution of aesthetic and social trends at an unprecedented scale.[^manovich17cultural]

![Analytics](images/analytics-vlamis.png)  
*Image generated by DALL-E 3 with the prompt to ChatGPT 4o 'Generate an image that represents the new media keyword "analytics."'*


[^analyticsoxford]: “Analytics.” n.d. In _New Oxford American Dictionary_. Apple Dictionary. 
[^dong24study]: Dong, Jiaqi. 2024. “A Study on the Application of Data Analytics in New Media Communication.” _Media and Communication Research_ 5 (3). https://doi.org/10.23977/mediacr.2024.050314. 
[^manovich17cultural]: Manovich, Lev. 2017. “Cultural Analytics, Social Computing and Digital Humanities.” In _The Datafied Society: Studying Culture through Data_, edited by Mirko Tobias Schäfer and Karin Van Es, 55–68. Amsterdam University Press. https://doi.org/10.25969/mediarep/12514.

## API

API short for Application Programming Interface is described as an "interface that is defined in terms of a set of functions and procedures and enables a program [...] within an application."[^simon] Simon and Jean explain how using an API allows existing or new business capability from across a network with it being an on-demand service for usage from multitudes of applications and software.

API are varied and have different concepts to it, in the text “A Systematic Review of API Evolution Literature” as an example “when the concept of information was first coined by Parnas in 1972, it was based on interfaces among modules, which today would be called API’s” [^Lamothe]

API is valuable for business due to its ability to connect many people to communicate and share data with each other with ease. The book "API Architecture" explains very clearly how "Now, think about these entities that are run by software. Industrial production processes are controlled by software. Machines, cars and many consumer products contain software.[...] APIs provide a possibility to connect these separate software entities"[^Biehl] and with the use of APIs, it allows it to connect between separate software entities.

An API when it comes to new media is significant as it can allow the user to be able to reach a lot of people across the world when the API is used by other people through the internet. When it comes new media, if an artist wants to reach more people to view and support their work, they will purchase and use an API service to reach other people globally.

![api](images/API_emmabeldick.png)


[^simon]: Simon, Jean Paul. 2021. “APIs, the Glue Under the Hood. Looking for the ‘API Economy.’” Digital Policy Regulation and Governance 23 (5): 489–508. @article https://doi.org/10.1108/dprg-10-2020-0147.
[^Lamothe]:Lamothe, Maxime, Yann-Gaël Guéhéneuc, and Weiyi Shang. 2021. *“A Systematic Review of API Evolution Literature.”* ACM Computing Surveys 54 (8): 1–36.8. @article https://dl.acm.org/doi/pdf/10.1145/3470133
[^Biehl]: Beihl, Matthias. 2015. "_API architecture: The Big Picture for Building APIs_".API-University Series, Volume 2.CreateSpace Independent Publishing Platform. @book https://books.google.ca/books?id=6D64DwAAQBAJ&lpg=PA15&ots=zb5N9tMh6r&dq=%22api%20%22%20&lr&pg=PA15#v=onepage&q=%22api%20%22&f=false



## AR

Augmented Reality (AR) integrates digital content into the real world, engaging multiple senses. Unlike Virtual Reality (VR), which immerses users in a fully computer-generated environment, AR blends virtual elements with real-world settings, enhancing how users perceive and interact with their surroundings.[^Pesce21AR] AR is highly relevant to new media because it transforms digital content into interactive, engaging experiences. By merging physical and digital environments, AR creates new ways for users to connect, explore, and engage with their surroundings in real time.

The fundamental concept of AR lies in the combination of two sources of information—real and digital—merged into a single viewing space. Although AR may seem like a recent technological development, its history dates back to 1892 with Pepper’s Ghost, a stage illusion that is considered a precursor to AR. This technique involved projecting images into the real world, creating the illusion of objects existing in physical space.

In 1962, Ivan Sutherland created the first modern VR system, which projected computer-generated images onto the real world using a head-mounted display.[^Peddie23AR] Over time, AR technology continued to evolve, and with the rise of devices like smartphones and tablets, it became far more accessible. Through apps on these devices, users can now interact with virtual objects or use AR for purposes such as navigation, gaming, and shopping. This marked a significant step in AR’s integration into everyday life.

A key feature of AR is its ability to allow users to see and interact with their physical environment while experiencing digital enhancements. According to the book VR/AR: Foundations and Methods of Extended Realities, Wolfgang Broll suggests that there are three main types of AR: Video See-Through AR, Optical See-Through AR, and Projection-Based AR.[^Doerner22AR] (Although there is no official classification system, this is one of the ways to categorize them.) In Video See-Through AR, the real world is captured by a video camera, and the image is superimposed with virtual content before being displayed on an output device. Optical See-Through AR, on the other hand, allows users to directly view the real world through an optical device, with virtual content overlaid in real-time. Lastly, Projection-Based AR applies virtual content directly onto objects in the real environment using projection mapping.

As AR technology continues to evolve, the possibilities for how it can be applied expand, making it an exciting field to watch. While we cannot predict exactly how AR will develop in the future, one potential direction is the merging of AR with VR to create a hybrid experience known as mixed reality. This convergence may offer even more immersive and interactive experiences, further blurring the lines between the digital and physical worlds.

![AR](images/AR-Tang.png)

[^Pesce21AR]: Pesce, Mark. 2021. Augmented Reality : Unboxing Tech’s next Big Thing. Cambridge, UK: Polity Press.

[^Peddie23AR]: Peddie. Jon. 2023. _Augmented Reality: where we will all live. Second edition. Cham: Springer.

[^Doerner22AR]: Doerner, Ralf, Wolfgang Broll, Paul Grimm, and Bernhard Jung, eds. 2022. Virtual and Augmented Reality (VR/AR): Foundations and Methods of Extended Realities (XR). Cham Switzerland: Springer.

## Artificial Life
When discussing artificial life (ALife), many people often confuse it with artificial intelligence (AI). 
While the two fields are closely related, they are fundamentally distinct areas of study. The study of artificial 
life is a scientific exploration of the fundamentals of life and the construction of “living systems” through hardware
or software. In contrast, artificial intelligence focuses on mimicking intelligent behaviour, such as learning, reasoning, 
and decision-making, to perform tasks that typically require human cognition.[^Gardient19Intro]

The foundation of ALife was first formally introduced in 1987 by computer scientist Christopher Langton at the first workshop
on artificial life in New Mexico. He defined artificial life as “life made by man rather than by nature.” [^Korb09ALife]
The goal of ALife is not merely to mimic biological life but to understand the broader possibilities of lifelike processes 
in any form. Today, artificial life can be classified into three main categories: Hard Artificial Life, Soft Artificial Life,
and Wet Artificial Life. For a quick summary: 

•	Hard Artificial Life refers to artificial life realized as computer hardware 
or machines, such as self-replicating robots or autonomous systems.

•	Soft Artificial Life refers to artificial life realized as computer software, such as simulations 
of evolving digital organisms in virtual environments.

•	Wet Artificial Life refers to artificial life realized biochemically, such as synthetic cells
or DNA-based systems created in laboratories.

While conducting research on artificial life, I have developed concerns about how this technology may impact our lives. We
now live in a fast-paced world where new technologies are consistently being released. New media has become an integral part
of our lives, turning a significant portion of our daily activities into highly connected digital experiences. Artificial life
can easily be linked to domains related to new media, such as virtual reality, avatars, and cybernetics. As a result, we must
remain aware of how this technology may influence our lives, both positively and negatively.

No matter whether it is on a software platform or a hardware platform, artificial life technology is intended to create life,
which will likely entirely change our perception of what life is, based on our current understanding. [^Akama13ALife] For example,
the creation of synthetic organisms or self-evolving digital entities challenges traditional definitions of life and raises
ethical, philosophical, and societal questions. While such powerful technology is emerging, is the current regulatory framework
sufficient to control its development and ensure it does not negatively impact society?

[^Gardient19Intro]: The Gradient. 2019. _Introduction to Artificial Life for People Who Like AI_. https://www.thegradient.pub.

[^Korb09ALife]: Korb, Kevin B., Marcus Randall, and Tim Hendtlass, eds. 2009._Artificial Life: Borrowing from Biology: 4th Australian Conference, ACAL 2009_. Melbourne, Australia. Berlin: Springer.

[^Akama13ALife]: Akama, Seiki. 2013. Artificial Life: How to Create a Life Computationally. Berlin: Springer.

## Avatar
The term ‘avatar’ is not a new concept. According to the Myriam Webster dictionary, it comes from the Sanskrit word referring to a deity's descent or incarnation on earth, and later evolved to refer to any embodiment or personification of an idea, entity or philosophy. [^MiramWebster01Defintion] However, more recently, the first thing that comes to mind when we hear the word avatar is closer to B. Coleman’s definition in her book *Hello Avatar*, where she describes them as “a computer-generated figure controlled by a person via a computer. It is often a graphical representation of a person with which one can interact in real time” .[^Coleman]
	Coleman, however, expands further on her definition, adding that the avatar also "includes the many modes of representation we employ that make up the different roles we play and places we go".[^Coleman]  Jerry Liu explains is his article “From Sci Fi to Commercialization: The Rise of Digital Avatars,” that the idea of an avatar is not just referencing the self, but also “digital influencers” and “digital celebrities”, including examples such as 1990s band *The Gorillaz*, international digital superstar Hatsune Miku and AI created influencers such as Lil Miquella.[^Liu]
	Avatar, then, is not only referring now to the way one chooses to represent oneself within a game world, but is quickly evolving to also encompass entire personalities and independent characters divorced from any real life person. Yet, despite them existing in the digital world, they are not free from human scrutiny and bias. In a real-world experiment, the behaviour of multiple individuals and avatars were observed, the results being that "Participants assigned to more attractive avatars were more intimate with confederates in a self-disclosure and interpersonal distance task than participants assigned to less attractive avatars. Participants assigned to taller avatars behaved more confidently in a negotiation task than participants assigned to shorter avatars."[^PanSteed01]
	As technology grows and evolves within new media spaces, the definition of what is simply an ‘avatar’ and what is an entirely new entity (in the case of AI influencers, for example) become blurred. The question of what is ‘real’ identity within digital spaces will only continue to become more and more complex. 
	
	
[^MiramWebster01Defintion]: “Avatar Definition & Meaning.” Merriam-Webster. Accessed February 1, 2025. https://www.merriam-webster.com/dictionary/avatar.
[^Coleman]: Coleman, Beth. 2011. *Hello Avatar: Rise of the Networked Generation.* Cambridge, MA: MIT Press.
[^Liu]: Lu, Jerry. 2019. “From Sci Fi to Commercialization: The Rise of Digital Avatars.” Medium. https://medium.com/hackernoon/from-sci-fi-to-commercialization-the-rise-of-digital-avatars-ea164495c377.
[^PanSteed01]:Pan, Ye, and Anthony Steed.December 14, 2017. “The Impact of Self-Avatars on Trust and Collaboration in Shared Virtual Environments.” PLOS ONE 12, no. 12. https://doi.org/10.1371/journal.pone.0189078. 

![Virtual environment](images/avatar-julia.png)

## Big Data

"Big data" is a term refering to large datasets. The term was coined in the mid 1990s, and is often attributed to Doug Mashey who was chief scientist at Silicon Graphic, Inc. 

Big Data is often characterized by the "threee V's": Volume, Velocity, and Variety. Volume refering to the large size of the datasets, velocity for the speed that the data is produced and analyzed, and variety refering to the differences of files and types of data. [^gregersen25bigdata]

In terms of New Media, Big Data can be useful in optimal usage of resources while making informed decisions [^allam19aismartcities]. "The analysis of Big Data can be done through Artificial Intelligence; which can be interpreted as the way of training computers to mimic thinking patterns and can even be done to simulate human behaviours". Most modern technology platforms have adapated big data to improve the "quality of life" for their services. 

Google's algorithm stores and predicts data for each one of it's users, such as their gender, age, ethnicity, location, etc. It then uses this data to recommend the user results that it thinks will align with their biases [^grind19googlealgo]. For example, if you were to search for news in a republican state in the USA, it would probably bring up something like Fox News. Here in Canada, the latest search for news brings up CTV, since Fox News is pretty irrelevant for users here. All of this algorithm is trained on big data, so that every user can have their own personalized experience based on machine learned biases. 


[^gregersen25bigdata]: Gregersen, E. "big data." Encyclopedia Britannica, January 15, 2025. https://www.britannica.com/technology/big-data.

[^allam19aismartcities]: Allam, Zaheer. “On Big Data, Artificial Intelligence and Smart Cities.” Cities, January 25, 2019. https://www.sciencedirect.com/science/article/abs/pii/S0264275118315968?via%3Dihub. 

[^grind19googlealgo]: Grind, Kirsten, Sam Schechner, Robert McMillan, and John West. "How Google interferes with its search algorithms and changes your results." The Wall Street Journal 15 (2019). 


## Biometrics 

Biometrics refers to the use of the physical or behaivour characteristics for identifiying 
and verifie individuals, unlike the traditional methods od authentication like passwords or a key, biometric
focus on who you are, this traits such as fingerprints, iris patterns, voice recognition and 
facial features. biometrics began to take shape in the late 20th century. Biometrics plays a 
critical role in security, in high level authentication is necessary.[^Yingzi12Bi]

Biometrics has had a influence on digital culture, particulary in the terms of security, and 
privacy concerns with the use of biometric authentication the convenience of accessing new systems
and services has drasticallly improved. However this also introduces privacy risk, raising 
serious concerns about data security and surveillance.[^Arte08Bi]

such as examples can be: 

Fingerprint Recognition: Used extensively in smartphones (e.g., Apple's Touch ID), laptops, and secure facilities.

Voice Recognition: Common in virtual assistants such as Amazon's Alexa, Apple's Siri, and Google Assistant.

Iris Scanning: Used in high-security environments, such as government agencies or military settings, for identification.[^Centre21BiCa]

Biometrics can implied different challenges since it cannot be accesible to everyone. For 
instance, not all individuals may be able to use biometric systems effectevely due to physical 
disabilities. The collection of the data storage and use can lead legal and ethical concerns. 
Without proper regulation, individuals may be subjected to excessive surveillance.

![biometrics](images/biometrics-Sanchez.png)

[^Yingzi12Bi]:Du Yingzi. 2012. Biometrics: From Fiction to Practice. Singapore: Pan Stanford Pub. http://www.crcnetbase.com/isbn/9789814364133. 
[^Arte08Bi]:ARTE France, Films for the Humanities & Sciences (Firm), and Films Media Group. 2008. Biometrics. New York, N.Y.: Films Media Group. http://digital.films.com/PortalPlaylists.aspx?aid=37189&xtid=39552.
[^Centre21BiCa]: Cyber Centre. (2021, September). Biometrics (ITSP.00.19). Canadian Centre for Cyber Security. https://www.cyber.gc.ca/en/guidance/biometrics-itsap00019 


## Blockchain

Blockchain is known for its role in cryptocurrency systems where it maintains a secure and decentralized record of transactions. 
Blockchain “(a.k.a. distributed ledger technology) enables the secure transfer of money, assets, and information via the Internet 
without the need for a third-party intermediary”. [^swan17philosophy] It enables peer-to-peer transactions without the need 
for intermediaries, ensuring transparency and immutability.“the digital ledger software underlying cryptocurrencies such as bitcoin, 
for the secure transfer of money, assets, and information via the Internet”[^swan17philosophy].

Beyond its role as a digital payment system designed specifically for the Internet, “the underlying blockchain technology is touted 
as a way to store and transact everything from property records to certificates for art and jewelry” [^narayanan16research]. 
This means that blockchain's potential extends far beyond cryptocurrency, offering a decentralized and secure method for verifying ownership
and authenticity across various industries. An example of what Blockchain can used for is as a digital registry “ to record, transfer,
and verify asset ownership (home, auto, stocks, bonds, mortgages, and insurance)” [^swan17philosophy]. It can also preserve the authenticity 
of sensitive documents and records “(such as passports, visas, driver's licenses, birth and death certificates, voter registration, contracts,
wills, patents, and medical records)” [^swan17philosophy].

One of blockchain’s key advantages is that it is “distributed: It runs on computers provided by volunteers around the world, 
so there is no central database to hack” [^tapscott16blockchain]. Additionally, it is “public: Anyone can view it at any time because
it resides on the network” [^tapscott16blockchain], ensuring transparency by allowing anyone to access and verify transactions.
At the same time, it is “encrypted: It uses heavy-duty encryption to maintain security” [^tapscott16blockchain], 
which helps protect data and build trust in the system. By eliminating intermediaries and creating tamper-resistant records,
blockchain technology enhances transparency, reduces fraud, and streamlines transactions in areas such as real estate, 
supply chain management, and digital identity verification.


[^narayanan16research]: Narayanan, Arvind, Andrew Miller, and Song Han. 2016. “Research for Practice: Cryptocurrencies, Blockchains, and Smart Contracts; Hardware for Deep Learning Expert-Curated Guides to the Best of CS Research.” _Queue_ 14 (6): 43–55. 
https://doi.org/10.1145/3028687.3043967. 
 
[^swan17philosophy]: Swan, Melanie, and Primavera de Filippi. 2017. “Toward a Philosophy of Blockchain: A Symposium: Introduction.” _Metaphilosophy_ 48 (5): 603–19. https://doi.org/10.1111/meta.12270.

[^tapscott16blockchain]: Tapscott, Don, Alex Tapscott, and Jenna Pilgrim. 2016. _How Blockchain Will Change Organizations._ Cambridge, Massachusetts: MIT Sloan Management Review. https://learning.oreilly.com/library/view/~/53863MIT58222/?ar?orpq&email=^u. 

## Blog
A blog, short for "weblog," is an online platform where individuals or groups publish content in reverse chronological order.
Blogs combine written entries, multimedia, and hyperlinks, serving as a medium for personal expression, journalism, marketing, and community engagement. 
The term "weblog" was coined by Jorn Barger in 1997 to describe his site, Robot Wisdom, which curated links and commentary.
It was humorously shortened to "blog" by Peter Merholz in 1999, and the term gained widespread adoption.
Initially created by web enthusiasts, blogs now feature contributions from experienced professionals and amateurs sharing insights and experiences. [^rebecca2weblog]

Blogs play a vital role in shaping new media by serving as accessible platforms for information sharing and communication.
They democratize content creation, enabling individuals to bypass traditional media gatekeepers, such as publishers and editors, and share their insights and experiences directly with a global audience.
This open nature fosters a participatory culture, allowing readers to engage with content through comments, shares, and links, creating a dynamic dialogue between authors and their audience.
**Blogs blur the lines between public and private spheres, creating a unique space for personal expression and public discourse**.[^herring5weblog] 
Furthermore, blogs are essential for mass communication in today's fast-paced digital landscape, providing quick access to varied information, from personal opinions to expert analysis.
They have evolved from simple link-sharing platforms to comprehensive tools for self-publishing and community building, highlighting their integral role in the evolution of new media.
In essence, blogs are foundational to the modern information ecosystem, shaping how knowledge is produced, shared, and consumed.[^miller4weblog]

Over the years blogs have evolved into diverse formats, including microblogs (Twitter), vlogs (video blogs), and corporate blogs. They are integrated into content marketing strategies and social media ecosystems.
Blogs paved the way for user-generated content platforms like YouTube and Instagram, which rely on similar participation and community-building principles.
Some of the most notable blogging platforms include [HuffPost](https://www.huffpost.com/) (formerly The Huffington Post), which evolved from a blog into a major news outlet. [Boing Boing](https://boingboing.net/) stands out as an influential technology and culture blog, while [Gizmodo](https://gizmodo.com/) has become a leading source for gadgets and tech news.


![Blog](images/blog-nisma.png)
[^inspo] [^china] [^recipe] [^clic] [^pixel] [^etoys] [^sara] [^web] [^julia]




[^rebecca2weblog]: Blood, Rebecca. 2000. "Weblogs: A History and Perspective". http://www.rebeccablood.net/essays/weblog_history.html.
[^herring5weblog]: Herring, Susan C., Lois Ann Scheidt, Elijah Wright, and Sabrina Bonus. 2005. “Weblogs as a Bridging Genre.” Information Technology & People 18 (2): 142–71. https://doi.org/10.1108/09593840510601513.
[^inspo]: n.d. "inspo". Pinterest. https://www.pinterest.com/pin/185843922118184755.
[^julia]: Julie. 2017. "Vintage Recipes Bread and Rolls." The Old Design Shop. http://olddesignshop.com/2017/11/vintage-recipes-bread-and-rolls/.
[^miller4weblog]: Miller, Carolyn R., and Dawn Shepherd. 2004. _"Blogging as Social Action: A Genre Analysis of the Weblog."_ Into the Blogosphere: Rhetoric, Community, and Culture of Weblogs. https://conservancy.umn.edu/items/6065cb61-5392-4cee-8d2a-11199caeba6e.
[^china]: n.d. "China Daily Design". Pinterest. https://www.pinterest.com/pin/2322237300626423.
[^recipe]: n.d. "Vintage Recipe Cards: Pumpkin Bread". Pinterest. https://www.pinterest.com/pin/374924737748790719.
[^clic]: n.d. "Clic png! ^^". Pinterest. https://www.pinterest.com/pin/904308800178971812/.
[^pixel]: n.d. "Pixel Mouse Icon". Pinterest. https://www.pinterest.com/pin/35325178321625125/.
[^etoys]: KB Holdings, LLC. 2002. "eToys - Where Great Ideas Come to You!." eToys.  https://web.archive.org/web/20020601072752/http://www.etoys.com/etoys/index.html.
[^sara]: Sara. 2019. "TRAVEL GUIDE: From Kyoto to Tokyo." Collage Vintage. http://www.bloglovin.com/blogs/collage-vintage-3891721/travel-guide-from-kyoto-to-tokyo-6751281949.
[^web]: Web Design Museum. n.d. "iQVC in 2001." Web Design Museum. https://www.webdesignmuseum.org/all-websites/iqvc-in-2001.








 

## Brain Interfaces

Brain Interfaces, also often referred to as Brain-Computer Interfaces (BCI), are devices that measure brain activity and translate it into control signals for machines (such as computers) to execute tasks[^gentle2010]. In other words, BCI allow for a brain and a machine to directly communicate [^interface2022]. Using BCI, the execution of a task requires no movement or usage of muscles. 

BCIs first rely on measuring brain activity. This is achieved by using methods that detect the electrical signals generated in specific regions of the brain by networks of neurons communicating with each other when a person performs a given cognitive task.  A non-invasive method of doing this is electroencephalography (EEG), which refers to reading brain activity by using electrodes placed on the scalp [^gentle2010]. Other more invasive methods exist, such as surgically implanted electrodes that are either placed on the surface of the cortex or penetrate brain tissue [^gentle2010]. 

For the machine to be able to ''understand'' what intent the brain activity is trying to convey, the recorded signals need to be processed. It is important to note that different cognitive tasks generate distinct discernable and classifiable brain activity patterns. For instance, sensorimotor rhythms, a distinct type of brain waves, have characteristic features like frequency and amplitude[^gentle2010]. After some filtering out of possible artefacts and noise, the principal features of the signals are extracted. Once the BCI has properly processed and categorized the pattern, it can find the associated command and execute the task. 

Brain Interfaces were first developed to aid individuals facing motor issues[^interface2022]. Tasks like being able to move in a space or communicating with others can become inhibited by various neurodegenerative diseases. For example, Amyotrophic lateral sclerosis (ALS), a disease which affects neurons controlling motor ability, can result in Locked-In syndrome (being physically paralyzed but cognitively sound)[^gentle2010]. BCI allows these people to utilize their brains as an extension of their physical body. They get to interact with the world regardless of their ability. This technology has great potential in the medical field by making communication and movement more accessible.

This technology also has applications in the field of new media art. In fact, one of the earliest applications of BCI was in American composer Alvin Lucier's ''Music for Solo Performer'', where Lucier's alpha brain waves, detected by an EEG headset, activate various percussive instruments[^autodesk2018][^interface2022]. Lucier's BCI was developed by scientist Edmond Dewan; a great example of science and art collaboration! Another example is the performance ''Dual Brains'' by artists Eva Lee and Aaron Trocola, in which the two wear connected EEG headsets and generate sounds and visuals based on their mental states; what is demonstrated is the result of their empathic interactions (hand holding, proximity)[^autodesk2018]. In this sense, Brain-Computer Interfaces exert a dual function. They serve as innovative tools to give shape to the formal characteristics of a piece and they act as a new, expansive conceptual framework that artists can explore by creating pieces about related subjects like accessibility, imagination or embodiment.

![Brain Interfaces](images/brain-interface-prosper.png)
Image taken from these sources[^brain][^waves][^eeg].

[^gentle2010]: Graimann, Bernhard et al. 2010. "Brain-Computer Interfaces: A Gentle Introduction". In *Brain-Computer Interfaces:  Revolutionizing Human-Computer Interaction*, edited by Bernhard Graimann et al. Springer.
[^interface2022]:Dehais, Frédéric and Fabien Lotte. 2022. "Brain-computer interfaces". In *Dictionnaire du numérique*, edited by Marie Cegarra-Cauli et al. ISTE Editions. 
[^autodesk2018]: Dorfman, Peter. 2018. "2 heads -and a Brain-Computer Interface- are making waves in the art world." Autodesk. Last modified May 22, 2018.  https://www.autodesk.com/design-make/articles/brain-computer-interface 
[^brain]: Unknown Author, Unknown year. "2-25161_human-brain-png-svg-free-human-brain.png". PNGKey. Unknown date. https://www.pngkey.com/png/detail/2-25161_human-brain-png-svg-free-human-brain.png
[^waves]: Sirven, Joseph I. and Steven C. Schachter, 2013. "infect_2-s_0.gif". Epilepsy Foundation. August 22nd. https://www.epilepsy.com/sites/default/files/inline-images/infect_2_s_0.gif
[^eeg]: Unknown Author, 2016. "eeg-2.jpg". AdaFruit Blog. August 31st. https://cdn-blog.adafruit.com/uploads/2016/08/eeg-2.jpg 

## Cloud Computing

Cloud computing touches on the delivery of computing services through the internet. These services include storage, databases, networking, servers, software and analytics. Before cloud computing, the only way to access these resources is through physical hardware and computers. With cloud computing, users can enjoy a fast and flexible experience with these services without the need of any physical components. Although cloud computing offers versatility and adaptability, it uses a pay-as-you-go model that lets the user pay a fixed amount depending on their usage of the product. [^mell2011]

Key characteristics of cloud computing includes: 
-	Self-service: Users can use these services without the need of any human interaction.
-	Accessibility: Users can access these services through various devices.
-	Monitored data: Statistics are consistently monitored 
-	Resource allocation: Everything is much more efficient and cost effective since these resources are only activated for you when it is being used. When the user is done with the service, it is pulled away. [^mell2011]

Types of Cloud Computing Models:
-	Infrastructure as a Service (IaaS): Like Amazon Web Services (AWS) EC2, these services give users access to virtual versions of hardware like networks etc. Like renting a computer instead of owning a physical hardware. [^armbrust2010]
-	Software as a Service (SaaS): Services such as Microsoft 365 is part of SaaS. This is a software delivery structure that allows user to interact with applications remotely through their web browser. Mostly on a subscription plan, these services give user the freedom to use their applications without having to download them directly onto to their computers. [^buyya2013]
-	Platform as a Service (PaaS): Like Google App Engine, these services grants users the ability to build, run and manage applications without the having to own their own hardware. [^buyya2013]

Cloud computing plays an important role in new media in the form of on-demand streaming services such as Youtube, Netflix, Disney Plus and many more. Applications like Instagram gives people the opportunity to upload, store and share media effortlessly.

[^buyya2013]: Buyya, Rajkumar, Christian Vecchiola, and S. Thamarai Selvi. 2013. _Mastering Cloud Computing: Foundations and Applications Programming_. Burlington, MA: Morgan Kaufmann. https://books.google.ca/books?hl=en&lr=&id=wqKkqHJhPJQC&oi=fnd&pg=PP1&dq=Mastering+Cloud+Computing:+Foundations+and+Applications+Programming&ots=jmRLGzoW3Y&sig=NikQkmuoseYt66dpUyUzSMrWsBs#v=onepage&q&f=false
[^mell2011]: Mell, Peter, and Timothy Grance. 2011. _The NIST Definition of Cloud Computing. Gaithersburg, MD: National Institute of Standards and Technology_. https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf
[^armbrust2010]: Armbrust, Michael, Armando Fox, Rean Griffith, Anthony D. Joseph, Randy Katz, Andy Konwinski, and Matei Zaharia. 2010. "_A View of Cloud Computing." Communications of the ACM_ 53 (4): 50-58. https://dl.acm.org/doi/fullHtml/10.1145/1721654.1721672

## CMYK
CMYK colorspace stands for "Cyan, Magenta, Yellow and Black(K)" colorspace. This type of colorspace is considered a "substractive" color model, due to colors being displayed via absorbtion (substraction) of the colors of the light. Each main color absorbs a different color of light: cyan absorbig red, magenta absorbing green, and yellow absorbing blue.[^westland23cmyk] It is for this reason that mixing two base colors of the CMYK system will result in one of the primary colors of the RGB colorspace. By removing two of the three colors of the light, you only have one pure color left. For example, by mixing yellow and magenta, we remove both green and blue, leaving us with red. CYMK is mostly used in printing.
In terms of color mixing, the CYMK, due to it's substractive nature, will mix both wavelenghts and keep the lowest points of the clash of the two waves. In comparison, RGB, with its additive mixing, will keep the high points of the waves. [^lee08theevolution]
The use of differnet color spaces will differ depending on how you intend to use the image. for example, a design that is meant to only ever be viewed on screens would preferably be realized in RGB as you will have access to more colors on the monitor. If you intend on printing the design, it is better to use the CMYK model, since not all RGB colors can be produced accurately with printers. It is usually the case for very vivid colors that are produced by a color mix, as achieving vivid colors through mixing is very complex. The type of file to use may also vary on the color space you are using, .pdf and .ai files being prefered for CMYK, and more traditional digital image files such as .jpg or .png are prefered for the RGB colorspace.[^ellis25rgb]


[^westland23cmyk]: Westland, Stephen, and Vien Cheung. 2023. “CMYK Systems.” ResearchGate. In Handbook of Visual Display Technology, 1–7.
[^lee08theevolution]: Lee, Barry B. 2008. “The Evolution of Concepts of Color Vision.” PubMed, July. https://pubmed.ncbi.nlm.nih.gov/21593994.
[^ellis25rgb]: Ellis, Matt, and Peter Vukovic. 2025. “RGB Vs CMYK: What’s the Difference?” Vistaprint Ideas and Advice US (blog). March 5, 2025. Accessed March 9, 2025. https://www.vistaprint.com/hub/correct-file-formats-rgb-and-cmyk?srsltid=AfmBOorEwPM2klKkK5VTpiRTvh5uTgk1bJWGwNT-Lm0Vs_t2zmrb1PGP.

## Codec

A Codec stands for coder/decoder. Its job is to compress and/or decompress files like video or audio.[^microsoft] 
Common codecs we use in our everyday lives are the MPEG series so [MP3](../main/glossary.md#mp3), MP4, as well as Quicktime 
(for Apple users out there).[^internetManagement]Codecs have been developed by multiple different companies and groups 
and they continue to be looked into to create more efficient ways of compressing large files.[^briefHistory] 
Codecs facilitate sharing of larger files as the compressed version is much easier to upload and download.

The invention of codecs influenced the web and the way we can share video and audio through the
Internet. Downloading an uncompressed video can take lots of time and energy so with codecs in 
the picture, large video files are able to be shared with less time spent on downloading. It 
also serves to make the Internet more accessible as people with bad connections are still able 
to access the videos. While an uncompressed video file might take hours to download, with codecs, 
that time becomes much shorter. [^internetManagement]

Since codecs improved accessibility for video and audio files, designers are more likely to 
choose those formats for their websites and other works. For example, rather than having a long 
list of instructions to explain how to build or fix a certain product, a video might be more 
efficient and provide much more precise information than words can.[^internetManagement]
Additionally, codecs make 
video and audio more shareable which encourages artists to delve into that medium and share 
their works online. 

[^microsoft]:Microsoft. “Codecs FAQ.” Accessed March 16 2025. https://support.microsoft.com/en-us/windows/codecs-faq-392483a0-b9ac-27c7-0f61-5a7f18d408af
[^briefHistory]:Jacobs, Marco and Probell, Jonah. 2007. *A Brief History of Video Coding.* ARC International. https://www.researchgate.net/profile/Jonah-Probell/publication/228745838_A_Brief_History_of_Video_Coding/links/58c6c2d7aca27237ccba570b/A-Brief-History-of-Video-Coding.pdf
[^internetManagement]:Keyes, Jessica. 1999. Internet Management. Auerbach Publications.  https://doi-org.lib-ezproxy.concordia.ca/10.1201/b12445 




## Commons 

Commons is a general term that refers to a resource shared by a group of people. These resources can range from small, localized systems (e.g., a family refrigerator) to community-level resources (e.g., sidewalks, playgrounds, libraries) and extend to global or transboundary systems (e.g., the deep seas, the atmosphere, the Internet, and scientific knowledge). The commons can be well-bounded (e.g., a community park or library), transboundary (e.g., the Danube River, migrating wildlife, the Internet), or without clear boundaries (e.g., knowledge, the ozone layer). [^hessOstrom]

The salient characteristic of commons, as opposed to property, is that no single person has exclusive control over the use and disposition of any particular resource in the commons. Instead, resources governed by commons may be used or disposed of by anyone among some (more or less well-defined) number of persons, under rules that may range from “anything goes” to quite crisply articulated formal rules that are effectively enforced. [^benkler]

The term “commons” originates from the English legal term for common land, historically used to describe shared agricultural or grazing lands. The modern conceptualization of the commons was popularized by ecologist Garrett Hardin in his 1968 article, [*The Tragedy of the Commons*](https://doi.org/10.1126/science.162.3859.1243). [^hardin] Hardin’s work highlighted the challenges of managing shared resources, sparking widespread academic and practical interest in the topic. Prior to Hardin’s article, references to “the commons,” “common pool resources,” or “common property” were rare in academic literature. [^laerhovenOstrom]

If we link this concept of the commons to new media, we can notice how the commons is now a foundational element to the philosophy of open-source software, crowdsourced projects, and decentralized technologies. All of these systems are based on emphasizing shared resources and collective governance to contribute to community resilience and prefigure better futures. [^smith] [Wikis](#wiki) are one of the most popular types of digital commons. An example of this would be [Wikipedia](https://www.wikipedia.org/), which is a digital encyclopedia that relies on a collective of editors to modify or add information. [OpenStreetMap](https://www.openstreetmap.org/) is another similar concept that provides crowdsourced geographic data. 

[^hessOstrom]: Hess, Charlotte, and Elinor Ostrom. 2007. *Understanding Knowledge as a Commons: From Theory to Practice.* MIT Press. 
[^benkler]: Benkler, Yochai. 2006. *The Wealth of Networks: How Social Production Transforms Markets and Freedom*. Yale University Press. 
[^hardin]: Hardin, Garrett. 1968. “The Tragedy of the Commons.” *Science* 162 (3859): 1243–1248. https://doi.org/10.1126/science.162.3859.1243.
[^laerhovenOstrom]: Laerhoven, Frank Van, and Elinor Ostrom. 2007. “Traditions and Trends in the Study of the Commons.” *International Journal of the Commons* 1 (1): 3–28. https://doi.org/10.18352/ijc.76.
[^smith]: Smith, E.T.  2024. "Practising Commoning." *The Commons Social Change Library*. Accessed January 29, 2025. https://commonslibrary.org/practising-commoning/#Introducing_%E2%80%98Commoning

## Compression 

Compression is a word used in many domains and can be summed up as “the act of pressing something into a smaller space or putting pressure on it from different sides until it gets smaller”. [^CambridgeDictionary] 
In the new media space, it is more commonly known as data compression which can be defined as “the process of converting a source data into a smaller sized version of it”. [^Salomon02]
While there are many ways to compress data, they can be categorized as either [lossy](../main/glossary.md#lossy) or [lossless](../main/glossary.md#lossless). 

While it is impossible to give credit to a specific individual, 
an early example of compression can be attributed to Samuel Morse and the Morse code when he decided to “assign shorter codes for the most common letters in the English alphabet, 
such as ‘A' and ‘E’, to help transmit messages quickly”. [^Sayood02]

Compression is important in media spaces as it helps to solve two major problems,
the first being the need for storage space and the second is to allow users to share and view content rapidly. [^Salomon02]

Data compression is becoming more and more relevant in the tech industry as the rise of [Artificial-Intelligence](../main/glossary.md#ai)  services requires faster and more efficient ways to be able to transmit images and data. [^Tsukuba24] 
As such, new algorithms are being developed and published like the “Universal Adaptive Stream-Based Entropy Coding” system to aid in this issue [^Shinichi24].

Compression is also present in current-day media through short-form content such as TikTok and Instagram Reels as well as TV and movie scenes length decreasing over time. 
This includes content on YouTube containing jump cuts and the removal of silent pauses during speech. 
This type of compression could be linked to the lowered attention span that our society is currently facing with the increase in the amount of content available 
due to having more space or from the short content duration format. [^Mills23]



[^CambridgeDictionary]: Compression | English meaning - Cambridge dictionary, accessed February 2. 2025. https://dictionary.cambridge.org/dictionary/english/compression.

[^Salomon02]: Salomon, David. 2002. "Data compression." *Springer US: 2*. https://vyomaonline.com/studymaterial/uploads/pdf/2020/12/06_ed4b65ab7f178238646aa579cd516806.pdf

[^Sayood02]: Sayood, Khalid. 2002. "Data Compression." *Encyclopedia of Information Systems 1 (2002)*: 423-444. https://www.csd.uoc.gr/~hy438/lectures/Sayood-DataCompression.pdf

[^Tsukuba24]: University of Tsukuba. 2024. “New Tech Boosts Real-Time Data Compression for AI.” *Techxplore*. https://techxplore.com/news/2024-08-tech-boosts-real-compression-ai.html.

[^Shinichi24]: Yamagiwa, Shinichi and T. Kato. 2024. "Universal Adaptive Stream-Based Entropy Coding." in *IEEE Access. vol 12*. https://ieeexplore.ieee.org/document/10599517

[^Mills23]: Mills, Kim. 2023. “Why Our Attention Spans Are Shrinking, with Gloria Mark, Phd.” *American Psychological Association*. https://www.apa.org/news/podcasts/speaking-of-psychology/attention-spans. 

![Compression](images/compression-le.png)

## Computer Vision

Computer vision (CV) is a subfield of AI research concerned with equipping computers with the ability to mimick human vision  (and, by extension, human visual processing capabilities). There exists many different types of visual data, such as live camera feeds, still images or videos, that can be used as inputs for CV. These inputs are processed by using various models and enable a computer to analyze and recognize what exactly is going on in an image or a sequence of images.

Implementations of Computer Vision are primarily based on deep learning, a collection of techniques which allow computational models (like neural networks) to learn from and understand the patterns that exist within visual data[^comp]. As a general rule, large amounts of data (identified using labels) must be fed to these models[^comp][^ibm]. 

One of the most widely used deep learning techniques used in CV is Convolutional Neural Networks (CNN). Inspired by models of the human visual system, CNNs consists of three types of neural layers: convolutional layers, pooling layers and fully connected layers[^comp]. Convolutional layers use various small matrices called kernels to perform convolutions on the input data and generate feature maps. These feature maps can be described as 2D mappings of the different features an image might possess. Different kernels allow for the extraction of different features. Pooling layers are characterized by their ability to reduce the dimensions of the input data (also called downsampling) for subsequent layers: doing so reduces computing overhead but results in information loss[^comp]. Fully Connected layers have neurons that are connected to every other activation in the previous layer and perform the high level reasoning in the network[^comp]. 

A CNN begins by discerning simple shapes and edges before iterating and recognizing more complex elements[^ibm]. Enventually, utilizing these models, CV systems can perform multiple tasks, such as object detection (or semantic segmentation) [^ibm][^comp], facial recognition[^comp], or object tracking [^ibm].
Real-world applications of these basic capabilities include the development of self-driving cars or translation by pointing a camera at a piece of written text [^ibm]. 

In the field of contemporary art and media studies, Computer Vision can provide a new language, based in mathematical/numerical features, with which to analyze cultural artifacts[^mano]. For example, every minute hue shift in a video could be translated into a collection of very precise datapoints which could then be processed and stored. Similarly, CV allows us to adequately analyze the massive amounts of visual information created and shared everyday on social networks and the Internet in general[^mano].  In other words, Computer Vision can allow us to apprehend and comprehend our own rapidly evolving, ever-expanding visual cultures. 

However, some new media artists question Computer Vision and its ties with our modern technological societies' propensities for ubiquitous surveillance. In the early 2010s, artist and researcher Adam Harvey worked on CV Dazzle, an art project which sought camouflage human faces from computers by using design and fashion[^harv]. Harvey used bold and unconventional combinations of makeup, accessories and hair styling to break apart the facial features used in CV to recognize faces[^harv]. While these patterns do not work on current, more advanced Computer Vision systems, CV dazzle gives us insight into how we might go about creatively resisting the normalization of facial recognition.  


[^harv]: Harvey, Adam. n.d. "CV Dazzle". Accessed March 15, 2025. https://adam.harvey.studio/cvdazzle/ 
[^ibm]: IBM. n.d. "What is Computer Vision." Accessed March 15, 2025. https://www.ibm.com/think/topics/computer-vision#:~:text=Computer%20vision%20is%20a%20field,they%20see%20defects%20or%20issues. 
[^mano]: Manovich, Lev. 2021. "Computer vision, human senses and language of art." *AI & Society* 36, 1145-1152. https://doi.org/10.1007/s00146-020-01094-9
[^comp]: Voulodimos, Athanasios, Nikolaos Doulamis, Anastasios Doulamis and Eftychios Protopapadakis. 2018. "Deep Learning for Computer Vision: A Brief Review." *Computational Intelligence and Neuroscience* 2018, no.1: 13 pages. [https://doi.org/10.1155/2018/7068349](https://doi.org/10.1155/2018/7068349) 



## Computer

"Computer" is first and foremost a word that gets its origins from the Latin word "Computare", roughly translated as "to calculate", or perhaps "someone who computes". 
The word started being widely recognized sometime around the 1600's, and was used comfortably to describe human mathematicians of that era. [^rojas_hashagen_2002_first]

It was also around this time that different tools that could help with computing would start to appear, for example, William Oughtred's slide rule, a mechanical analogue device primarily used for multiplication, division, roots, logarithms, and trigonometry.
This word, however, would see a paradigm shift of its use around the turn of the 17th - 18th century, as computation moved towards mechanics and automation. 
In 1822, Charles Babbage completed one of his inventions, announcing it as the "Difference Engine", 	designed to compute and print mathematical tables via automated mechanical calculations. [^garfinkel_grunspan_2019_abacus]

This invention could be described as the precursor to the idea of the "Analytical Engine", largely known as the first mechanical computer ever made, though Babbage would never complete or build the machine as he envisioned it, passing in 1871.
The 19th century brought a series of groundbreaking innovations in the world of computation, paving the way for more modern computers. 
An important development came in the late 19th century with Herman Hollerith’s punched card machine.
The punched card machine used cards to automate the process of tabulating census data, which was a great leap in data management.
The machine functioned by reading the punched holes in the cards using electrical sensors, processing the data through internal mechanics, and finally outputting the results on a printout. [^pugh_heide_2013_card]

Opening the doors to greater computation power, the late 19th century also brought with it great advancements to our understanding of what we know now to be digital computing technology.
In 1936, Alan Turing created the "Turing Machine", a hypothetical computational model that was meant to answer the question of whether certain mathematical tasks could be computed.
His machine consisted of a long strip of tape with separated symbols that were meant to store data, as well as a "head" that would move both left and right down the tape, being able to read, write, and make decisions in accordance to the tape.
Today the child of this idea is known as binary and widely used as part of the the basis of all of our programming knowledge. [^mol_2024_turing]

In a similar, if more physical sense, John Von Neumann created the "Von Neumann Architecture" in 1945. 
This was the model for a computer's organization that now forms the basis of most modern computers, acting as our introductions to the concept of CPUs (Central Processing Unit), RAM (Random-Access Memory), Input / Output Devices, and of course storing commands using memory. [^o'regan_2018_neumann]

### Modern Definition

As the idea of a computer became more widely known, understood, and of course used, it became engrained in nearly every bit of our culture.
The term "computer" is incredibly significant to new media because it has been cemented as a foundational concept to the way we interact with, and produce media today.
This term has never been just about the device itself but is instead a vehicle for technologies that create and transform data into interactive experiences.

For a grand majority of modern people, their media, whether focused on education or entertainment, is the shared data we find on the internet.
In this case, computers act as our access to said data, allowing us to take, make, and change its form.
While a simple instrument would allow us to access specific, or perhaps on some occasions numerous, forms of art, the computer allows us to access and transform a whole new world of information in an unimaginable number of ways. [^kay_1991_computers]

Graphic Design, Video Editing, Web Development, Social Entertainment Environments, and finally in great deal, AI, all have computers to thank for their existence and continued development.
Our ability to govern, communicate, and collaborate has increased significantly with this rise in technological understanding, and the ability to "compute" is no doubt what led us to that understanding.

Paradigmatic examples of the use of computers that we often take for granted are Social Media Platforms, Streaming Services, and Video Games.
In conclusion, the computer is not only central to the production of new media but also to its constantly ongoing distribution.

![Computer](images/computer-verba.png)[^computer1][^computer2][^computer3]

[^rojas_hashagen_2002_first]: Rojas, Raul, and Ulf Hashagen. 2002. _Classifications of Computing Machines. In The First Computers: History and Architectures_, pp. 1-2. MA: MIT Press.
[^garfinkel_grunspan_2019_abacus]: Garfinkel, Simson L., and Rachel H. Grunspan. 2019. _The Computer Book: From the Abacus to Artificial Intelligence, 250 Milestones in the History of Computer Science_, Union Square & Co.
[^pugh_heide_2013_card]: Pugh, Emerson W., and Lars Heide. 2013. _Early punched card equipment: 1880-1951 [Scanning Our Past]_, pp. 546-552. IEEE.
[^mol_2024_turing]: Mol, Liesbeth D., Edward N. Zalta, and Uri Nodelman. 2024. _The Stanford Encyclopedia of Philosophy (Winter 2024 Edition)_, Metaphysics Research Lab, Stanford University.
[^o'regan_2018_neumann]: O'Regan, Gerard. 2018. _The Innovation in Computing Companion: A Compendium of Select, Pivotal Inventions_, Springer Nature Switzerland.
[^kay_1991_computers]: Kay, Alan C. 1991. _Communications, Computers and Networks: How to Work, Play and Thrive in Cyberspace_, pp. 138-149. Scientific American.
[^computer1]: Triponez, Nao. 2018. _Shallow Focus Photography of Macbook_.
[^computer2]: Hasanbekava, Dziana. 2021. _Serious man writing in notebook at table during work_.
[^computer3]: Unknown. 2021. _Calculator on Black Surface_. Kindel Media.

## Content Management System (CMS)
A content management system (CMS) is software that enables users to create, manage, store, and modify digital content efficiently, often through a user-friendly interface. Typically, a CMS provides an all-in-one solution for creating, managing, storing, and publishing digital content, such as text, images, videos, and websites.[^IBMCMS2024] A CMS consists of two core components: the content management application (CMA), where content creators input, manage, and edit digital materials without needing technical expertise, and the content delivery application (CDA), which manages storage, retrieves content, and presents it to end-users. 

Content management as a practice predates the web, originally emerging from needs within the technical documentation and scientific publishing communities, who required systematic ways to reuse and manage information effectively.[^BoikoBible] Modern content management systems became particularly prominent with the rise of corporate web publishing, allowing businesses to automate and simplify web content updates. These systems streamline workflows, improve efficiency, and enhance collaboration, enabling various organizational roles to focus on their strengths rather than managing complex code and databases.[^BoikoBible]

CMS software typically incorporates several key features such as publishing controls, editing tools, built-in SEO capabilities, security measures, and consistent branding tools, making it accessible for organizations of all sizes, from small businesses to large enterprises.[^IBMCMS2024] Effective content management requires understanding not only the technology involved but also the specific information requirements, workflows, and audiences that the organization aims to reach.[^McKeeverCMS] This comprehensive approach positions CMS as not merely a technological tool but as an essential part of organizational strategy and communication.

[^BoikoBible]: Boiko, Bob. 2005. _Content Management Bible_. 2nd ed. John Wiley & Sons. 
[^IBMCMS2024]: Finn, Teaganne, and Amanda Downie. 2024. “What is a content management system (CMS)?” _IBM_ (blog). March 7, 2024. https://www.ibm.com/think/topics/content-management-system. 
[^McKeeverCMS]: McKeever, Susan. 2003. “Understanding Web Content Management Systems: Evolution, Lifecycle and Market.” _Industrial Management & Data Systems_ 103 (9): 686–92. https://doi.org/10.1108/02635570310506106.

## Copyleft

Copyleft is a method of data/information publication that is largely concerned with free use and access. You may have heard the term "copyright" before,
which is a similar method of publication that reserves all rights of use for a publication to the original creator. The term for "copyleft" was coined by Li-Chen Wang[^WIKI]
in his publication of his Palo Alto TINY BASIC language for the intel 8080 in the Dr Dobb's Journal (a popular computing and engineering magazine) in May of 1976, where
for his publication, he spoofed the common "Copyright, all rights reserved" by putting "COPYLEFT ALL WRONGS RESERVED" in his publication. The term was later expanded
upon and used by Richard Stallman in the creation of GNU[^GNU]. He believed that making copyleft the term for publications in the public domain or open source was a good way 
to fight against the use of copyright methods in programming/software and to help programmers and creators take back what is theirs and make it useful for the world. 

The concept of free use and the public domain has become more and more popular as of late with the creation and distribution of open-source programs, publications and projects, 
such as GitHub itself. Additionally, leaving the intent of the original user present while also allowing other artists, creators and users to use and interpret services is an idea
largely present in new media theory. When the idea of copyright as it pertained to the digital age came into question, many factors had to be considered. Many believe that
the claiming of publications is important to maintaining and the identity of the creator (preserving intellectual property), while some believe that in order to create a world 
in which people feel free to create what they wish, they should have access to any resource at their disposal. Stallman's GNU Manifesto thinks of copyleft within the terms of 
open-source and proprietary software[^SCIDIR], and notes that such developers can use their copyright over the software that they create to claim copyright over publications 
and projects made in or using that software. In the example of GNU, (an open source operating system similar to Linux), the copyleft nature of the system allows users to use software 
to better suit their needs, rather than being confined by the restrictions of copyrighted tools

In essence, copyleft represents the true freedom of the creator as a manifestation of how they use, share and expand their work. Copyright methods have been put in place in order to
circumvent creativity by setting limitations and boundaries for those that use and peruse the publications they are made with. It is important to note that while copyleft is a
method of publication used to combat the ideas behind copyright, it still legally must be published under a copyright notice. The rights are still reserved to the author/creator,
but the method in which those rights can be used and distributed is then where the copyleft methodology takes place. So unfortunately, the circled backwards C that represents a copyleft
publication is not legally viable, and even the GNU website is copyrighted under the Free Software Group.



![Copyleft](/images/copyleft.png)

[^WIKI]: Various Authors. “Copyleft.” Wikipedia, January 14, 2025. https://en.wikipedia.org/wiki/Copyleft. 
[^GNU]: Stallman, Richard. What Is Copyleft? - GNU Project - Free Software Foundation.” *GNU Operating System*, 1996. https://www.gnu.org/licenses/copyleft.en.html. 
[^SCIDIR]: Benkler, Y., W.M. Cohen, N. Harabi, A.W. Branscomb, P. Dasgupta, P.B. De Laat, D.G. Johnson, and L. Lessig. “Copyright or Copyleft?: An Analysis of Property Regimes for Software 
Development.” *Research Policy - Science Direct*, September 22, 2005. https://www.sciencedirect.com/science/article/pii/S0048733305001484. 

## Copyright 

Copyright is a system of law that allows creators to claim ownership of their ideas and creations. Copyright has always existed mostly as a form of dynamic law,
in order to enforce creative freedom within the confines of free use and distribution. Globally, copyright laws were created as early as the 16th century, only seeking to evolve
as societies moved into industrialization and from industrialization to data and service-driven systems. In the legal world of copyright, the Copyright Act of 1909 in the US
coined the idea of the "right of first publication,"[^CASE] which gave all published works copyright to their original publishers for an unlimited time, which less for legal purposes
and more for intellectual property rights-related reasons. However, having your work *registered* under a copyright takes it out of the public domain and it remains so 
until a significant amount of time after you die[^UPENN]. 

Copyright is an idea that acts in direct opposition to a "free-use" information model that a lot of creatives value for things such as collaboration, inspiration and innovation. It's
also impossible to discuss copyright as a concept without also acknowledging the damage copyright has done in our present-day ultracapitalist society, which commodifies creativity 
and profits off of maintaining ideas that are inherently harmful to the ability of creatives to flourish. Because copyright naturally opposes free-use and open-source collaboration models 
(which are healthy and beneficial methods of creating and distributing work), copyright when used in the modern age can actively damage an artist's ability to create what they wish.
Because creative services and tools almost never are actually owned by the artist[^GNU], they are complicit in the work that they are used to create, and this can be exploited to claim any work
made with them under the copyright purview of that service's distributors. This also goes for corporate artists and designers that work under a larger company, which means that none of their
work can be attributed directly to them or be shared/expanded upon without conflicting with the company's "property". 

Copyright ironically started as a way for creators to take back what was theirs and make sure it isnt commodified and distributed unwillingly. Unfortunately, the concept has been twisted
and exploited to create a reality where it serves the opposite purpose, neccessitating the need for creatives to go the extra mile to make sure their art is ACTUALLY protected while still being
shared effectively and in ways that are beneficial to like-minded creators. The idea that a tool meant to make life easier or more personal for people that ends up being a figurative bullet in the
foot, is a common trend amongst innovations, and could continue to happen with new tools and methods, such as LLM's and AI tools. How these tools will follow this trend is unclear to us at the 
moment, but there's no doubt that due to their volatile nature, they should be treated with caution and regulated in the same way copyright has been. 

![Copyright](/images/copyright.jpg)

[^UPENN]: Gorman, Robert A. “Essay: An Overview of the Copyright Act of 1976.” Essay. In *University of Pennsylvania Law Review* 126, 126:856–86. LibGuide, 1978. https://heinonline.org/HOL/Page?collection=journals&handle=hein.journals/pnlr126&id=868&men_tab=srchresults. 
[^CASE]: Joyce, Craig. “Chapter 1: The Landscape of Copyright (Introduction).” Essay. In *Copyright Law: Sixth Edition*, 1–26. Lexis Nexis, 2003. https://case.edu/affil/sce/authorship/Joyce-part1.pdf. 
[^GNU]: Stallman, Richard. "The GNU Manifesto - GNU Project - Free Software Foundation.” *GNU Operating System*, 1996. https://www.gnu.org/gnu/manifesto.en.html.

## Creative Commons

  Creative commons was crafted by a law professor at Stanford University named Lawrence Lessig and others which aren't mentioned by name in December
  of 2002 which provides a set of copyright licenses which is free for public use. A website was created called Creative Commons (creativecommons.org) where a creator who wants to publish their work can select what license they want to use
  and she can chose to retain her copyright of her work but allow other people to use her work without permission and without payment as long as they credit her for the original work. [^Kim]

  A study was created to examine whether Creative Commons can resolve the conflict surrounding copyright laws in the digital era.
  there's further explanation of the study as "that framework posits that there are two competing visions of the fundamentals of copyright law: a "private property" vision and a "public property vision.""[^Kim] When it comes to private property vision
  is the belief that artist should have full ownership over their work and that its the private interest of authors who created original works deserve to have property rights over their work. As for public property vision is that "those who note that copyright has historically developed as society's grant of a limited 
  monopoly and who think that the rights of authors must be weighted against the freedom of everyone else to use the copyrighted work."[^Kim] The role of copyright as a matter of public policy that aims to achieve a balance between private interests and public interests.

  *What is Creative Commons?*
  
  Creative Commons is a "suite of copyright-based licenses defining terms for the distribution and re-use of creative works." [^Hagedorn,Mietchen,Morris,Agosti,Penev,Berendsohn,Hobern] Creative Commons provides licenses for different use cases and includes open content license such as the Attribution License (e.g. CC BY, used by many open access scientific publishers), and the Attribution Share Alike license(CC BY-SA which is used by Wikipedia). 
  The license suite also contains non-free and non-open lcienses like those containing a "non-commercial" which in short is referred to as NC condition. When providing background information from studies created by professors, CC licenses in general, and how there are advantages, disadvantages.

  *Creative Commons and New Media*

  When it comes to Creative Commons in New Media, institues have adopted Creative Commons Licenses for their content. For example, nphoto.net and bababian.com are both presenting their web pages with photographic works licensed under a local Creative Commons licensed and many more individual photographers are choosing CC as their licensing regimen of their choice. 
"Creative Commons licenses are commonly ised in new media and the internet. The internet media, for example social networking sites, user geenrated sites {...} all have distinct features such as openess, sharing, re-usability and unification of interests of authors and users."[^Wang]
And emphasizes that a participative culture or a 'read-write culture', as dinstiguished from the traditional 'read-only culture'.
Creative Commons has become an effective alternative to copyright. One of the impportant aims of CC is "to build a free and extensive infrastructure at the content layer that enables freedoms that many different Web 2.0 creative projects require."[^Wang]

![creative commons](images/creative-commons_emmabeldick.png)
Prompt used on ChatGPT: create image for creative commons, concept visual, have it more towards new media.  


[^Kim]: Minjeong Kim.2007. *The Creative Commons and copyright protection in the digital era: Uses of Creative Commons licenses*.Journal of computer-mediated communication:187-209.https://academic.oup.com/jcmc/article-abstract/13/1/187/4583060
[^Hagedorn,Mietchen,Morris,Agosti,Penev,Berendsohn,Hobern]: Gregor Hagedorn, Daniel Mietchen, Robert A. Morris, Donat Agosti, Lyubomir Penev, Walter G. Berendsohn, Donald Hober.2011. *Creative Commons licenses and the non-commercial condition: Implications for the re-use of biodiversity information*.Zookeys. https://pmc.ncbi.nlm.nih.gov/articles/PMC3234435/
[^Wang]: Chunyan Wang.2008.*Creative commons license: an alternative solution to copyright in the new media arena*. Syndney University Press https://core.ac.uk/download/pdf/41231144.pdf

## Crowdsourcing

Crowdsourcing can be defined by the process of collecting work, data, and opinions from a big, often anonymous group, typically through the Internet. [^hargrave24crowdsourcing] It diverges from traditional outsourcing because the task is assigned to an undefined crowd instead of a pre-selected person or group. [^lebraty13crowdsourcing] 

### History
One of the first uses of crowdsourcing was used in 1714. After losing 5 navy vessels and many lives, the British government offered a prize of £20,000 (£3 million today) if someone could find a way to accurately measure longitude while at sea. John Harrison created the H1 Marine Timekeeper to solve that problem.[^lebraty13crowdsourcing]

Crowdsourcing was used many times after that all over the world but the term was finally coined by Jeff Howe in 2006 while he was presenting an article about how the internet was being used to outsource jobs to the general population or “crowd”. The article was titled “The Rise of Crowdsourcing” and was published in June 2006 by Wired.[^howe06rise]

Modern digital crowdsourcing was pioneered by Luis von Ahn, the creator of ReCAPTCHA and Duolingo, which leverage human input for AI training and translation.[^halton24wisdom] Today, crowdsourcing is used in so many different ways and is a fundamental part of many industries and will continue to play an important role in our world as it evolves further.

### Significance
Since it uses cooperation and collective intelligence, crowdsourcing is essential to new media. It allows for large-scale collaborations to be possible, enabling users to provide resources, labor or knowledge without centralized control. It is used by digital platforms for previously untenable data collection, innovation, and problem solving. [^halton24wisdom]

However, many ethical concerns have been raised, especially in relation to power imbalances and labor exploitation. According to Du et al. (2024)[^du24ethical] many online workers perform microtasks with minimal protection, low wages and limited worker rights. Despite its issues, crowdsourcing continues to shape digital culture by fostering decentralization and user-driven innovation. 

### Relation to Current Technologies and Media Practices 
Crowdsourcing is widely used across digital media and technological landscapes:
- **Scientific Research & Problem-Solving:** Foldit enables gamers to assist scientists in protein folding research, accelerating discoveries that traditional methods struggle with. [^lebraty13crowdsourcing]
- **AI & Data Labeling:** Services like ReCAPTCHA use human input to improve machine learning models by digitizing text and training algorithms.[^halton24wisdom]
- **Content Creation & Curation:** Wikipedia relies on crowdsourced knowledge, while platforms like Fiverr and 99designs offer freelance creative work.[^lebraty13crowdsourcing]
- **Business & Innovation:** Companies like LEGO leverage user-generated ideas for new product designs, and crowdfunding platforms such as Kickstarter enable community-driven funding for creative projects[^halton24wisdom]


![Crowdsourcing](images/crowdsourcing-lapierrefurtado.png)

[^hargrave24crowdsourcing]: Hargrave, Marshall. 2024. “Crowdsourcing: Definition, How It Works, Types, and Examples.” Investopedia. October 17, 2024. https://www.investopedia.com/terms/c/crowdsourcing.asp.
[^lebraty13crowdsourcing]: Lebraty, Jean-Fabrice, and Katia Lobre-Lebraty. 2013. _Crowdsourcing : One Step Beyond._ Hoboken, NJ: Wiley. http://site.ebrary.com/id/10756811. 
[^halton24wisdom]: Halton, Clay. 2024. “Wisdom of Crowds: Definition, Theory, and Examples.” Investopedia. August 1, 2024. https://www.investopedia.com/terms/w/wisdom-crowds.asp#:~:text=Wisdom%20of%20crowds%20refers%20to,collectively%20smarter%20than%20individual%20experts. 
[^du24ethical]: Du, Shuili, Mayowa T. Babalola, Premilla D’Cruz, Edina Dóci, Lucia Garcia-Lorenzo, Louise Hassan, Gazi Islam, Alexander Newman, Ernesto Noronha, and Suzanne van Gils. 2024. “The Ethical, Societal, and Global Implications of Crowdsourcing Research.” Journal of Business Ethics 193 (1): 1–16. https://doi.org/10.1007/s10551-023-05604-9.
[^howe06rise]: Howe, Jeff. 2006. “The Rise of Crowdsourcing.” Wired, June 1, 2006. https://www.wired.com/2006/06/crowds/.

## Cryptocurrency

Cryptocurrency is a digital or virtual form of currency that utilizes cryptographic techniques to secure transactions and control the creation of new units.
"Cryptocurrency in its purest form is a peer-to-peer version of electronic cash. It allows online payments to be sent directly from one party to another without going
through a financial institution.” [^bolt17bitcoin]
Therefore this currency is often based on blockchain technology, which is a distributed ledger enforced by a network of computers. 
Unlike traditional fiat currencies issued by governments, cryptocurrencies operate on decentralized networks. In the concept of computing, decentralized networks mean
“the allocation of hard and software resources and the majority of the associated activities and functions are not carried out, obtained or stored
in a centralized location.” [^lee18inclusive] In simpler terms, this decentralized nature works through a computer network that allows for peer-to-peer transactions
that are not reliant on one central authority like the government or banks. 

Cryptocurrencies do not have any government-mandated or inherent value assigned to them.
Instead, their worth is entirely determined by market demand and the price that buyers and sellers are willing to agree upon during transactions.
This means their value can fluctuate significantly based on factors such as investor interest, public perception, and overall market conditions.
Therefore “a cryptocurrency is more useful when more people use it, then benefits are maximized when everybody adopts the same cryptocurrency. Until now, Bitcoin remains the dominant cryptocurrency”[^bolt17bitcoin] 
which is the first cryptocurrency that existed and the most commonly traded. Bitcoin was founded in 2009, and invented by Satoshi Nakamoto, which is widely assumed 
to be a pseudonym name for an individual or group as  their identity is unknown. Bitcoin itself is run using an open source software “It is not only decentralized
but also supposedly fully distributed. That means that every node or computer terminal is connected to each other.”[^lee15handbook] Interestingly enough it can be downloaded by anyone.

Bitcoin is not the only cryptocurrency but it is the most popular. Alternative cryptocurrencies (called “altcoins”) 
numerous others exist, e.g., Namecoin, Litecoin, Peercoin, or Dogecoin, etc. [^bolt17bitcoin]
However, after Bitcoin, some of the most popular are Ethereum (ETH), Tether (USDT), USD Coin (USDC) and many more.

![cryptocurrency](images/cryptocurrency.png)

[^bolt17bitcoin]: Bolt, Wilko. 2017. “Book Review: Bitcoin and Cryptocurrency Technologies: A Comprehensive Introduction.” _Journal of Economic Literature_ 55 (no. 2): 647–49.
[^lee18inclusive]: Lee, David Kuo Chuen and Linda Low. 2018. _Inclusive fintech: Blockchain, Cryptocurrency and ICO_. World Scientific.p.43
[^lee15handbook]: Lee Kuo Chuen, David, ed. 2015 _Handbook of Digital Currency: Bitcoin, Innovation, Financial Instruments, and Big Data_. Amsterdam: Academic Press is an imprint of Elsevier. [http://literati.credoreference.com/content/title/estdigital.].p:8,11-12 



## CSS

The Cascading Style Sheets, or CSS for short, is one of the core components of building a website, alongside HTML (Hypertext Markup Language) and JavaScript (JS). Together, these three technologies are often referred to as "the cornerstone of the World Wide Web."[^ebersbach01css]

CSS is the language used for styling HTML documents. It defines the visual appearance of HTML on various devices, such as screens, printers, or other display mediums.[^ebersbach02css] In other words, CSS is responsible for handling visuals like text colour, background styles, buttons, user interface elements, and much more. These features are present on virtually all modern websites. It also enables interactive effects, such as hover states on buttons or cards, and optimizes visuals for different screen sizes—from small smartphones to large desktop monitors. Many people compare CSS to the skin on the HTML skeleton,[^ebersbach03css] or, as I like to say, the sweet icing on the cake.


### A Brief History of CSS

The concept of CSS emerged in the early 1990s to address the limitations of early web design. At that time, the web was primarily built with HTML for structure, and visual design capabilities were quite limited. The goal was to create a system that separated content (HTML) from presentation (CSS), allowing for more visually appealing websites.

In 1994, Håkon Wium Lie, a Norwegian web pioneer, and Bert Bos, a Dutch programmer, proposed the idea of CSS. Their specification introduced the idea of separating HTML as the structure for web content and CSS as the language for styling and formatting that content. This foundational idea became a critical step in the evolution of web design.[^ebersbach04css]

The first official CSS specification, CSS1, was published by the World Wide Web Consortium (W3C) in 1996, laying the foundation for modern web development with basic text colours, fonts and backgrounds. In 1998, CSS2 introduced new features, such as the "position" property for improved control over webpage layout and support for media types to optimize designs for different devices. In 2011, CSS2.1 was released, providing greater stability, improved browser compatibility, and enhanced accessibility across most browsers (except Internet Explorer). Currently, we use CSS3, which introduced a modular specification approach. This version brought transformative features like animations, gradients, flexbox, and many more tools for modern web design. Looking ahead, CSS4 is under development, with goals to enhance web design capabilities further, including responsive design improvements, advanced typography options, and a more robust grid system.[^ebersbach04css]


### How to apply CSS in HTML

As established, CSS is used for formatting the visual presentation of HTML. Here’s a visual example of how CSS is set up:  
![CSS example](images/01exampleCSS.png)[^ebersbach05css]


#### Let's break it down

- We start with the **selector**, which in this case is `h1`. In our CSS, we are modifying the `h1` selector from our HTML.
- Next, we define a **declaration block**, which contains the styles we want to apply to the selected element.
- A **property** refers to the specific style we want to modify (e.g., `color`, `background`, `width`).
- A **value** is the specific setting applied to a property (e.g., `color: red; background: yellow; width: 10px`).

In the visual example, we are applying styles to our `h1` selector. The declaration block sets the text colour to red and the background colour to yellow.[^ebersbach05css]

In order to stylise our website, we must make a connection between both HTML and CSS.
![CSS style](images/02exampleCSS.png)[^ebersbach06css]

This image represents the external method of linking a CSS file to an HTML file to style the website. This method is easier to manage when handling CSS declarations. *(You can also apply CSS internally, but it tends to be messier.)*



[^ebersbach01css]: Wikipedia. 2025. "CSS." Last modified January 21, 2025. https://en.wikipedia.org/wiki/CSS
[^ebersbach02css]: W3Schools, "CSS Introduction," accessed January 21, 2025, https://www.w3schools.com/css/css_intro.asp
[^ebersbach03css]: Nology Team, "HTML, CSS, and JavaScript: The Anatomy of a Website," last modified February 6, 2020, accessed January 21, 2025, https://nology.io/news/html-css-and-javascript-the-anatomy-of-a-website/
[^ebersbach04css]: Anupama Ra. "History of CSS: The Evolution of Web Design." AlmaBetter. Accessed January 28, 2025. https://www.almabetter.com/bytes/articles/history-of-css
[^ebersbach05css]: Eric Meyer and Estelle Weyl, CSS: The Definitive Guide (Sebastopol, CA: O'Reilly Media, 2023), 3
[^ebersbach06css]: Keith J. Grant, CSS in Depth, 2nd ed. (Shelter Island, NY: Manning Publications, 2023), 6.

BONUS: Last fall session, I created a website to help both students and curious learners learn the basics of CSS and HTML: https://hybrid.concordia.ca/hu_sia/GG-Web_developer/



## Cultural Determinism 

Cultural determinism is the theory that human behavior, identity, and group dynamics are shaped primarily by cultural factors[^OxCul] such as norms, beliefs, and values.
It reflects that a person's or group's social context, including their cultural construction, determines how they perceive the world, behave, and interpret experiences.
This framework suggests that mental illness, social roles, and even individual actions are products of cultural expectations rather than biological or normal factors [^Meford01Cult]. 
Cultural determinism often critiques the idea that behavior is biologically determined, proposing instead that it is molded by the societal and cultural systems within which individuals live.

Cultural determinism is especially relevant in the context of digital technologies and platforms. 
In virtual environments, identities and norms are often constructed and reinforced through algorithms[^Mike80Cult] that prioritize certain types of content, reinforcing specific cultural ideals. 
Social media platforms, such as Instagram and TikTok, exemplify how cultural values around body image, success, and lifestyle are propagated and amplified. 

Cultural determinism in the digital medium also raises controversial questions about free will, personal autonomy, and the power of technology in shaping behavior.

[^Meford01Cult]:Spiro, Melford E. 2001. “Cultural Determinism, Cultural Relativism, and the Comparative Study of Psychopathology.” Ethos 29, no.2 (2001): page range. https://www-jstor-org.lib-ezproxy.concordia.ca/stable/640637?seq=1 
[^Mike80Cult]:López, Mike. “Cultural Determinism, Educational Achievement, and a Brief Lesson on Chicano Studies.” The Clearing House 53, no. 6 (1980): 292–94. http://www.jstor.org/stable/30197897. 
[^OxCul]:Oxford Review. (n.d.). Cultural determinism: Definition and explanation. The Oxford Review. https://oxford-review.com/the-oxford-review-dei-diversity-equity-and-inclusion-dictionary/cultural-determinism-definition-and-explanation/ 

## Cyberculture

Cyberculture is a broad social and cultural movement shaped by the growth and spread of advanced digital technologies. It includes various social groups, forms of communication, and cultural practices that reflect how technology is transforming society. Cyberculture views technology as both a force for change and a tool for empowerment, fostering new types of influence, virtual spaces, and authentic digital experiences. Through online communities, digital interactions, and shared narratives, cyberculture redefines how people engage with technology and one another in an increasingly connected world.[^macek]

Cyberculture emerged in the 1960s at the Massachusetts Institute of Technology as a term describing the evolving relationship between computers and networks. It gained prominence with William Gibson's 1984 novel Neuromancer, which was closely tied to the cyberpunk subculture. Initially linked to hackers and digital avant-garde communities in the 1980s, it became more established in the 1990s with the rise of the internet, evolving into a broader sociocultural and political movement that shapes new digital behaviors and practices.[^vicenteamaral]

As digital technologies continue to advance, cyberculture evolves, reshaping how people interact with cultural content. The concept of cultural interfaces highlights how computers present and structure media, drawing from influences like cinema, print, and human-computer interaction.[^manovich] As media shifts away from physical formats, digital interfaces adapt, blending familiar elements from traditional culture with new ways of navigating and experiencing information. The rise of hyperlinking in the 1990s further transformed cyberculture by encouraging non-linear, decentralized structures, challenging traditional hierarchies in media.[^vicenteamaral] These changes illustrate the ongoing interplay between technology and culture, shaping the way digital experiences are created and understood.

![Cyberculture](images/cyberculture_aneder.jpg)

[^manovich]: Manovich, Lev. 2002. The Language of New Media. Leonardo. Cambridge, Mass.: MIT Press. ISBN: 9780262632553.

[^macek]: Macek, Jakub. 2005. "Defining Cyberculture." In Defining Cyberculture, 19. Translated by Monika Metyková and Jakub Macek. https://www.academia.edu/460099

[^vicenteamaral]: Vicente, Paulo Nuno, and Inês Amaral. 2020. "Cyberculture." In The SAGE International Encyclopedia of Mass Media and Society, edited by Debra L. Merskin, 430-432. Thousand Oaks: SAGE Publications, Inc., https://doi.org/10.4135/9781483375519.n171.

## Cyberfuture

Cyberfuture, as one might find by a quick search, can be defined simply as “a projected future based around cyberspace or computer technology.”[^wiktionary25cyberfuture] It is deceivingly simple, but in truth represents a complicated interlinking definition between how we think of the future and how we interact with different forms of cyberspace.
	To understand cyberfuture, we must define cyberspace. According to Cees J, Hamelink in his chapter in Cyberidentities for the University of Ottawa Press, defines cyberspace as “not limited to the operation of computer networks, but also encompasses all social activities in which digital information and communication technologies (ICT) are deployed.” [^Hamelink98human] These range from a microwave embedded with digital technology to shoes, to any other object embedded with such technologies, therefore contributing to a virtual life-space [^Hamelink98human].
	
If cyberfuture is defined in part by cyberspace, the topic of a cyberfuture would then include discussions on these technologies and how they may affect the ways in which humans develop alongside these digital and mechanical subjects. This as well makes the term anthropocentric, as the effects of technology in the cyberfuture is always discussed around human interactions and societies. 

This idea of “virtual life-space” as discussed by Hamelink seems a popular object of discussion, with instances of cyberfutures being imagined in science-fiction where “there is no physical contact between bodies” [^franck98imagining]. Karen A. Franck discusses an example where “all humans are raised and live alone their entire lives in 10 foot square cells, food and drink and all life support is provided by computer. Contact with anything else is exclusively in a virtual world.”[^franck98imagining]. While Franck’s example is noticeably bleak, the ties of cyberfuture to a social integration, even dependency, on cyberspace is clear. The technologies included in a discussion surrounding cyberfutures do not have to be so overt, but the idea of an essential fusing of  human society and technology is fundamental.

When considering the effects of modern and developing technologies on human society, the discussion is automatically linked to cyberfuture, as any developments of human society involving an integration of technology is such a future.

![cyberfuture](images/cyberfuture-axiuk.png)
*Tool: [DALL-E through Microsoft Copilot](https://copilot.microsoft.com/). Prompt: “cyberfuture where humans and technology are interlinked”*

[^wiktionary25cyberfuture]: 2025.“Cyberfuture - Wiktionary, The Free Dictionary.” *Wiktionary*. https://en.wiktionary.org/wiki/cyberfuture#:~:text=cyberfuture%20(plural%20cyberfutures),around%20cyberspace%20or%20computer%20technology. 

[^franck98imagining]:  FRANCK, KAREN A. 1998. “Imagining as a Way of Knowing: Some Reasons for Teaching ‘Architecture of Utopia.’” *Utopian Studies* 9, no. 1 : 120–41. http://www.jstor.org/stable/20719746.

[^Hamelink98human]:Hamelink, Cees J. 1999. “Human Rights in Cyberspace.” In *Cyberidentities: Canadian and European Presence in Cyberspace*, edited by Leen d’Haenens, 31–46. Ottawa: University of Ottawa Press. https://doi.org/10.2307/j.ctt1cn6rfb.6.​





## Cybersecurity

Cybersecurity refers to the practice of protecting digital systems, networks, and data from cyber threats, including hacking, data breaches, and malware attacks. In the context of new media, cybersecurity is crucial for maintaining the integrity, privacy, and security of digital communication, social media platforms, and online transactions. As media consumption and interaction shift towards digital spaces, the need for robust cybersecurity measures has intensified to prevent unauthorized access and ensure user trust.

### Historical Origins

The concept of cybersecurity dates back to the early days of computing in the 1960s and 1970s when researchers at the Advanced Research Projects Agency (ARPA) began studying network security. The development of the ARPANET, the precursor to the internet, exposed vulnerabilities that necessitated protective measures [^schneier2020]. In the 1980s, the emergence of computer viruses and worms, such as the Morris Worm (1988), underscored the need for cybersecurity frameworks [^anderson2021]. The rise of the internet in the 1990s further propelled cybersecurity research, leading to encryption techniques, firewalls, and cybersecurity legislation.

### Technological Significance

Cybersecurity is integral to modern technologies such as cloud computing, artificial intelligence (AI), and blockchain. Encryption methods, such as RSA and AES, provide data confidentiality, while machine learning algorithms enhance threat detection [^gordon2022]. The proliferation of Internet of Things (IoT) devices has also increased the attack surface, requiring advanced cybersecurity protocols to prevent unauthorized data access.

### Influence on Digital Culture & Society

Cybersecurity affects various aspects of digital culture, including online privacy, digital rights, and national security. The rise of cyber warfare and state-sponsored attacks has influenced geopolitical relations, as seen in cases like the 2016 U.S. presidential election cyber interference. Social media platforms implement cybersecurity measures to combat misinformation, identity theft, and data exploitation [^singer2014]. Additionally, businesses and governments enforce cybersecurity compliance through regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA).

### Paradigmatic Examples

Several high-profile cybersecurity incidents illustrate its impact on new media:

* The 2013 Yahoo data breach exposed over 3 billion user accounts, highlighting the risks of inadequate security protocols.
* The 2017 Equifax breach compromised sensitive information of 147 million individuals, prompting stricter cybersecurity regulations.
* The WannaCry ransomware attack in 2017 affected systems worldwide, emphasizing the importance of timely software updates and security patches.

### Key Figures and Groups

Cybersecurity research and policy development have been shaped by key individuals and organizations, including:

* Clifford Stoll – Author of *The Cuckoo’s Egg*, who uncovered an early case of cyber espionage.
* Kevin Mitnick – Former hacker turned security consultant, whose exploits revealed major system vulnerabilities.
* The National Institute of Standards and Technology (NIST) – Developed widely adopted cybersecurity frameworks.
* The Electronic Frontier Foundation (EFF) – Advocates for digital rights and online security.





[^anderson2021]: Anderson, Ross. 2021. *Security Engineering: A Guide to Building Dependable Distributed Systems*. 3rd ed. Hoboken: Wiley.

[^gordon2022]: Gordon, Lawrence A., Martin P. Loeb, and Lei Zhou. 2022. "The Economics of Cybersecurity: A Survey of the Literature." *Journal of Cybersecurity* 8 (1): 1-15.

[^schneier2020]: Schneier, Bruce. 2020. *Click Here to Kill Everybody: Security and Survival in a Hyper-Connected World*. New York: W.W. Norton.

[^singer2014]: Singer, Peter W., and Allan Friedman. 2014. *Cybersecurity and Cyberwar: What Everyone Needs to Know*. New York: Oxford University Press.

## Cyberspace


It is a space where computers can interact with each other and share their data and content with each other. A reality that is computer generated, computer accessed and virtual. It is the place we are in during our communications through computers. It is a complete virtualisation of our physical world. [^bauwens94cyberspace]

The term ‘cyberspace’ was first used by William Gibson in 1982 when he used it to describe a computer generated virtual reality. ‘Cyber’ is related to ‘cyborg’ which describes humans connecting to machines. It is an artificial reality, another world. Networks and the Internet are examples of what we might consider a cyberspace. [^fourkas04cyberspace]

A global domain of interdependent networks of information systems infrastructures like the internet. It is a complex network environment formed by the connection of all these systems and technology devices. What exists in cyberspace doesn’t have physical form. It is made and accessed only through computers and their connections. [^nist12information]

Cyberspace is new representation of the physical world. It contains its own new ecosystem and media. This interlinked web of computers holds the space for new media to exist and permeate.

![Cyberspace image](images/cyberspace-ryanbujold.png)

[^bauwens94cyberspace]: Bauwens, Michel. 1994. “What Is Cyberspace?” *Computers in Libraries* 14 (4): 42–48. 

[^fourkas04cyberspace]: Fourkas, Vassilys. 2004. "What Is Cyberspace." *Media Development* 3: 6-7.

[^nist12information]: “NIST: Information Security” *NIST.gov*. Last modified September, 2012. https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-30r1.pdf 

Image credits:
- Drew Williams, *Macbook Pro*, photograph, Pexels, July 2019, https://www.pexels.com/photo/macbook-pro-2657669/.
- fauxels, *Photo of people doing handshakes*, photograph, Pexels, November 2019, https://www.pexels.com/photo/photo-of-people-doing-handshakes-3183197/

## Data Mining

Data mining is described as a “involves the use of sophisticated data analysis tools to discover previous unknown, valid patterns and relationships in large data sets”[^Seifert] and that data mining tools have different models such as statistical models, mathematical algorithms, and machine learning methods. Data mining doesn’t just consist of collecting and managing data, but it also includes analysis and predictions. 

Data mining is used for a various of purpose whether it would be private or public sectors. Data mining is used in banking, insurance, medicine and retailing to reduce costs, enhance research and increase sales. There are examples of data mining being used such as in banks, to detect fraud and assist in risk assessment and uses customer’s data to learn about if they are in good credit scoring or if it will detect any fraudulent insurance claims.[^Seifert]

Data mining is considered a “powerful artificial intelligence (AI) tool which can discover useful information by analyzing data from many angles or dimensions, categorizing that information and summarize the relationship identified in the database”[Algarni]  one of the main process used in data mining is knowledge discovery which is essentially “a process of nontrivial extraction of implicit, unknown and potential information from a large database.”[^Algarni] it is considered to be the most interactive process and examines many decisions made by the user.

When it comes to the influence of data mining in digital culture and society, it’s important to know that data mining is everywhere, it is used on social media such as sending twitter (now X) post on the website, humidity of shipping crates which in studies of a lot of data accumulated is equivalent to a zettabyte of data or sextillion bytes and growing. There is new analytical knowledge driven by accompanying forms of data mining and analytics. One of the most important parts of data mining and digital culture is “these companies and their analytic methods promise to help us gain insight into public opinion, mood, networks, behaviour patterns and relationship”[^Andrejevic,Hearn,andKennedy] as for socially it is also mentioned how “Data analytics involve far more than target advertising, however; they envision new strategies for forecasting, targeting and decision-making in a growing range of social realms, such as marketing, employment, education, health care, policing, urban planning and epidemiology.”[^Andrejevic,Hearn,andKennedy] 

However, there are important role to play in considering the social and political consequence of data mining and analytics.  There is knowledge of “when every move we make online is tracked by privately owned corporations and the state, advertisements follow us around in material retail spaces, and even our sleep patterns become fodder for self-tracking (to gain ‘self-knowledge’) {…} by approaching them solely as communication media.”[^Andrejevic,Hearn,andKennedy]

Data mining is significant in new media as it allows companies, the internet to collect data that you have grown accustomed to and use it to advertise or push products to the user online or in person.  It is something very evident online of seeing advertisements of a product of any sort to try to elicit you to engage in it more.


[^Seifert]: Seifert, Jeffrey W. 2004._National security issues_. Google Scholar: 201-217.
[^Algarni]: Algarni, Abdulmohsen.2016._Data mining in education_. International Journal of Advance Computer Science and Applications: 456-461.
[^Andrejevic,Hearn,andKennedy]: Andrejevic Mark, Hearn Alison, Kennedy Helen.2015._Cultural studies of data mining: Introduction_. European Journal of Cultural Studies: 379-394.



 


## Data Science

Data science is the procedure of using scientific methods, algorithms, and systems to extract insights and information 
from data. It combines expertise in statistics, programming, and domain knowledge to analyze large datasets, uncover 
patterns, and make predictions. The objective is to help associations and companies make informed, data-driven decisions,
uncover their target audience and commercialize within a specific algorithm. [^dhar13data]

In new media, data science allows for the analysis of large datasets generated by user interactions, helping platforms 
like Netflix, Facebook, TikTok, and YouTube optimize content delivery and advertising strategies. The ability to process
and interpret big data allows for more personalized experiences and real-time trend forecasting, which is important in
navigating user engagement and maximizing revenue. [^sweeney13discrimination]

Data science is closely tied to contemporary technologies such as machine learning, artificial intelligence, and big data 
frameworks. These technologies support automated processes, content recommendation algorithms, and targeted advertising. 
For example, Netflix uses data science to recommend shows, while Facebook tailors ads to specific user demographics, 
both approaches shaping the way media is consumed and monetized. [^gomez-uribe,hunt15algorythm]

[^dhar13data]: Dhar, Vasant. 2013. “Data Science and Prediction.” *Communications of the ACM* 56 (12): 64–73.

[^sweeney13discrimination]: Sweeney, Latanya. 2013. "Discrimination in Online Ad Delivery." *Communications of the ACM* 56 (5): 44-54.

[^gomez-uribe,hunt15algorythm]: Gomez-Uribe, Carlos A., Neil Hunt. 2015. "The Netflix Recommender System: Algorithms, Business Value, and Innovation." *ACM Transactions on Management Information Systems* 6 (4): 1-19.

## Dataset 

A Dataset is a structured collection of data, typically organized in a tabular form where each column represents a specific variable, 
and each row corresponds to an individual record. This organization facilitates efficient data management, analysis, 
and retrieval across various disciplines. “Even the term dataset is problematic, referring to at least four common themes—grouping, 
content, relatedness, and purpose—each of which has multiple categories” [^borgman15bigdata]. 

A dataset is an organized compilation of interconnected information sets, consisting of distinct elements that,
despite being separate, it can be processed and managed collectively by a computer as a single entity. 
These information sets are data “In rhetorical terms, data are that which exists prior to argument or interpretation 
that converts them to facts, evidence and information” [^kitchin14revolution] in the Book “The Data Revolution: ...” 
they defined data as the basic,  input used to represent aspects of the world by organizing them into categories,
measurements and other representational forms. These forms can include numerical values, text, images, sounds, and digital signals, 
which are the foundation for producing meaningful information and knowledge.[^kitchin14revolution]

There are different types of datasets as these datasets can be categorized by the type of data they contain or by the relationship
between the variables of data. These classifications can be categorical, correlation, bivariate, multivariate and structured. 
These classifications are affected by the various types of data “such as relational data (structured data), text (unstructured data),
semi-structured data (like XML or JSON), and streaming data from sources like sensors or social media.” [^russom13managing]. 

Depending on the use of the dataset or its size and complexity can affect how it is managed. Datasets that are very large and complex
are difficult to manage and analyze using traditional data processing tools. This can be defined as Big Data which are datasets
of multiple terabytes or larger as it contains “a wide range of data types (relational, text, multi-structured data, etc.) 
from numerous sources, including relatively new ones (Web applications, machines, sensors, and social media)”[^russom13managing].
Datasets can vary in size from just a few bits to several terabytes and can function as standalone entities. 
The suitable unit of measurement is determined by the specific purpose for which the dataset is being used 
“In addition, digital datasets can be opened only with certain software, whether statistical tools, instrument-specific code,
or software suited to applications in the domain” [^borgman15bigdata].

![Dataset](images/dataset.png)

*ChatGPT prompt: "Can you generate me an image that visually depicts the word "dataset" without using words in the image"*


[^borgman15bigdata]: Borgman, Christine L. 2015. _Big Data, Little Data, No Data: Scholarship in the Networked World_. Cambridge, Massachusetts: The MIT Press.
https://cornell-library.skillport.com/skillportfe/main.action?assetid=82612. 

[^kitchin14revolution]: Kitchin, Rob. 2014. _The Data Revolution: Big Data, Open Data, Data Infrastructures & Their Consequences_. Los Angeles, California: SAGE Publications. 

[^russom13managing]: Russom, Philip.2013. "Managing big data." _TDWI Best Practices Report_, TDWI Research. 1-40.

## Deepfakes

Deepfakes are realistic manipulations of video, audio and images using deep learning, specifically Generative Adversarial Networks (GANs) to swap faces, mimic voices and create entirely fabricated events [^farid2022]. While CGI has been used in films for a long time, deepfakes are different because they do not require Hollywood budgets or specialized training. With the right tools, nearly anyone can generate a convincing fake [^kietzmann2020]. The term “deepfake” comes from a Reddit user who, in 2017, started sharing face-swapped celebrity adult content. What made this particularly alarming was that the user also released the code, allowing others to replicate and refine the technology [^kietzmann2020]. Since then, deepfake capabilities have evolved significantly. One of the first viral examples was "Synthesizing Obama," a 2017 project that used AI to perfectly mimic Barack Obama’s speech and facial movements, making him appear to say things he never actually did [^kietzmann2020]. At the core of deepfake technology is the GAN, first introduced by Ian Goodfellow in 2014. These networks operate through a competitive process: one AI (the generator) creates fake media, while another AI (the discriminator) tries to detect it. As they continuously challenge each other, the generator improves and produces convincing results [^farid2022]. 

Deepfakes are not inherently harmful. In entertainment, they have been used for digital concerts, such as the virtual ABBA performance featuring AI-generated younger versions of the band [^fernandez2024]. Filmmakers have also used similar technology to de-age actors or recreate deceased performers [^fernandez2024]. AI-generated digital doubles have even been explored for virtual assistants and online interactions [^fernandez2024]. 

However, the technology is more often associated with harmful applications. Manipulated media is widely used for disinformation, political propaganda, identity theft and non-consensual explicit content [^farid2022]. In today’s “post-truth” era, where fake news spreads rapidly, deepfakes amplify the challenge of distinguishing fact from fiction [^fernandez2024]. The technology has been weaponized in political attack ads, misinformation campaigns and fabricated speeches designed to manipulate public opinion [^farid2022]. Given how convincing these fakes have become, they pose a serious threat to journalism, democracy, and personal security [^fernandez2024].    

[^farid2022]: Farid, Hany. 2022. “Creating, Using, Misusing, and Detecting Deep Fakes.” *Journal of Online Trust and Safety* 1 (4). https://doi.org/10.54501/jots.v1i4.56. 

[^fernandez2024]: Fernández Gambín, Ángel, Anis Yazidi, Athanasios Vasilakos, Hårek Haugerud, and Youcef Djenouri. 2024. “Deepfakes: Current and Future Trends.” *Artificial Intelligence Review* 57: 64. https://doi.org/10.1007/s10462-023-10679-x.

[^kietzmann2020]: Kietzmann, Jan, Linda W. Lee, Ian P. McCarthy, and Tim C. Kietzmann. 2020. “Deepfakes: Trick or Treat?” *Business Horizons* 63 (2): 135. https://doi.org/10.1016/j.bushor.2019.11.006.

## Deep Learning

Deep learning can be defined as an advanced form of machine learning. These algorithms usually operate by taking information in and pushing out an output after processing. The Deep Learning process instead involves multiple algorithms of processing information that communicate with each other to analyze data.[^jo23foundations]

This process creates a computer that can learn from experience independently. It learns complex concepts by building from simpler ones. These layers of understanding create hierarchies many layers deep.[^goodfellow16learning]

These artificial neural networks are inspired by the design of the human brain. Each node of this network is responsible for learning different parts of the data. They can solve many different kinds of problems from image recognition to language processing. It is the latest frontier for AI and complex machines. Deep learning is still held back by large data requirements, cannot distinguish good from bad data well and be biased due to the data it is given.[^google25whatis]

With machines taking a step closer to human intelligence, their existence makes us question our own understanding of intelligence and human identity. Learning and understanding was once only thought to be a natural trait of man. Now we must face the reality of machines that may catch up to our intellect.


![Deep learning Image](images/deeplearning-ryanbujold.jpg)
*Tool: [Stable Diffusion Web](https://stablediffusionweb.com). Prompt: “Deep Learning Networks”*

[^jo23foundations]: Jo, Taeho. 2023. "Deep Learning Foundations" *Springer Nature Link* https://link-springer-com.lib-ezproxy.concordia.ca/book/10.1007/978-3-031-32879-4.
[^goodfellow16learning]: Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. "Deep learning" *Vol. 1, no. 2. Cambridge: MIT press* https://synapse.koreamed.org/pdf/10.4258/hir.2016.22.4.351.
[^google25whatis]: Google. 2025. “What is Deep Learning?” *Google Cloud*. https://cloud.google.com/discover/what-is-deep-learning 

## Digital Footprint

Digital footprints are defined as a way to map and record the online activities of an individual’s life, using trails of metadata to log information that can prove someone’s personal identity [^blue]. These patterns are categorized in two distinct classifications; passive and active digital footprints [^arakerimath]. Passive digital footprints are collected online without direct user intervention, usually in the form of geo-tracking, browsing history, IP addresses and cookies, while active digital footprints are tracked using the deliberate data left by users on social media platforms and other uploading websites [^kaku]. 

Something as innocuous as liking a friend’s photo, commenting or using hashtags creates a digital lineage of personal actions that tracks your daily activity [^arakerimath]. Companies use this data and valuable information to promote specific products to users, targeting individual’s personality traits, behaviours and desires [^arakerimath]. These predictions are implemented with the use of algorithms that search for both passive and active footprints, scraping the web for details such as your name, age, gender, location, and profession [^arakerimath]. 

For these reasons, it is imperative to monitor your personal online activities so as to reduce the possibility of identity theft, and remain vigilant about online scams. The Canadian Centre for Cyber Security provides some important advice which may help individuals in these situations, including installing ad-blockers, firewalls and anti-virus software, implementing multi-factor authentication (MFA), using strong passwords, avoiding public wifi, using virtual private networks (VPN), removing metadata from photos, and more [^canadian]. 

Digital artifacts can be complicated to deal with on a personal level, as seen with people in the midst of transitioning between genders. An empirical analysis was conducted to track the challenges associated with this issue, demonstrating that this process was problematic for most. This resulted in either editing personal data on Facebook manually to keep their previous contacts, by maintaining multiple accounts with different names, or by creating an entirely new account [^haimson]. This fact raises concerns about how digital footprints can impact our lives in intricate ways, especially for those wanting to edit or erase digital artifacts from the web. As designers, we must consider these issues when creating apps and websites, implementing ways to easily delete or forget information in conjunction with remembering [^haimson]. This data contains a multitude of details which may alter your daily life in various ways, so it is essential to understand the implications of digital footprints and the secure solutions, protecting your IRL identity while maintaining your online privacy.

[^arakerimath]: Arakerimath, Anjana R., & Gupta, Pramod Kumar. "Digital Footprint: Pros, Cons, and Future." *International Journal of Latest Technology in Engineering, Management & Applied Science* 4, no. 10 (2015): 52–56. https://www.ijltemas.in/DigitalLibrary/Vol.4Issue10/52-56.pdf. 

[^blue]: Blue, Juanita, Condell, Joan, & Lunney, Tom. "Digital Footprints: Your Unique Identity." ResearchGate, 2018. https://www.researchgate.net/publication/328896992_Digital_Footprints_Your_Unique_Identity. 

[^canadian]: Canadian Centre for Cyber Security. "Digital Footprint." ITSAP.00.133. https://www.cyber.gc.ca/sites/default/files/itsap.00.133-digital-footprint.pdf. 

[^haimson]: Haimson, Oliver L., Brubaker, Jed R., Dombrowski, Lynn, & Hayes, Gillian R.. "Digital Footprints and Changing Networks During Online Identity Transitions." https://oliverhaimson.com/PDFs/HaimsonDigitalFootprints.pdf. 

[^kaku]: Kaku, Sonia. "Navigating the Digital Landscape: Understanding and Managing Your Digital Footprint." *Global Media Journal*, July 23, 2024. https://www.globalmediajournal.com/open-access/navigating-the-digital-landscape-understanding-and-managing-your-digital-footprint.pdf. 


## Digitization

Digitization describes the conversion of data, physical, analog, into a digital form.[^oxfor10dictionary] The process brings with it discussions on the positive and negative sides of this conversion, particularly when it comes to the preservation and accessibility of data, both from a creator and consumer perspective.
	
From a creator perspective, this shift disrupts “a number of copyright-protected media industries, including books, music, radio, television, and movies.” Because  “once information is transformed into digital form, it can be copied and distributed at near-zero marginal costs.” (Waldfogel 195) Not only is copyright infringement an issue, as in the unwanted distribution of an artist’s material, but it becomes an issue of income as well due to the fact that “digitization  has threatened the traditional revenue sources of some of these media industries.”[^waldfogel17golden]

Interestingly, the digitization of analog media has allowed for more seemingly positive outcomes for consumers. Due to the aforementioned widespread distribution, media is more easily reachable for those with access to digital versions, especially the internet. It reshuffles the mix of public and private.[^greenstein13digitization] As well, discussions around digitization often center around the preservation of media, for which it is applied in the case of museums and other such archives, both as backup for the physical version and contributing to its accessibility if made available online.

However, Paul Conway in his article Preservation in the Age of Google presents an interesting perspective on the consumer end of this digital era of information. He explains that there are two sources which convey the feeling of a flood of digitized information; “[It feels as if] we are indeed immersed in an all-digital environment. One source is the collectively massive and accelerating conversion of book and nonbook materials from analog to digital form. The second source derives from the fact that nearly all new information is created digitally, communicated digitally, used in a digital environment, and stored in digital systems, sometimes ‘for posterity.’”[^conway10preservation]

The process of copying analog information into digital versions of itself seems straightforward and logical given the capabilities of archiving that comes with modern technologies. Yet although it does have many advantages, it is important to consider how this new way of distributing information fundamentally changes the way that media is accessed and received, with these changes being felt on both the creator and consumer end. 


[^conway10preservation]: Conway, Paul. 2010. “Preservation in the Age of Google: Digitization, Digital Preservation, and Dilemmas.” *The Library Quarterly: Information, Community, Policy* 80, no. 1: 61–79. https://doi.org/10.1086/648463.

[^oxfor10dictionary]: “Digitization, N. Meanings, Etymology and More .” 2010 *Oxford English Dictionary*. https://www.oed.com/dictionary/digitization_n?tab=meaning_and_use&tl=true.

[^greenstein13digitization]: Greenstein, Shane, Josh Lerner, and Scott Stern. 2013 “Digitization, Innovation, and Copyright: What Is the Agenda?” *Strategic Organization* 11, no. 1: 110–21. http://www.jstor.org/stable/43581893.

[^waldfogel17golden]: Waldfogel, Joel. 2017. “How Digitization Has Created a Golden Age of Music, Movies, Books, and Television.” *The Journal of Economic Perspectives* 31, no. 3: 195–214. http://www.jstor.org/stable/44321286.

## DNS

DNS stands for Domain Name System. It was invented by Paul Mockapetris in 1983. [^cocoran11branding]

Each computer and server have their own IP address, a series of numbers, similar to how houses have addresses for their locations. 
To be able to connect to another computer through the internet, it was required to know the IP address. An IP address is a series of 
numbers that would point to the location of the site, similar to how our home addresses tell us where we are geographically.

In the past, to find this specific number, a `hosts.txt` file had a table with the name of the host linked to their IP address. 
However, this file had to be updated every time a new “location” was created. With the rise of the Internet, it quickly fell behind. [^cocoran11branding]
To fix this issue, Paul Mockapetris invented the domain name system. 
This system takes domain names and translates them into IP addresses for the browser to use. [^cloudflare25DNS] The system is divided into different categories and levels to help organization and navigation.
While most top-level domains categorize sites by geographical location, they also have a few to signify the site’s purpose. For example, “ca” is for canadian sites, “com” is for commercial sites and “org” is for non-profit organizations.
You could also add subdomains for even more specificity. They just have to be separated by a dot. Example: https://library.concordia.ca/
The “.ca” part is the top-level domain and there’s the subdomain “.concordia”.
This helps connect sites to each other and signify relationships between different pages. [^cocoran11branding]

The creation of the DNS led to a much more accessible Internet. Rather than having to remember and type long strings of numbers to reach other sites, 
users could now use words that are much easier to remember and in most cases shorter to type out. 
This made the Internet more appealing and accessible to the masses which led to an even bigger influx of users. [^cocoran11branding]

Without the DNS, new media that uses the online world as it’s main platform to be shared or diffused may not have been as accessible. The domain name, or name used to find a website has become an important part of that site’s identity. Websites are recognized by their domain names.
The DNS also paved the way for the World Wide Web and helped the Internet become what it is today. [^veaandreubonastre19origins]

![DNS](images/DNS-wong.png)  
*Computer image (left) from : 
[bert b](https://unsplash.com/@bertsz?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash) on [Unsplash](https://unsplash.com/photos/white-macintosh-computer-Zd6PL6PSW5E?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash). House image (right) from: 
[Luke Stackpoole](https://unsplash.com/@withluke?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash) on [Unsplash](https://unsplash.com/photos/red-and-white-house-surround-green-grass-field-eWqOgJ-lfiI?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)*



[^cocoran11branding]: Cocoran, Ian. 2011. _The Art of Digital Branding. Rev. ed._ New York: Allworth Press.
[^cloudflare25DNS]: Cloudflare. 2025. What is DNS| How DNS works.
[^veaandreubonastre19origins]: Bonastre, Oscar M., and Andreu Vea. 2019. “Origins of the Domain Name System.” _IEEE Annals of the History of Computing_ 41 (2): 48–60. https://doi.org/10.1109/MAHC.2019.2913116.


## e-Commerce

E-commerce (electronic commerce) is a term for buying and selling stuff through the internet. The increase of e-commerce over the years has been helped a lot by the increase of digital 
payment systems that have secure transactions and logistics networks. E-commerce’s significance in new media is shown by how it has completely changed the normal ways that the retail 
systems managed by allowing businesses to reach everyone everywhere on a global scale, without the need for a physical place. It allows for digital technologies like phones and computers 
to provide people with more convenience, variety, and better pricing. The large amount of popularity for e-commerce has also started to bring the use of new media marketing strategies, 
like social media advertising and influencer partnerships, to attract and keep customers. 

E-commerce’s relation to current technologies in the modern world is seen through the different platforms combining technologies like artificial intelligence (AI) and machine learning 
with their e-commerce stores, to give the users a more personalized shopping experience, like having recommend products based off of what they have been browsing. Additionally, 
live commerce was created as a combination of live streaming and online shopping, which has came out as a new trend to give a more interactive and immersive shopping experience. [^sciencedirect.com] 
The increase of mobile apps have also improved the accessibility by allowing users to shop anytime and anywhere. 

The historical origins of e-commerce are rooted back to when the development of Electronic Data Interchange (EDI) in the 1960s were created, which then allowed for businesses 
to exchange documents and files like purchases that happen nowadays through the internet, getting rid of the need for paper. [^britannica.com] In 1979, Michael Aldrich lead the way for 
this kind of work by attaching a television to a transaction processing computer through a telephone line, which brought about the idea of teleshopping.[^researchgate.net] The 1990s were 
when the World Wide Web and the advancements in internet security like Secure Socket Layers (SSL), allowed online retail platforms to start making a name for themselves.

E-commerce’s influence on contemporary digital culture and society has greatly changed the user’s behaviors by changing their preferences towards online shopping because of its convenience and often 
has lower prices. It also has influenced the gig businesses, with platforms like Etsy that allows people to monetize their crafts.On top of this, e-commerce has changed the supply chain’s 
way of being by making the need for fast and effiecent delivery systems and warehousing solutions very important.

### Paradigmatic Examples
- Amazon: Founded by Jeff Bezos in 1994, Amazon started as an online bookstore and has evolved into the world's largest e-commerce platform.
- Alibaba: Established by Jack Ma in 1999, Alibaba is a Chinese multinational conglomerate specializing in e-commerce, retail, and technology.
- eBay: Launched in 1995 by Pierre Omidyar, eBay is an online auction and shopping website where individuals and businesses buy and sell a large variety of goods and services worldwide.

[^sciencedirect.com]: S Chen, A.M Degeratu, A Enders, V Katros, C Zott. “_E-Commerce and the Retail Process: A Review._” Journal of Retailing and Consumer Services, December 17, 2002. (https://www.sciencedirect.com/science/article/pii/S0969698902000620). 
[^researchgate.net]: Tian, Yan. “_(PDF) History of e-Commerce._” ResearchGate, January 2007. (https://www.researchgate.net/publication/314408412_History_of_E-Commerce). 
[^britannica.com]: Zwass, Vladimir. “_E-commerce | Definition, History, Types, Examples, & Facts._” Encyclopædia Britannica, December 17, 2024. [https://www.britannica.com/money/e-commerce].

## Embodiment


Embodiment is a term which is informed by and used in many different fields. 
Consequently, embodiment is a highly polysemic term, even inside a given discipline. 

Many researchers believe embodiment to be crucial to intelligence, artificial or natural.[^txtZ] This sort of cognition, embodied cognition, leads us to wonder what sorts of bodies true artificial intelligences might require in the future. Many different embodied forms of intelligences can inform that discussion. For example, physical bodies equipped with sensorimotor skills modeled after living organisms or even living organisms themselves and their autonomous/autopoietic character can be seen as two different embodiments of intelligence[^txtZ]. 

Some physiological illusions provide insight into how embodiment functions. One such illusion,  the Rubber Hand Illusion (RHI), occurs during an experiment in which a participant observes a synthetic hand being stroked at the same time as their own hand[^txtRE]. Eventually, the participant starts feeling the strokes on the synthetic hand even if their real hand hasn't been touched[^txtRE]. Given proper visual information (the synchronous stroking), they assimilate the hand as their own.  In other words, it is possible for an external object, which bears morphological resemblance to a human body part, to be incorporated as a part of a person's body. 

In the context of new media artforms - such as Virtual Reality (VR) - in which virtual representations of bodies are prominent, embodiment represents a key concern.

In the case of VR, the challenge rests in trying to understand if and how it is possible for a person to feel the same things towards a virtual body in a virtual world as towards their biological body situated in a physical world. [^txtVR] Three conditions can break down that challenge and contribute to creating a sense of embodiment (SoE) in VR contexts.

First, a person must feel a sense of self-location. In other words, they must locate their self as being contained in the body space represented by the virtual body. First person perspective can enhance this feeling, since it emulates our real-world egocentric perspective[^txtVR].

Second, a sense of agency over the virtual body needs to be felt by the person "inhabiting" it. This sense can be described as a feeling of motor control over the body paired with the impression that the predicted consequences of an action match its actual consequences (in other words, feeling like you actually performed that action, not a random event). Thus, decreasing the latency between actual movement in the physical world and visual feedback in the virtual world is essential. Having a strong delay between the two can negatively impact the feeling of agency and, by extension, embodiment[^txtVR]. 

Third, a sense of ownership should be felt by the person immersed inside the virtual world. A virtual avatar's morphological similarity to human body can increase this feeling of ownership[^txtVR].


![Embodiment](/images/embodiment-prosper.jpg)
Generated using StableDiffusion Online with these parameters: “Schematics for a pcb are enscribed on a patch work of skin, small transistors that replace moles and copper lines act as veins. The skin is neutral and looks slightly like plastic. Synthetic skin. Organic lines and connections. 
Guiding Scale: 13
With Fixed Seed.”

[^txtZ]: Ziemke, Tom. 2003. "What’s that Thing Called Embodiment?" *Proceedings of the Annual Meeting of the Cognitive Science Society* 25, 1305-1310
[^txtRE]: De Preester, Helena. 2010. "Technology and the Body: the (Im)Possibilities of Re-embodiment." *Found Sci* 16, 119–137.
[^txtVR]: Kilteni, Konstantina, Raphaela Groten and Mel Slater. 2012. "The Sense of Embodiment in Virtual Reality." *Presence* 21, no.2 (Fall): 373–387. 



## Encoding & Decoding

Within information theory and communications, encoding and decoding are complementary processes that manage how data is transformed, transmitted, and finally interpreted across various forms of media [^1]. Within the context of new media in particular, these processes are much more important due to them being, at their base, digital in nature when it comes to content generation, distribution, and consumption [^2].  Encoding involves the conversion of information into a digital format optimized for transmission or storage, while decoding translates that encoded information back into a comprehensible form, the inverse [^3].

Encoding is fundamental in new media as it facilitates the digitization of various content and mediums, including textual, visual, auditory, and video data [^4]. This digitization enables efficient storage, manipulation, and transmission of media across digital networks [^5]. For instance, video encoding algorithms, such as H.264/AVC or H.265/HEVC, compress raw video files by exploiting spatial and temporal redundancies, thus reducing the computational demand for a given task over the internet. Without efficient encoding techniques, bandwidth requirements for transmitting such data would be inconveniently high, rendering real time streaming applications impractical[^6].

On the flip side, decoding represents the process by which digital devices and software applications interpret the already encoded information to render it back into a human usable format. This encompasses displaying images on a screen, reproducing audio through speakers, or rendering text in a readable font as opposed to zeros and ones. The quality of the decoding directly impacts the user experience, as it determines the fidelity and accuracy with which the original information is reconstructed then interpreted [^7].

One example of encoding and decoding in new media is the utilization of the Advanced Audio Coding (AAC) format for digital audio. AAC, standardized as part of the MPEG-4 suite, employs psychoacoustic modeling to selectively discard perceptually irrelevant audio information, hence achieving smaller file sizes compared to older formats like MP3 while maintaining superior audio quality. When an AAC-encoded audio file is played, a decoder reconstructs the audio signal based on the encoded data to make actual sound, enabling users to listen to high-fidelity music on portable devices with limited space and streaming platforms [^8].

Encoding and decoding are intricately linked to contemporary technologies and media practices, including streaming media services, digital broadcasting, and online gaming as they rely on the encoding algorithms to deliver high quality content over limited networks [^9]. The growth of social media platforms and content has further amplified the demand for accessible encoding tools that empower individuals to create and spread their own media, encouraging the development of softwares and mobile apps that streamline the encoding process. Perhaps an easy example would be the ability to encode a video, uploading to Youtube, thus disseminating your own content publicly on the site. 

To finish, encoding and decoding represent foundational processes that support the functionality of new media. They shape our interactions with technology and our participation in digital life. Thus, an understanding of encoding and decoding principles is essential for navigating the intricacies of the digital world.

![Encoding-Decoding](images/encodingdecoding-abdulaziz.png)
*Initial images made in Ideogram, controlnet & img2img done with Fluxdev in ComfyUI, variations and fine tuning done with SDXL in Fooocus. Prompt example (variations used): “A horizontally split image illustrating the concept of audio encoding and decoding. On the left side, a clean and smooth waveform in blue, representing the original sound. In the center, the waveform becomes distorted, pixelated, or fragmented, symbolizing the encoding process where data is compressed or altered. The distortion includes binary numbers, glitch effects, or pixel breakdowns. On the right side, the waveform is restored to its original smooth form, signifying successful decoding. Dark background, subtle neon highlights, futuristic, digital aesthetic.” Negative Prompt (variations used): “red, pink, people, person, cube, anime”*

[^1]: Cover, Thomas M., and Joy A. Thomas. 2006. *Elements of Information Theory*. 2nd ed. Wiley-Interscience.

[^2]: Manovich, Lev. 2001. *The Language of New Media*. MIT Press.

[^3]: Haykin, Simon. 2001. *Communication Systems*. 4th ed. John Wiley & Sons.

[^4]: Salomon, David. 2007. *Data Compression: The Complete Reference*. 4th ed. Springer.

[^5]: Gillespie, Tarleton. 2018. *Custodians of the Internet: Platforms, Content Moderation, and the Hidden Decisions That Shape Social Media*. Yale University Press.

[^6]: Wiegand, Thomas, Gary J. Sullivan, Gisle Bjøntegaard, and Ajay Luthra. 2003. "Overview of the H.264/AVC Video Coding Standard." *IEEE Transactions on Circuits and Systems for Video Technology* 13 (7): 560-576.

[^7]: Steinmetz, Ralf, and Klara Nahrstedt. 2002. *Multimedia Computing, Communications and Applications*. Prentice Hall.

[^8]: Bosi, Marina, and Richard E. Goldberg. 2003. *Introduction to Digital Audio Coding and Standards*. Springer.

[^9]: Jenkins, Henry. 2006. *Convergence Culture: Where Old and New Media Collide*. New York University Press.

## Facial Recognition 

Facial recognition is a biometric technology that identifies or verifies individuals by analyzing facial features. It captures and processes facial data to convert it into a digital format for comparison using specialized algorithms.[^zhao03face]

The development of facial recognition dates to the 1960s when computational techniques for feature extraction were first explored.[^adjabi20past] One of the breakthroughs came with the introduction of "Eigenfaces", which applied principal component analysis (PCA) to improve recognition efficiency.[^adjabi20past] Over time, facial recognition methods have evolved into three major categories: holistic matching methods, feature-based methods, and hybrid methods.[^zhao03face] The integration of artificial intelligence (AI), particularly deep learning and convolutional neural networks (CNNs), continues to improve the accuracy and speed of facial recognition systems,[^adjabi20past] which makes them crucial in contemporary applications such as smartphones and social media platforms.

Early facial recognition methods, like Principal Component Analysis (PCA) and Local Binary Patterns (LBP), faced challenges such as lighting variations, changes in pose, and occlusions.[^jafri09survey] These issues are now addressed by more advanced techniques like 3D facial recognition (which captures depth information for more accurate results), video-based recognition (which tracks facial motion across frames), and Infrared (IR) recognition (which contributes to better performance in low-light conditions). However, challenges persist, such as misidentification, motion blur, and the impact of accessories like eyeglasses.[^jafri09survey]

Today, facial recognition technology is widely used in security, authentication, and law enforcement. Governments and private companies employ it for border control, criminal investigations, and user authentication in smartphones and banking applications.[^jafri09survey] Facial recognition also lies in new media practices. For example, video indexing (labeling faces in video),[^jafri09survey] and Apple’s iPhone Face ID (enhancing security and user experience). However, its widespread adoption has raised privacy and ethical concerns, especially regarding personal privacy, mass surveillance, and algorithmic bias.[^raji20saving] Issues such as misidentification and discriminatory outcomes have led to increased scrutiny and regulatory measures to ensure responsible use.[^smith20ethical] As technology advances, efforts are being made to improve facial recognition accuracy while addressing privacy and ethical concerns.

![Facial recognition](images/facialrecognition-wong.jpg) 

Background image taken from[^buolamwini18gender]

[^zhao03face]: Zhao, Wenyi, Rama Chellappa, P. Jonathon Phillips, and Azriel Rosenfeld. 2003. "Face recognition: A literature survey." _ACM computing surveys (CSUR)_ 35, no. 4 : 399-458.https://dl.acm.org/doi/abs/10.1145/954339.954342?casa_token=NbvTsfTR4YgAAAAA:v5ygUeXDYzUq6-5A-1j-3SNsrVFQNntIBnss8bkaB5K4MCBGn4LiDMU6AtVX0TP0XVGQDwCBhLJORQ

[^adjabi20past]: Adjabi, Insaf, Abdeldjalil Ouahabi, Amir Benzaoui, and Abdelmalik Taleb-Ahmed. 2020. "Past, present, and future of face recognition: A review." _Electronics_ 9, no. 8 : 1188. https://doi.org/10.3390/electronics9081188

[^jafri09survey]: Jafri, Rabia, and Hamid R. Arabnia. 2009. "A survey of face recognition techniques." _journal of information processing systems_ 5, no. 2 : 41-68. https://koreascience.kr/article/JAKO200920237949770.page

[^raji20saving]: Raji, Inioluwa Deborah, Timnit Gebru, Margaret Mitchell, Joy Buolamwini, Joonseok Lee, and Emily Denton. 2020.  "Saving face: Investigating the ethical concerns of facial recognition auditing." In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, pp. 145-151. https://dl.acm.org/doi/abs/10.1145/3375627.3375820 

[^smith20ethical]: Smith, Marcus, and Seumas Miller. 2022. "The ethical application of biometric facial recognition technology." _Ai & Society_ 37, no. 1 : 167-175. https://link.springer.com/article/10.1007/S00146-021-01199-9

[^buolamwini18gender]: Buolamwini, Joy, and Timnit Gebru. 2018. "Gender shades: Intersectional accuracy disparities in commercial gender classification." In _Conference on fairness, accountability and transparency_, pp. 77-91. PMLR.

## Fan/Fandom

The idea of groups of individuals bonding over a shared interest and/or support of something (whether this be music, a sports team, or virtually anything else) and then creating what is almost their own cultures around said media, seems almost instinctual. Individuals within these spaces are ‘fans’, whereas the social contexts they exist within are often referred to as ‘fandoms’. However now these terms are most often associated with fan spaces surrounding music and media such as movies and TV series. Francesca Copa explains in her essay “A Brief History of Media of Media Fandom”, that the shift from mostly sports to media happened in the 1960s, with Star Trek in 1966 being widely regarded as the first media fandom [^Coppa43]. Although some might perceive these spaces as being frivolous distractions from everyday life, they are often also attributed with fostering environments that support a particularly strong sense of creative expression, community and identity. In fact, one of the most popular forms of medias to come out of ‘fandom’ spaces is the ‘fanzine’, a non-profit, often volunteer-based collection of articles, artwork and other fan-created content in the form of a magazine for other fans [^Güldenpfennig5]. This is just one example of the culture that develops in these spaces, where labour is most often rewarded only with the satisfaction of being part of the community. This sense of identity sometimes goes even further, as is explained in “Teenagers, fandom and identity” from Persona Studies, “the process of identity construction takes place in the framework of a community; [...] it is inseparable from the interaction of the self with others [..] and [...] the celebrity persona. [^LacasaFuentGarciaPerniaCortes53]. Because of the unique levels of dedication and passion associated with the ‘fandom’ culture we know today, it is important to hold it in high importance when having conversations on modern culture, youth and counterculture. Furthermore, social media has only skyrocketed the amount of information that can be shared and circulated within these fan spaces, creating enhanced sense of community as well as community discourse. As online spaces continue to develop, so will fan spaces. These developments will have major impacts (for better or for worse) on the youths who interact within these spaces within the context of  contemporary social platforms, misinformation and exposure.

![Fan+Fandom](images/Fandom-Axiuk.png)
*Tool: [DALL-E through Microsoft Copilot]([https://stablediffusionweb.com](https://copilot.microsoft.com/chats/RL7FcKmpTy5EY5XCPMsuy)). Prompt: “a group of people feeling connected through their shared love of a band”*



[^Coppa43]: Coppa, Francesca, 2006.  “A Brief History of Media Fandom.” Essay. In Fan Fiction and Fan Communities in the Age of the Internet: New Essays, 41–55. Jeffersen, North Carolina: McFarland.

[^Güldenpfennig5]: Güldenpfennig, Peter, 2011. “Fandom, Fan Fiction and the Creative Mind”. Tilburg University, Netherlands.


[^LacasaFuentGarciaPerniaCortes53]:Lacasa, Pilar, Julian de la Fuente, Maria Garcia-Pernia, and Sara Cortes, 2017. “Teenagers, Fandom and Identity.” Persona Studies 3, no. 2 : 51–65. https://search.informit.org/doi/10.3316/informit.956274108483142.

## FinTech

FinTech stands for "financial technology" and refers to the use of technology to help the efficiency and accessibility of financial institutions. Cryptocurrency is a leading aspect of the concept, but so are online banking and online lending platforms.[^schueffel]
FinTech has reshaped financial interactions within digital spaces. It has redefined how people handle transactions, shifting from physical cash exchanges to contactless payments, mobile wallets, and decentralized currencies. This transformation has democratized access to financial services, allowing many individuals with internet access and smart devices to participate in those technologies.[^arner] However, the move towards cash-free economies has raised concerns regarding financial exclusion, particularly for vulnerable individuals who rely on physical currency to get by.
Many technological inventions have affected FinTech. For example, artificial intelligence makes automated investment tactics possible with AI advisors. In addition, new data analytics provide credit scoring and risk assessment. Blockchain technology offers a new, decentralized way to complete transactions. These technologies all add to the emergent FinTech world and are also changing wider digital culture. Many conversations regarding cryptocurrency, decentralized banking, and economic independence are spreading on social media[^zohar]. Many platforms are also now looking to incorporate FinTech within their features.
Being a rapidly growing phenomenon, many prominent examples of FinTech exist. As previously mentioned, mobile payment services like PayPal and Apple Pay have revolutionized consumer spending habits. Many transactions have shifted to an online format. Decentralized currencies powered by blockchain technology have also changed how people invest and exchange money. The 'Bitcoin boom' has especially disrupted conventional notions of currency and investment, becoming a trending topic worldwide.[^Nakamoto]
The term "FinTech" gained momentum in the early 1990s when Citicorp pioneered the Financial Services Technology Consortium, but gained popular usage in the 2010s when tech companies and startups began to change established financial service business models. Scholars such as Douglas Arner have examined its evolution, noting its phases of development from early financial digitalization to contemporary blockchain-based systems.[^arner] The impact of FinTech continues to evolve, with debates in relation to its impact on financial access, security, and regulatory concerns.


[^schueffel]: Schueffel, Patrick. 2017. "Taming the Beast: A Scientific Definition of Fintech." *Journal of Innovation Management* 4 (4): 32-54. https://doi.org/10.24840/2183-0606_004.004_0004.

[^arner]: Arner, Douglas W., Janos Barberis, and Ross P. Buckley. 2015. "The Evolution of Fintech: A New Post-Crisis Paradigm?" *University of Hong Kong Faculty of Law Research Paper* 2015/047. https://doi.org/10.2139/ssrn.2676553.

[^zohar]: Zohar, Aviv. 2015. "Bitcoin: Under the Hood." *Communications of the ACM* 58 (9): 104-113. https://doi.org/10.1145/2701411.

[^Nakamoto]: Nakamoto, Satoshi. 2008. "Bitcoin: A Peer-to-Peer Electronic Cash System." https://bitcoin.org/bitcoin.pdf.

## Firewall

Most people think of a firewall as a network security system that detects and controls both incoming and outgoing data.
However, that is just a basic understanding. In reality, there are multiple types of firewalls, each designed to prevent malware,
viruses, and other malicious activities in different ways and for specific purposes. This discussion will focus on three main types:
Packet Filtering Firewalls, Proxy Firewalls, and Next-Generation Firewalls.

A Packet Filtering Firewall follows the most fundamental approach. It inspects data packets—small units of information that networks 
exchange—and determines whether to allow or block them based on predefined rules (e.g., IP addresses). While this method is fast and efficient,
it does not inspect the packet’s payload (the actual message), making it less secure.[^Noonan06Firewall]

A Proxy Firewall, in contrast, acts as an intermediary between users and external services, filtering data at the application layer. 
By analyzing full requests before forwarding them, it provides a higher level of security compared to traditional firewalls.[^DeCarlo24types]

Finally, a Next-Generation Firewall (NGFW) combines standard packet filtering with deep packet inspection—meaning it can analyze packets
more thoroughly than previous firewall types, identifying hidden threats.[^DeCarlo24types]

As new media technologies rapidly evolve, users interact with digital content such as applications, social media, and cloud services more 
than ever before. While this expands access to information and connectivity, it also increases exposure to cyber risks. Firewalls play a 
crucial role in protecting networks from external threats, but they do not prevent companies from collecting user data. Many digital platforms 
actively gather and analyze user information, often without users being fully aware of the extent of data tracking. As legal scholar Teresa Scassa
explains in Privacy and Social Media in the Age of Big Data, companies use data profiling to predict consumer behavior, determine service eligibility 
(such as insurance or loans), and even apply price discrimination based on personal data.[^Dusseault13privacy]This means that almost every online interaction leaves
a digital footprint, which can be stored, analyzed, and sometimes exploited.

The challenge is that while firewalls protect networks from external attacks, they do not regulate how platforms handle user data. Social media, 
online marketplaces, and other web-based services continuously collect, store, and monetize personal information, raising concerns about privacy, 
security, and data ownership. The increasing interactivity between users and online platforms makes data more vulnerable to exposure, especially 
as people share personal details, preferences, and behaviors. While firewalls help prevent unauthorized access, they are not a complete solution 
to digital privacy risks. To truly safeguard personal data, users must combine strong cybersecurity measures with awareness of how their data is 
being collected and used. This includes practices such as adjusting privacy settings, using encrypted communication, and being cautious about sharing 
personal information online.

Firewalls remain an essential first line of defense against cyber threats, preventing unauthorized access and filtering harmful traffic. 
However, in the modern digital landscape, cybersecurity is only one part of the equation. Protecting personal data also requires understanding 
how platforms collect, store, and use information. As new media technologies continue to evolve, the need for both stronger security measures 
and better privacy protections will only grow.

[^Noonan06Firewall]: Noonan, Wesley J., and Ido Dubrawsky. 2006. _Firewall Fundamentals_. Indianapolis, IN: Cisco Press.

[^DeCarlo24types]:DeCarlo, Amy Larsen. 2024. _The 5 Different Types of Firewalls Explained_. TechTarget, August 15, 2024. https://www.techtarget.com/searchsecurity/feature/The-five-different-types-of-firewalls.

[^Dusseault13privacy]:Dusseault, Pierre-Luc, and Canada. Parliament. House of Commons. Standing Committee on Access to Information, Privacy and Ethics. 2013. _Privacy and Social Media in the Age of Big Data: Report of the Standing Committee on Access to Information, Privacy and Ethics_. Ottawa: House of Commons.


## Futurism

"Futurism" is an art movement, though it could be argued to be more than that due to its ideals.
This movement was started in Italy around 1909, and was at its most popular from then until some time in the 1920's, thought it is reported to have "ended" around 1944. [^unknown_accessed_2025_artstory]
The creation, and much of the fire behind the movement, is largely attributed to an Italian poet named Filippo Tommaso Marinetti, who published a text titled "Manifesto of Futurism" in 1909, seemingly sparking much of the movement.

Futurism was primarily driven by a dissatisfaction with the nostalgia much of the country held for classical art and history, and identified with speed, efficiency, and promoting chaotic changes in life.  
Many Futurists saw tradition as lifeless and no longer relevant. They believed that classical media only served to keep society chained to outdated ways of thinking. [^filippo_marinetti_1909_manifesto]
Much of this thinking was shown through their dislike of romanticism, with them blaming sentimentality and the glorification of a rural life as the reason for a world feeling stagnant.

Much of this movement was inspired by the Industrial Revolution, which had quickly transformed society with machines, electricity, and new forms of transportation. [^ekin_pinar_2024_speed]
Futurists looked at these creations as a beacon of efficiency and beauty in a world that had lost itself in its past. 
Many of them favored gaining inspiration from environments such as factories and industry.

One of the most controversial view that was / is held by a lot of the Futurist community is that violence and conflict are cleansing forces meant to purify the world.
Think of this view as creating an artificial river from a pond, draining the stagnant water and creating new inspiration for innovation in its wake.
Marinetti referred to war as a form of hygiene, a destruction that was necessary for rebirth of idea and innovation.

### Relation to New Media and Modern Definition

While Futurism itself isn't considered a very popular movement by todays standards, its ideas and reasoning greatly connects to the world we find ourselves in today.
One example is its worship of speed and constant efficiency. Much of todays technology and especially the internet operates through its instantanous responses, with social media, viral trends, and digital content constantly spreading at high speeds.
On the daily, we see trends rise and fall from fame as the chaotic speed that our creations evolve at give more and more for us to see while leaving us less time to do it.

Another idea that clearly mirrors Futuristic ideals is our rejection of the past.
While Futurists of said past might have called classical art and tradition obsolete, the society of today simply disrupts traditional forms of media by digitalizing anything and everything.
Simply put, the internet prioritizes the now over the past.

In an interesting twist of events, the events of the now seem to have some similarities to those seen during the origin of Futurism, with many finding the current world a bore, and tensions build off of the social unrest gathering throughout several large countries.

I would define modern Futurism as a philosophical movement that is characterized by innovation, technology, and chaos, while challenging more traditional values and conventions.

[^unknown_accessed_2025_artstory]: Unspecified. 2025. _Futurism Movement Overview._, The Art Story.
[^filippo_marinetti_1909_manifesto]: Marinetti, Filippo T. 2016. _The Manifesto of Futurism_. Passerino Editore.
[^ekin_pinar_2024_speed]: Pınar, Ekin. 2024. _Becoming-Speed: Futurism’s Conflicts with Subjectivity and Motion._ pp. 115. Art Vision.




## Geotagging

**Geotagging** is a technology that involves attributing geolocational information to different forms of media[^longley-geographic]. This is done by gathering the data through a devices positional services and storing it in the medias metadata. Geotagging started taking prominence in the early to mid 2000s with the rise of cell phones due to their photo-taking abilities and integrated GPSs. 


Social media services were some of the first to adopt the technology. Though it took some time, these services allowed users to share both pictures and their locations with additional visualization as seen with [Snapchat](https://www.snapchat.com/). Geotagging has also been widely used in specific scientific fields, where researchers use it to more easily catalogue massive amounts of information. [^nowak-natural] 


Due to the additional data, many privacy experts have warned about the increased use of geotagging. This includes a short period in time where celebrities would unknowingly publicize the location of their home by sharing pictures online.[^abc-celbrities] Geotagging is a feature that is automatically enabled on most devices in the present day, but users are unlikely warned.[^lipschultz-social] This means most users are providing sensitive information for their digital footprint without notice, increasing their digital vulnerability.

[^longley-geographic]: Longley, Paul. 2015. *Geographic Information Science & Systems*. Fourth edition. Hoboken, NJ: John Wiley & Sons. https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=1639301.

[^nowak-natural]: Maciej M. Nowak, Katarzyna Słupecka, and Bogdan Jackowiak. 2021. “Geotagging of Natural History Collections for Reuse in Environmental Research.” *Ecological Indicators* 131: 108131.

[^abc-celbrities]: ABC News. 2010. “Celebrities’ Photos, Videos May Reveal Location.” *ABC News*, July 14, 2010. Accessed March 8, 2025. https://abcnews.go.com/Technology/celebrity-stalking-online-photos-give-location/story?id=11162352.

[^lipschultz-social]:Lipschultz, Jeremy Harris. 2024. *Social Media Communication : Concepts, Practices, Data, Law and Ethics*. Fourth edition. New York: Routledge, Taylor & Francis Group. https://www.oreilly.com/library/view/-/9781000910278/.

## Gesture Recognition

 Gesture recognition is a technology that interprets human movements, primarily hand gestures, using computer vision and machine learning algorithms. This process transforms physical actions into digital commands, enabling natural, non-verbal interaction with digital systems.[^escalera17gesture]

### Significance in New Media
 Gesture recognition redefines user interaction paradigms in digital environments. Its significance lies in:
- **Natural Communication:** Allowing users to communicate with technology through intuitive body movements, which enhances immersion in digital and virtual experiences.
- **Cultural and Social Impact:** Facilitating touchless interfaces broadens accessibility and inclusivity, transforming how digital content is experienced and consumed.[^mohamed24realtime]

### Relation to Current Technologies and Media Practices 
This technology is deeply integrated into today's hardware and software ecosystems, for example: 
- **Hardware Integration:** This involves using devices like depth-sensing cameras (e.g., Microsoft Kinect) and specialized sensors in VR/AR systems for real-time gesture capture.
- **Software and Algorithms:** Incorporation of advanced methods, from traditional computer vision to modern deep learning approaches, that enable accurate, real-time gesture detection.
- **Practical Applications:** Deployment in gaming, smart home controls, and assistive technologies streamlines user interfaces and enhances interaction efficiency. [^oudah20computer] [^kadem19survey]

### Brief History
#### Evolution:
Initially, computer vision research focused on static image analysis. Over time, the field expanded to encompass dynamic gesture interpretation.[^escalera17gesture] Advances in real-time processing, increased computational power, and refined algorithms have enabled the shift from simple image analysis to sophisticated systems that capture continuous, nuanced human gestures.[^oudah20computer]
#### Milestones:
Pivotal breakthroughs, such as the introduction of depth-sensing technology (e.g., Microsoft Kinect), revolutionized gesture recognition. These innovations provided accurate three-dimensional data, significantly enhancing precision and enabling broader applications in interactive gaming, virtual reality, and real-time human-computer interaction.[^mohamed24realtime] [^kadem19survey]

### Examples
- **Interactive Gaming and VR/AR:** Systems like Microsoft Kinect and Leap Motion translate bodily gestures into gameplay commands, setting industry standards for immersive, controller-free experiences.[^mohamed24realtime]
- **Gesture Recognition Toolkits:** Open-source libraries support the rapid development of gesture-based applications, fostering innovation and customization.[^escalera17gesture]
- **Algorithmic Foundations:** Techniques such as neural networks, hidden Markov models, and fuzzy clustering underpin modern gesture recognition systems, demonstrating the technology’s evolution and adaptability.[^escalera17gesture]

[^escalera17gesture]: Escalera, Sergio, Isabelle Guyon, and Vassilis Athitsos. 2017. Gesture Recognition. the Springer Series on Challenges in Machine Learning. Springer International Publishing. https://doi.org/10.1007/978-3-319-57021-1.
[^mohamed24realtime]: Mohamed, Aws Saood, Nidaa Flaih Hassan, and Abeer Salim Jamil. 2024. “Real-Time Hand Gesture Recognition: A Comprehensive Review of Techniques, Applications, and Challenges.” Cybernetics and Information Technologies 24 (3): 163–81. https://doi.org/10.2478/cait-2024-0031.
[^oudah20computer]:Oudah, Munir, Ali Al-Naji, and Javaan Chahl. 2020. “Hand Gesture Recognition Based on Computer Vision: A Review of Techniques.” Journal of Imaging 6 (8): 73. https://doi.org/10.3390/jimaging6080073.
[^kadem19survey]: Kadem Hamed Al-Saedi, Ahmed, and Abbas H Hassin Al-Asadi. 2019. “Survey of Hand Gesture Recognition Systems.” Journal of Physics: Conference Series 1294 (September): 042003. https://doi.org/10.1088/1742-6596/1294/4/042003.

## Git

Git is a free and open-source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. It allows multiple developers to work on a project simultaneously without interfering with each other's contributions.[^chacon14progit]

Git was created by Linus Torvalds in 2005. The name ”Git” is British slang for a foolish or worthless person, which Torvalds humorously acknowledged, stating, “I'm an egotistical bastard, and I name all my projects after myself. First 'Linux', now 'git'.”[^torvalds05kernel]

In new media studies, Git is significant because it facilitates collaborative content creation and management. Its distributed nature allows creators from various locations to contribute to a project, promoting decentralized and collaborative workflows. This aligns with the principles of new media, which emphasize participatory culture and collective intelligence.[^jenkins06convergence]

Git underpins many modern development workflows and platforms. Services like [GitHub](../main/glossary.md#github), GitLab, and Bitbucket build upon Git to provide hosting, collaboration tools, and project management features. These platforms are integral to open-source projects, content management systems, and even digital art collaborations, reflecting Git's broad applicability in current media practices.

A paradigmatic example of Git use is the Linux Kernel Development. Git was initially developed by Linus Torvalds for managing the Linux kernel's source code. The original thread for defining the protocol is [still available](https://lkml.org/lkml/2005/4/6/121).



[^chacon14progit]: Chacon, Scott, and Ben Straub. 2014. _Pro Git_. 2nd ed. New York: Apress.

[^jenkins06convergence]: Jenkins, Henry. 2006. _Convergence Culture: Where Old and New Media Collide_. New York: New York University Press.

[^torvalds05kernel]: Torvalds, Linus. 2005. "Re: Kernel SCM saga..." _Linux Kernel Mailing List (LKML)_. April 7. https://lkml.org/lkml/2005/4/7/11.

## GitHub
GitHub is a web-based platform for version control and collaboration using Git,[^github24wikis] allowing users to manage and share code in various repositories and offering wikis for documentation and collaborative editing. GitHub employs Git software, offering distributed version control, access control, bug tracking, feature requests, task management, continuous integration, and project [wikis](../main/glossary.md#wiki).[^kinsta24what]

As of 2020, GitHub is the most important platform for collaborative Free/Libre Open Source Software (FLOSS) development, with 31 million user accounts and over 100 million code repositories.[^zoller20topology] FLOSS development is associated with the idea of commons-based peer production, presenting it as a different approach to creating goods and services compared to traditional market or hierarchical methods. It is seen as a method for producing without the usual power imbalances.

GitHub's development process is distributed, allowing developers to collaborate, discuss, and comment on code from various locations. This platform generates a wealth of textual, numerical, and collaborative data, making it a prime source for software engineering research.[^seker20open]

[^seker20open]: Seker, Abdulkadir, Banu Diri, Halil Arslan, and Mehmet Fatih Amasyalı. 2020. “Open Source Software Development Challenges: A Systematic Literature Review on GitHub.” In _International Journal of Open Source Software and Processes (IJOSSP)_ 11, no. 4: 1-26. 

[^zoller20topology]: Zöller, Nikolas, Jonathan H. Morgan, and Tobias Schröder. 2020. “A Topology of Groups: What GitHub Can Tell Us about Online Collaboration.” In _Technological Forecasting and Social Change_ 161: 120291​.

[^github24wikis]: GitHub Docs. 2024. “About wikis.” Accessed January 2. https://docs.github.com/en/communities/documenting-your-project-with-wikis/about-wikis.

[^kinsta24what]: Kinsta. 2024. ”What Is GitHub? A Beginner's Introduction to GitHub.” Accessed January 2. https://kinsta.com/knowledgebase/what-is-github/.

## GPU

A Graphics Processing Unit (GPU) is a specialized electronic circuit designed to accelerate image rendering and complex computational tasks. Originally developed for graphics rendering in video games, GPUs have evolved into powerful parallel processors widely used in artificial intelligence, scientific computing, and data analysis.[^fatahalian08gpu] Their ability to process multiple calculations simultaneously makes them essential for modern computing.

The concept of GPUs emerged in the late 20th century as computer graphics became more advanced. In the 1990s, companies like NVIDIA and ATI (now part of AMD) pioneered dedicated graphics hardware to offload rendering tasks from the CPU.[^owens07survey] The introduction of programmable shaders in the early 2000s marked a significant shift, allowing developers to harness GPU power beyond graphics. The General-Purpose computing on GPUs (GPGPU) movement further expanded their use into scientific research, machine learning, and blockchain technology.[^kirk10gpu]


In the field of new media, GPUs play a crucial role in real-time rendering, video processing, and interactive digital environments. They enable photorealistic graphics, virtual reality (VR), augmented reality (AR), and high-performance computing applications.[^pharr19physically] The ability of GPUs to process large amounts of data efficiently aligns with contemporary digital media practices, from 3D animation to deep learning models used in content generation. Moreover, platforms such as Unreal Engine and Unity leverage GPU acceleration to create immersive experiences.


Beyond gaming and media production, GPUs have become indispensable in scientific visualization, cryptocurrency mining, and artificial intelligence research.[^goodfellow16deep] Frameworks like CUDA (developed by NVIDIA) and OpenCL enable programmers to utilize GPU parallelism for deep learning, financial modeling, and real-time simulations. Cloud computing services now offer GPU instances, further integrating them into large-scale data processing.


Several key figures have contributed to GPU development. NVIDIA co-founder Jensen Huang played a significant role in advancing GPU technology and promoting its use in AI.[^nvidiahistory] The academic work of researchers like John D. Owens and David Kirk has also been influential in optimizing GPU performance for non-graphics applications.

### External References  
- NVIDIA’s official documentation on CUDA: [https://developer.nvidia.com/cuda-zone](https://developer.nvidia.com/cuda-zone)  
- Khronos Group’s OpenCL specifications: [https://www.khronos.org/opencl/](https://www.khronos.org/opencl/)  
- Unreal Engine’s GPU rendering pipeline: [https://docs.unrealengine.com/](https://docs.unrealengine.com/)  

[^fatahalian08gpu]: Fatahalian, Kayvon, and Mike Houston. 2008. "A Closer Look at GPUs." _Communications of the ACM_ 51(10): 50–57.  
[^owens07survey]: Owens, John D., et al. 2007. "A Survey of General-Purpose Computation on Graphics Hardware." _Computer Graphics Forum_ 26(1): 80–113.  
[^kirk10gpu]: Kirk, David B., and Wen-mei W. Hwu. 2010. _Programming Massively Parallel Processors: A Hands-on Approach_. Morgan Kaufmann.  
[^pharr19physically]: Pharr, Matt, Wenzel Jakob, and Greg Humphreys. 2019. _Physically Based Rendering: From Theory to Implementation_. Morgan Kaufmann.  
[^goodfellow16deep]: Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. _Deep Learning_. MIT Press.  
[^nvidiahistory]: NVIDIA. 2021. "The History of GPU Innovation." NVIDIA Blog. https://blogs.nvidia.com/  

## GUI

GUI (Graphical User Interface) is a way for users to interact and communicate with computers. Unlike the Command Line Interface (CLI), the GUI does not display code details. Instead, it facilitates human-computer interaction through graphical elements like windows, icons, and buttons.[^Shardeum23What]

Vannevar Bush's 1945 paper, "As We May Think," seems to have had a significant influence on the prototype of the GUI. The paper introduced the concept of an electronic device called Memex, which integrated various functionalities to aid human thought processes.[^Vannevar45As]

Today, GUI has evolved to offer a wide range of functionalities to meet users' operational needs across various devices and applications. All operating systems, software applications, and websites now incorporate GUI elements to enhance user experience.[^priya24Evolution]



### Examples
![Xerox](images/GUI_Xerox_Star_8010.jpg)[^XeroxWiki74]  

Xerox_Star_8010 GUI

![Macintosh](images/GUI_Macintosh.jpg)[^MacintoshZednet84]  

Macintosh GUI

![Windows](images/GUI_Windows1.0.png)[^WindowWiki85]  

Windows1.0 GUI



[^Wendy11Graphcal]: Wendy L. Martinez, "Graphical user interfaces" in *Wiley Interdisciplinary Reviews: Computational Statistics*, v3n2 (March/April 2011): 119-133, https://wires.onlinelibrary.wiley.com

[^Shardeum23What]: Shardeum Content Team, "GUI (Graphical User Interface) vs. CLI (Command-Line Interface)," *shardeum*, July 23, 2023, "5.1 GUI vs. CLI," accessed January 29, 2025, https://shardeum.org

[^Vannevar45As]: Bush, Vannevar. "As We May Think", *The Atlantic Monthly*, July 1945. https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/

[^Wayne98Computer]: Blinn, James E. "16.1 Xerox PARC" in *Computer Graphics and Computer Animation: A Retrospective Overview*. New York: ACM Press, 1998.

[^priya24Evolution]: Priya Patel. "The Evolution and Impact of Graphical User Interfaces (GUIs)", *Medium*, January 21, 2024, accessed January 29, 2025, https://medium.com/@learning3601/in-the-vast-lan

[^XeroxWiki74]: Wikipedia. 2025. *Xerox Star* Last modified January 4, 2025. https://en.wikipedia.org/wiki/Xerox_Star

[^MacintoshZednet84]: Zdnet. 2009. *The Mac at 25: GUI battles in business* Last modified January 23, 2009. https://www.zdnet.com/article/the-mac-at-25-gui-battles-in-business/

[^WindowWiki85]: Wikipedia. 2025. *Windows 1.0* Last modified January 26, 2025. https://en.wikipedia.org/wiki/Windows_1.0

## Haptics


"Haptic" technology is a long researched technology that aims to enhance user interactions by providing tactile feedback through various devices, creating a more immersive and responsive experience.
These devices can be practically anything, though some commonly known haptic devices are often smartphones, game controllers, VR systems, and even prosthetics.
"Haptics" often use mechanical (vibration, pressure, motion), and / or more non-traditional (ultrasonics, electrostatics, thermal) stimulations to create the illusion of a physical touch in virtual, or at least seperate, environments. [^yuxiang_shi_2024_feedback]

These technologies can be tracked to have originated in the mid 19th century, with some of the earliest uses of haptics having been recorded to have occured in the 1960s and 70s.
One of the oldest representations of the use of haptics was a system that was meant to help show images to the blind. 
It was created by neuroscientist Paul Bach-y-Rita, and took the shape of a chair that an array of movable metal rods. [^paul_bach-y-rita_1969_sense]
These rods would be gently pushed into the back of the user, maintaining a certain shape, serving a similar purpose as do pixels on a screen.
Users were shown to be able to recognize pictures that were poked into their backs, essentially acting as a sensory substitution for eyesight.

Another particulary interesting case in the early use of haptics was the invention of "Aura Interactor" sensory feedback vest. [^mike_klasco_2024_aura]
The vest was designed to provide force-feedback for video games by converting low-frequency sound into vibrations, allowing a player to feel in-game impacts like punches or nearby explosions.
It contained a large vibrating motor that would respond to sounds that were heavy in bass, and had a few different physical sliders to modulate strength.

### Relation to New Media and Modern Definition

"Haptics" only seem to find themselves being used more and more as time goes on, likely due to humanity finding itself to always be so close to the digitalized technologies around them.

Smartphones have, for many years now, harnessed simple haptics by form of vibrations, often to signal to a user that their interaction has caused a change, or that their attention was required.
Most game controllers feature haptic feedback that simulates different levels of resistance in an environment.
Many ATM machines and card terminals use haptics to confirm button presses and transactions.

In virtual reality, haptics have entered a sort of gold rush.
A large variety of both new and established companies have taken to experimenting with different haptics technologies in an attempt to find a viable and more immersive form of haptics.
Many of the new devices created have been vests, gloves, and even masks, all utilizing experimental forms of haptics technologies such as:

- "Microfluidic Tactile Actuators" that use tiny, fluid-filled channels and chambers to simulate textures, pressure, and deformation
- Robotic Exoskeletons that lock in place around objects to simulate grip
- Electro-Muscle Stimulators that deliver small shocks to simulate a feeling of touch
- Scent Cartridges that are triggered to simulate virtual smells

This is very much not a comprehensive list, with many such technologies have similar results that are achieved in different ways.

Through these uses, haptics have shown themselves to be a sort of delivery system of digital information to the physical world.
In the modern sense I would define haptics as a technology and science of creating, simulating, and perceiving touch-based sensations through electronic devices.


![Haptics](/images/haptics_sean.png)
*Tool: [DALL-E Free AI Generator](https://www.dall-efree.com/). Prompt: “Create an image that visually represents the concept of haptic technology. Futuristic environment where a person is interacting with a variety of advanced devices. Person is wearing a VR headset with a haptic feedback vest and gloves. Include representations of the person interacting with holograms. The setting should have a high-tech, immersive atmosphere, with a sense of interaction between the digital and physical worlds. The overall tone should be sleek and futuristic.”*


[^yuxiang_shi_2024_feedback]: Shi, Yuxiang, and Guozhen Shen. 2024. _Haptic Sensing and Feedback Techniques toward Virtual Reality._ pp. 14–24. Research: A Science Partner Journal.
[^paul_bach-y-rita_1969_sense]: Bach-Y-Rita, Paul, Carter C. Collins, Frank A. Saunders, Benjamin White, and Lawrence Scadden. 1969. _Vision Substitution by Tactile Image Projection._ pp. 1-2. Nature.
[^mike_klasco_2024_aura]: Klasco, Mike. 2022. _Speakers and Transducers - Haptic Devices for Augmentation, Entertainment, and Wellness._ Audioxpress: Magazine Articles.

## Hardware 

Hardware can be defined as “the physical and electronic parts of a computer” as opposed to the “instructions it follows” which is also known as software. [^cambridge] 

Hardware can appear in many forms and devices. More notably, in a computer, the most common pieces are the motherboard, the central processing unit (CPU), the random-access memory (RAM), 
storage drives such as a hard drive (HDD) or solid-state drive (SSD), the graphics processing unit (GPU), power supply unit (PSU), heat sinks or fans, the protective case, and many more. [^crucial] [^Jerome15]

As computers became more and more popular, so have the rise in the manufacturing of hardware parts across the world. 
As such, there was a need to set standards for compatibility purposes but also to protect against counterfeit components. [^Guin19]
These counterfeit hardware devices can be categorized into different types: recycled, remarked, overproduced, defective, cloned, forged documentation, or tampered. [^Guin19]

Considering how hardware is not a single entity, the first piece of hardware cannot be attributed to just one component. 
Instead, we can give credit to some of the first types of hardware such as the “first computer mouse that was invented in 1963 by Douglas Engelbart”, 
or the first RAM which was attributed to An Wang and Jay Forrester in 1951. [^Pingdom19]

As for the word itself, the earliest known usage dates back to 1419 according to Oxford English Dictionary. [^oxford].
Hardware relates heavily to modern technologies as it is present in every piece of technology that we use such as phones, computers, and servers, which enables us to share information and media around the world.
With artificial intelligence getting more and more advanced, it will also require new hardware developed to be able to handle its power. 
Fortunately, tech companies such as NVIDIA and Intel are actively producing new components that are being tested with AI. [^Sipola22]


![Hardware](images/hardware.jpg)
*Tool: [OpenArt web](https://openart.ai/home). Prompt: “A GPU and computer components”*



[^cambridge]: “Hardware | English Meaning - Cambridge Dictionary.” Cambridge Dictionary. Accessed March 14, 2025. https://dictionary.cambridge.org/dictionary/english/hardware. 
[^crucial]: “What Is Computer Hardware? Definition & Examples.” Crucial. Accessed March 14, 2025. https://www.crucial.com/articles/pc-builders/what-is-computer-hardware. 
[^Jerome15]: Casey, Jerome. 2015. "Computer Hardware: Hardware Components and Internal PC Connections."
[^Guin19]: Guin, Ujjwal, Navid Asadizanjani, and Mark Tehranipoor. 2019. "Standards for hardware security." *GetMobile: Mobile Computing and Communications 23*, no. 1 (2019): 5-9.
[^Pingdom19]: 2019. “The History of PC Hardware, in Pictures - Pingdom.” pingdom.com,. https://www.pingdom.com/blog/the-history-of-pc-hardware-in-pictures/. 
[^oxford]: “Hardware, Noun. Meanings, Etymology and More.” *Oxford English Dictionary*. Accessed March 13, 2025. https://www.oed.com/dictionary/hardware_n?tab=factsheet#2065310. 
[^Sipola22]: Sipola, Tuomo, Janne Alatalo, Tero Kokkonen, and Mika Rantonen. 2022. "Artificial intelligence in the IoT era: A review of edge AI hardware and software." In *2022 31st Conference of Open Innovations Association (FRUCT)*, pp. 320-331. IEEE, 2022.

## Hashtag

The Hashtag functions as a navigational tool that allows you to instantly search for publications on topics of interest. It also serves as a powerful tool for marketing promotion in the
social network of a product or service.[^budnik19dynamic] A hashtag typically consists of a string of characters preceded by the pound symbol.[^caleffi15word] It is most commonly used as a method of sorting between various topics or categories within social media platforms. 

The person who invented and popularized the usage of the hashtag is credited to social designer Chris Messina, who produced the first ever hashtag on the social media platform Twitter (now X) in 2007. His first usage of it was when he posted a tweet reading: “how do you feel
about using # (pound) for groups. As in #barcamp?”.[^caleffi15word] However, the hashtag also appeared during the 1990s in the messaging service IRC (Internet Relay Chat) for marking message topics, similar to modern-day usage.[^budnik19dynamic]

Within new media studies, the hashtag plays a very important role. Hashtags are now used in a number of ways, very often as a contextual aside to comment on, give more
depth to, or somehow emphasize what has been said. It is a reliable method for looking up specific posts or terms and has been utilized as an effective method of advertising and promotion.[^caleffi15word]

The hashtag has a profound impact on our society today, especially with the continuous rise of social media. Many social media platforms such as Twitter and Instagram have pushed the usage of the hashtag into mainstream digital advertising. Not only is the hashtag a tool, it represents an integral element of contemporary communication.[^rauschnabel19motivate]

![Hashtag](images/hashtag_armstrong.png)

[^budnik19dynamic]: Budnik, Ekaterina, Violetta Gaputina, and Vera Boguslavskaya. 2019. "Dynamic of hashtag functions development in new media: Hashtag as an identificational mark of digital communication in social networks." _In Proceedings of the XI International Scientific Conference Communicative Strategies of the Information Society_ 1-5. Accessed January 29, 2025.
[^caleffi15word]: Caleffi, Paola-Maria. 2015. "The 'Hashtag': A new word or a new rule?" _SKASE journal of theoretical linguistics_ 12 (2): 46-69. Accessed January 29, 2025.
[^rauschnabel19motivate]: Rauschnabel, Philipp A., Pavica Sheldon, and Erna Herzfeldt. 2019. "What motivates users to hashtag on social media?." _Psychology & Marketing_ 36 (5): 473-488. Accessed January 29, 2025.

## Human-Computer Interaction (HCI)


Human-Computer Interaction (HCI) as a field of study is primarily concerned with the design, evaluation, and implementation of interactive computing systems for human use. According to John M. Carroll in his influential work "Human Computer Interaction - Brief Intro" (2009), HCI focuses on "understanding how people make use of devices and systems that incorporate or embed computation, and how such devices and systems can be more useful and more usable." Carroll describes HCI as fundamentally multidisciplinary, drawing from computer science, psychology, design, and numerous other fields to understand the relationship between humans and technology.

Harrison, Tatar, and Sengers take on a paradigmatic analysis (human factors, cognitive, or phenomenological), to show the evolution by which HCI as a field of study seeks to improve the relationship between humans and computational technologies through better understanding of user needs, capabilities, contexts, and experiences.

Human-Computer Interaction (HCI) has a significant relationship with New Media, as both fields examine the evolving interfaces between humans and technology, but from different perspectives.
According to Lev Manovich in "The Language of New Media" (2001), New Media studies focuses on the cultural and aesthetic dimensions of digital technologies, while HCI provides the underlying principles for how these technologies are designed for human use. Manovich argues that "the interface shapes how the user conceives of the computer itself" and "determines how users think of any media object accessed via a computer." This perspective highlights how HCI design principles directly influence the cultural experience of New Media.

The relationship between these fields is symbiotic: HCI provides frameworks for creating usable interfaces, while New Media studies examines the cultural, social, and artistic implications of these interfaces. As technologies evolve, both fields continue to inform each other - HCI practitioners increasingly incorporate cultural considerations from New Media studies, while New Media theorists analyze how interface design shapes digital culture and user experience.
This complementary relationship extends Harrison, Tatar, and Sengers' paradigmatic analysis by suggesting that the third paradigm of HCI (phenomenologically situated) particularly aligns with New Media's concern for cultural context and meaning-making in technological interactions.

The following diagram illustrates the three paradigms of Human-Computer Interaction as described by Harrison, Tatar, and Sengers in their influential paper:

1. **First Paradigm: Human Factors**
    - Focuses on engineering and ergonomics
    - Views the human as another component in the system
    - Aims to optimize the fit between humans and machines
    - Uses formal metrics and controlled lab studies
2. **Second Paradigm: Classical Cognitivism/Information Processing**
    - Treats the mind as an information processor (the mind-as-computer metaphor)
    - Focuses on modeling and improving communication between human and computer
    - Uses predictive models and controlled experiments
    - Values rationality and task completion
3. **Third Paradigm: Phenomenologically Situated**
    - Focuses on meaning and context (social, cultural, historical)
    - Views interaction as a situated phenomenon
    - Uses ethnographic and participatory design methods
    - Values embodiment, social context, and meaning-making
  
![Non-chronological_Concepts of the Three HCI Paradigms - Claude](https://github.com/user-attachments/assets/09ad9eaf-0c1d-46b4-93df-d63bfc8260f0)



## HTML

### Intro
The HyperText Markup Language (HTML) is the building block of the web that constitutes the backbone of all web pages. It is accompanied by CSS (Cascading Style Sheets) and JavaScript (JS) as one of the core web technologies of the World Wide Web. Whilst CSS takes care of styling and JavaScript takes care of interactivity, HTML establishes the structure that holds everything together.[^ebersbach01html]

HTML employs an element-based method of marking up content to arrange web content. Tags are what are used to mark different types of content like headings, paragraphs, images, links, and interactive elements like forms. HTML is one of the foundation web construction languages of the contemporary web and as such is the base language for any web applications or web-site construction. [^ebersbach02html] To a callback of the keyword "CSS" as the frosting of the cake web, HTML is consider the base of the cake of the web.


### A Brief History of HTML
HTML was developed in 1991 by Tim Berners-Lee as a way of formatting and linking web documents. HTML in the early days was not too complicated, with minimal tags for formatting the text and for linking one document to another. With the growth of the web came the growth of HTML.[^ebersbach01html]

- **HTML 2.0 (1995):** The first standardized version, introduced basic form elements. [^ebersbach01html]
- **HTML 3.2 (1997):** Brought table support and basic scripting.
- **HTML 4.01 (1999):** Expanded capabilities with improved styling and multimedia support.
- **XHTML (2000):** Introduced stricter syntax and compliance with XML rules.
- **HTML5 (2014 - Present):** Revolutionized web development with native support for multimedia, semantic elements, and APIs for modern web applications.

The most recent standard is HTML5, constantly updated to keep up with new web technologies and users' expectations. HTML has become essential for web platforms like Wikipedia and Google Docs applications where web-based interfaces and structured content are defining modern digital experiences. [^ebersbach03html]


### How to Structure HTML
At its core, an HTML document follows a standard structure:

```html
<!DOCTYPE html>
<html>
  <head>
    <title>My First Webpage</title>
  </head>
  <body>
    <h1>Welcome to My Website</h1>
    <p>This is a simple paragraph.</p>
  </body>
</html>
```


#### Breaking It Down
- **`<!DOCTYPE html>`**: Declares the document type and version of HTML.
- **`<html>`**: The root element that wraps all content.
- **`<head>`**: Contains metadata like the page title and linked stylesheets.
- **`<title>`**: Sets the page title visible in the browser tab.
- **`<body>`**: Holds the visible content of the webpage.
- **`<h1>` and `<p>`**: Define a heading and a paragraph.



### Real-World Example: The Rise of Blogging Platforms
One significant impact of HTML has been its role in the rise of blogging platforms. Early platforms like **Blogger (1999)** and **WordPress (2003)** utilized HTML to structure user-generated content, allowing people with little to no coding knowledge to publish their thoughts online. These platforms made use of HTML elements such as `<h1>` for post titles, `<p>` for paragraphs, and `<img>` for embedding images, making web publishing accessible to a broader audience. The growth of blogging significantly influenced digital journalism, online marketing, and social media platforms. [^ebersbach05html]



### HTML and CSS: Connecting the Dots
While HTML structures content, CSS is used to style it. You can link an external CSS file to an HTML document using the `<link>` tag within the `<head>` section:

```html
<head>
  <link rel="stylesheet" href="styles.css" />
</head>
```

This method ensures a clean separation between structure and design, making it easier to maintain and update web pages. [^ebersbach04html]




[^ebersbach01html]: Berners-Lee, Tim. 1991. *[WorldWideWeb: Proposal for a HyperText Project](https://www.w3.org/History/1991-WWW-NeXT/)*. Accessed January 21, 2025.  
[^ebersbach02html]: Duckett, Jon. 2011. *HTML & CSS: Design and Build Websites*. Indianapolis: Wiley, pp. 23-45.  
[^ebersbach03html]: Meyer, Eric. 2018. *HTML5 for Web Designers*. Sebastopol, CA: O'Reilly Media, pp. 78-102.  
[^ebersbach04html]: Krug, Steve. 2020. *Don't Make Me Think, Revisited*. 3rd ed. Berkeley: New Riders, pp. 12-30.  
[^ebersbach05html]: Blood, Rebecca. 2002. *The Weblog Handbook: Practical Advice on Creating and Maintaining Your Blog*. Cambridge, MA: Perseus Publishing.  



![HTML](images/html_ai.png) 
Caption: Used ChatGPT, prompt: Create an image that represents html

## HTTPS

**HTTPS (Hypertext Transfer Protocol Secure)** is an extension of **HTTP (Hypertext Transfer Protocol)** that uses encryption to secure data exchanged between a user's browser and a web server. Through **SSL/TLS** protocols, it ensures data confidentiality, integrity, and authentication.

**Netscape** developed HTTPS in 1994 to secure online transactions for its browser. Initially, it used **SSL (Secure Sockets Layer)** for encryption. Over time, SSL was replaced by **TLS (Transport Layer Security)**, a more secure and efficient protocol. HTTPS became essential as online commerce and sensitive data exchanges grew, gaining widespread adoption in the 2000s and now a standard for safe browsing.


### The Importance of HTTPS in New Media

In the digital age, new media has transformed how information is shared, consumed, and created. Securing data transmission from social media platforms to online journalism and e-commerce has become crucial. HTTPS (HyperText Transfer Protocol Secure) is a key technology enabling secure online communication. HTTPS plays a significant role in ensuring privacy, security, and trust in new media, making it an essential standard for modern websites and online interactions.

First and foremost, HTTPS provides encryption, protecting user data from cyber threats such as hacking, phishing, and eavesdropping. Unlike HTTP, which transmits data in plain text, HTTPS uses SSL/TLS encryption to safeguard sensitive information. This is particularly vital in new media environments where personal data, login credentials, and payment details are frequently exchanged. Users engaging with social media, news sites, or digital content platforms benefit from this added layer of security.

Additionally, HTTPS enhances trust and credibility. In an era of misinformation and cybersecurity threats, users are more cautious about which websites they interact with. A website with HTTPS displays a padlock icon in the browser, signalling that it is secure and trustworthy. Search engines like Google also prioritize HTTPS-enabled sites in rankings, reinforcing its importance for content creators, businesses, and media organizations looking to reach wider audiences.

Moreover, HTTPS is widely used across various new media platforms. Social networks, video streaming services, blogs, forums, and e-commerce websites rely on HTTPS to ensure safe user experiences. Secure communication is particularly crucial for online journalism and digital activism, where protecting the integrity of information and sources is paramount.

In conclusion, HTTPS is an indispensable component of new media, providing security, trust, and improved visibility for online platforms. As the digital landscape evolves, ensuring secure communication will remain fundamental to engaging with new media safely and effectively.

![HTTPS](images/HTTPS.jpg)

[^hellegren17crypto]: Hellegren, Z. Isadora. 2017. “A History of Crypto-Discourse: Encryption as a Site of Struggles to Define Internet Freedom.” _Internet Histories_ 1 (4): 285–311. https://doi.org/10.1080/24701475.2017.1387466.  

[^fuchs11newmedia]: Fuchs, Christian. 2011. “New Media, Web 2.0 and Surveillance.” _Sociology Compass_ 5 (2): 134–47. https://doi.org/10.1111/j.1751-9020.2010.00354.x.  

[^wikipedia25https]: Wikipedia. 2025. “HTTPS.” _Wikipedia_, January 24, 2025. https://en.wikipedia.org/w/index.php?title=HTTPS&oldid=1271559473.  




## Identity Management

Identity management is a system that checks users and controls their access to an organization's resources. In new media, it shows how users create, maintain and protect their online profiles as they navigate digital spaces. These systems govern the processes of identifying, authenticating, and authorizing individuals and determining the level of access they have to various digital resources.[^windley]

The significance of identity management in new media stems from the shift where digital interactions have largely replaced or supplemented physical ones. The term became common in the late 1990s and early 2000s when enterprise networks expanded and the internet became a mainstream platform for commerce, communication, and content delivery. Today, identity management represents the intersection of security, privacy, user experience, and digital rights in networked environments.[^windley]

Identity management matters in new media for several reasons:

* The proliferation of digital services requiring authentication has created a complex ecosystem where users must manage multiple identities across platforms.
* The rise of data analysis through [analytics](#analytics) and [surveillance](#surveillance) has increased the stakes of identity protection and privacy.
* Digital identities have become extensions of personal and professional lives, blurring boundaries between online and offline existence.
* [AI](#ai) systems increasingly rely on identity data to personalize experiences and make decisions affecting users.

Modern identity management is integrated into nearly all digital platforms and technologies. Single sign-on (SSO) solutions allow users to access multiple applications with one set of credentials. Social login capabilities enable users to leverage existing identities from platforms like Google or Facebook across the web, creating interconnected identity ecosystems.[^fiesler] Biometric authentication—including facial recognition, fingerprint scanning, and voice identification—has transformed how we verify identity in digital spaces. Meanwhile, blockchain technology offers new paradigms for self-sovereign identity systems where users maintain greater control over their digital identifiers and credentials.[^allen]

Identity management also encompasses the management of [digital reputation](#online-reputation) and [personal branding](#personal-branding) across social media platforms, where users cultivate specific personas for different audiences. The concept of "context collapse"—where distinct social contexts converge in single platforms—has made identity management increasingly complex for everyday users.[^marwick]

Several examples illustrate the evolution and importance of identity management in new media:

* **OAuth and OpenID Connect**: These open protocols enable secure authorization between services without sharing password credentials, allowing for federated identity systems across the web.

* **Password Managers**: Applications like [LastPass](https://www.lastpass.com/), [1Password](https://1password.com/), and [Bitwarden](https://bitwarden.com/) help users generate, store, and manage complex passwords across multiple platforms, addressing the "password fatigue" problem.

* **Two-Factor Authentication (2FA)**: This security process requires users to provide two different authentication factors, significantly enhancing security beyond password-only systems.

* **Self-Sovereign Identity Systems**: Blockchain-based initiatives like [Sovrin](https://sovrin.org/) and [uPort](https://www.uport.me/) aim to give individuals control over their digital identities without relying on centralized authorities.

* **GDPR and Identity Regulations**: Legislation like the European Union's [General Data Protection Regulation](https://gdpr.eu/) establishes rights regarding personal data, including the "right to be forgotten," directly impacting how digital identities are managed.

While identity management as a practice existed in various forms previously, the term "identity management" in its current technological context emerged in the late 1990s within enterprise IT. The concept was formalized and popularized by various industry groups and vendors developing directory services and access management solutions.

Kim Cameron, Microsoft's former Chief Architect of Identity, made significant contributions to identity management theory with his "Seven Laws of Identity" published in 2005, which established core principles for digital identity systems.[^cameron] The subsequent development of the Identity Metasystem concept helped shape modern approaches to federated identity.

The Internet Engineering Task Force (IETF) and the Organization for the Advancement of Structured Information Standards (OASIS) have played crucial roles in developing standards like SAML (Security Assertion Markup Language) and XACML (eXtensible Access Control Markup Language) that underpin modern identity management systems.

[^windley]: Windley, Phillip J. 2005. *Digital Identity: Unmasking Identity Management Architecture (IMA)*. Sebastopol, CA: O'Reilly Media.

[^fiesler]: Fiesler, Casey, and Blake Hallinan. 2018. "'We Are the Product': Public Reactions to Online Data Sharing and Privacy Controversies in the Media." In *Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems*, 1-13. Montreal: ACM. https://doi.org/10.1145/3173574.3173627.

[^allen]: Allen, Christopher. 2016. "The Path to Self-Sovereign Identity." *Life With Alacrity* (blog), April 25, 2016. http://www.lifewithalacrity.com/2016/04/the-path-to-self-sovereign-identity.html.

[^marwick]: Marwick, Alice E., and danah boyd. 2011. "I Tweet Honestly, I Tweet Passionately: Twitter Users, Context Collapse, and the Imagined Audience." *New Media & Society* 13 (1): 114-133. https://doi.org/10.1177/1461444810365313.

[^cameron]: Cameron, Kim. 2005. "The Laws of Identity." Microsoft Corporation. https://www.identityblog.com/stories/2005/05/13/TheLawsOfIdentity.pdf.

## Image Recognition 

Image recognition is a part of artificial intelligence (AI) and computer vision. It’s goal is to focus on identifying objects, patterns, spaces or scenes in images to further categorize them. These image recognition technologies are used in security, e-commerce, smart vehicles and healthcare. For the technologies to be successful in what it does it must rely on deep learning (DL), especially convolutional neural networks (CNNs). Because of DL and CNN, image recognition models have greatly improved in terms of efficiency and accuracy. [^krizhevsky17imagenet]

There are multiple steps in the process of image recognition for a machine to recognize and categorize images.
1.	Collection & Preprocessing of data: Large datasets such as ImageNet or Coco are used for training and testing machine learning models. In order to improve recognition accuracy, preprocessing techniques such as image normalization, noise reduction, and resizing are put into use. [^rawat17deep]
2.	Extraction: Unlike traditional methods where image recognition would rely on handcrafted features, deep learning automatically learns from raw data. This data could be the edges, textures, or shapes of an object in order to distinguish them from others.[^rawat17deep]
3.	Model training: CNNs learn from labeled images to recognize different objects more accurately. [^he16residual]
4.	Classification: Once the model is trained, it is ready to analyze new images to extract and classify them in categories.[^krizhevsky17imagenet]
Deep Learning in Image Recognition
Deep learning, particularly CNNs, have pushed image recognition higher than ever, surpassing traditional learning machines. CNNs automatically learns patterns from digital images to improve accuracy [^rawat17deep]. One of the key models was AlexNet (2012) which demonstrates the effectiveness of CNNs in large-scale image classification. [^krizhevsky17imagenet]



Image recognition is becoming more and more used in our everyday wordd.
1.	Facial recognition & security
Facial recognition is used to verify identities, detecting facial features in realtime to provide security and privacy. These features can be found in smartphones, surveillance cameras, or various authentication process [^rawat17deep].
2.	Medical & healthcare
Image recognition is also used in healthcare (X-rays, MRIs and CT scans) to help in the diagnosis of diseases. Deep learning-based models have become more and more advanced allowing it to have high accuracy in determining skin cancer and other conditions through images. [^he16residual] 
3.	Smart Vehicles 
Self-driving cars uses image recognition to detect any incoming elements. Pedestrians, traffic signals, lane markers, stops signs and obstacles such as bumps are all detected by these smart vehicles. The KITTI dataset is commonly used to train deep learning models for self-driving.[^krizhevsky17imagenet]


Although image recognition has proven useful, it also provides a couple of new challenges. One of them is about ethical concerns. Image recognition could sometime provide inaccurate predictions through biases from datasets. These inaccuracies are mostly evident in facial recognition and medical diagnosis [^rawat17deep].


[^rawat17deep]: Rawat, Waseem, and Zenghui Wang. “_Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review_.” Neural Computation, vol. 29, no. 9, 2017, pp. 2352–449, https://doi.org/10.1162/neco_a_00990.
[^he16residual]: He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “_Deep Residual Learning for Image Recognition_.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. 
[^krizhevsky17imagenet]: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. “_ImageNet Classification with Deep Convolutional Neural Networks_.” Communications of the ACM 60 (6): 84–90. https://doi.org/10.1145/3065386.



## Immersion

Immersion is a key concept in new media and interactive experiences. It refers to the sensation of a person being enveloped in a digital space or virtual environment. It is often described as the user's presence in a mediated space, frequently observed in virtual reality (VR), augmented reality (AR), and interactive storytelling.


The concept of immersion can be traced back to early forms of media. Specifically, 19th-century devices such as the **Panorama** and **Peep Show** aimed to modernize passive spectatorship by engaging audiences with vast visual spectacles in controlled environments. According to Jonathan Crary's analysis: "It is a new isolated consumer of a mass-produced commodity"[^ebersbach01immersion]. 
Crary argues that it is valuable to emphasize the continuity between immersive experiences today and those from 150 years agoo[^ebersbach01immersion]. 
Thanks to the ealier devices of the sens fo depth and realism, it had evolve to the development of VR headset, haptic feedback systems and spatial audio.

An early theory discussion by Janet Murray describe it as the experience of being transported into a digital narritve[^ebersbach02immersion]. Later, Mel Slater and Sylvia Wilbur distinguished betweem i,mersion and precence as the subjective feeling of bneing "inside" a virtual world[^ebersbach03immersion].

### How is it holding in Media Practices

Immersion plays a central role in contemporary digital life in everything from video games and virtual reality simulation to interactive theater and narrative mediated by artificial intelligence. Immersive experience in video games is achieved through fidelity-rich visuals, physics engines that react to players' actions, and diegetic sound. Immersive installations in film and digital art rely on 360-degree projection, spatial audio, and interactive elements that react to movement.
In human-computer interaction, immersion is typically synonymous with flow theory of Csikszentmihalyi[^ebersbach04immersion] in that users are completely absorbed in an experience since it is so well designed and so captivating.

![Immersion](/images/immersion-hubert.png)


[^ebersbach01immersion]: Crary, Jonathan. 2002. "Géricault, the Panorama, and Sites of Reality in the Early Nineteenth Century." *Grey Room*, no. 9: 7–8.
[^ebersbach02immersion]: Murray, Janet H. 1997. *Hamlet on the Holodeck: The Future of Narrative in Cyberspace*. Cambridge, MA: MIT Press, 97.
[^ebersbach03immersion]:  Slater, Mel, and Sylvia Wilbur. 1997. "A Framework for Immersive Virtual Environments (FIVE): Speculations on the Role of Presence in Virtual Environments." *Presence: Teleoperators and Virtual Environments* 6 (6): 603–16.
[^ebersbach04immersion]:  Csíkszentmihályi, Mihály. 1990. *Flow: The Psychology of Optimal Experience*. New York: Harper & Row.




## Information Architecture

The term was coined by Richard Saul Wurman who described “Information Architecture” as the organisation and presentation of information. It is a focus on making the complex clear with structural design of shared information environments. This involves organization, labeling, search and navigation schemes for websites. To make everything clear and accessible for users of the web.[^ding10information]

There is an analogy that can be made between designing websites and designing buildings. Places like banks, coffee shops or malls all offer different purposes. Architecture is an important step to designing coherent, safe and efficient buildings for whatever purpose they may serve. The same is true for website design. Standards are made so that users can easily navigate each without becoming confused. Websites also offer their own unique architectural challenges that are all tackled in information architecture. [^morville06wideweb]

It is a discipline of making information findable and understandable. Searching, browsing, categorizing, presenting and contextualizing help people find what they are looking for. Information can be seen as a product or item and websites can be arranged to make finding the right thing easier. Good information architecture comes from understanding the content available, the context for seeking the content and the user who is consuming the content. Building website navigation to flow and address these main concerns is what makes for good information architecture. [^interaction25whatis]

This architecture is at the basis of the whole world wide web we navigate every day. It is an important part of optimizing the new media that is the internet so that it is approachable to everyone. Our understanding of what we expect from the medium can help us better build it and utilize it. Making it accessible to everyone who seeks information on the web.

[^ding10information]:Ding Wei, Lin Xia. 2010. Information Architecture: The design and integration of information spaces. Springer Nature Link https://link-springer-com.lib-ezproxy.concordia.ca/book/10.1007/978-3-031-02267-8 
[^morville06wideweb]:Morville Peter, Rosenfeld Louis. 2006. Information Architecture for the world wide web. 3rd Edition. DCC UChile https://users.dcc.uchile.cl/~nbaloian/ArquitecturaDeLaInformacion/materialDeLAWeb/InformationArchitecturefortheWorldWideWebThirdEditi.pdf 
[^interaction25whatis]:Interaction Design Foundation. 2025. What is Information Architecture (IA)? Interaction Design Foundation https://www.interaction-design.org/literature/topics/information-architecture 

## Information Overload

Information overload occurs when the volume of information surpasses an individual's capacity to process it, leading to decision fatigue, stress, and reduced productivity [^arnold2023]. The phenomenon has intensified with digital technologies, which generate vast amounts of data through emails, social media, and online platforms [^cazaly2021]. However, information overload is not a singular concept; research has distinguished between **information overload**—excessive data—and **interaction overload**, which stems from excessive communication and digital engagement [^he2020]. Both contribute to cognitive strain and reduced efficiency in decision-making [^he2020].  

Futurist **Alvin Toffler** popularized the term in *Future Shock* (1970), describing the overwhelming effects of excessive information [^sbaffi2020]. However, concerns over information excess date back to ancient Rome, with scholars lamenting the abundance of books [^sbaffi2020]. Earlier studies highlighted that while information availability was initially seen as beneficial, exceeding a cognitive threshold leads to a decline in productivity and comprehension [^noyes1995].  

### Impact on New Media  

Information overload is critical in new media, influencing digital consumption and cognitive well-being. Contributing factors include:  

- **Technological Expansion** – The rapid flow of information via search engines and social media overwhelms users [^cazaly2021].  
- **Algorithmic Amplification** – Personalized content recommendations create endless streams of information, reducing attention spans [^cazaly2021].  
- **Multitasking Culture** – Workplaces demand constant information processing, increasing cognitive strain [^sbaffi2020].  
- **Psychological Effects** – Linked to stress, anxiety, and burnout, particularly in high-stakes fields like healthcare [^arnold2023].  
- **Communication Overload** – The rise of collaborative tools and constant messaging (e.g., emails, chats, notifications) has made interaction overload a distinct form of information burden, disrupting concentration and reducing efficiency [^he2020].  

### Managing Information Overload  

- **Cognitive Filtering** – Prioritizing essential information while limiting distractions [^cazaly2021].  
- **Digital Minimalism** – Reducing notifications and structured content consumption [^cazaly2021].  
- **AI Tools** – Utilizing technology to filter and summarize information [^arnold2023].  
- **Organizational Policies** – Streamlining workplace communication to avoid excess data flow [^arnold2023].  
- **Cognitive Load Management** – Studies show that excessive information processing can lead to information fatigue; reducing unnecessary data exposure can improve retention and engagement [^noyes1995].  

### Examples  

- **Healthcare** – Physicians face information overload from patient records, research, and administrative updates, leading to fatigue [^sbaffi2020].  
- **Social Media** – Continuous streams of news and advertisements contribute to cognitive exhaustion [^cazaly2021].  
- **Enterprise Decision-Making** – Studies have found that managers and employees experiencing high information overload often make slower or suboptimal business decisions due to excessive data processing requirements [^he2020].

![Information Overload](images/information-overload-rowan.png)


[^arnold2023]: Arnold, Miriam, Mascha Goldschmitt, and Thomas Rigotti. 2023. "Dealing with Information Overload: A Comprehensive Review." *Frontiers in Psychology* 14: 1122200. https://doi.org/10.3389/fpsyg.2023.1122200.  

[^sbaffi2020]: Sbaffi, Laura, James Walton, John Blenkinsopp, and Graham Walton. 2020. "Information Overload in Emergency Medicine Physicians: A Multisite Case Study Exploring the Causes, Impact, and Solutions." *Journal of Medical Internet Research* 22 (7): e19126. https://doi.org/10.2196/19126.  

[^cazaly2021]: Cazaly, Lynne. 2021. "How to Save Yourself from Information Overload." *Harvard Business Review*. https://hbr.org/2021/09/how-to-save-yourself-from-information-overload.  

[^he2020]: He, Tingting. 2020. "Information Overload and Interaction Overload as Two Separate Attributes of Information Overload Syndrome." *Journal of Enterprising Culture* 28 (3): 263–279. https://doi.org/10.1142/S0218495821500114.  

[^noyes1995]: Noyes, J. M., and P. J. Thomas. 1995. “Information Overload: An Overview.” *IEEE Colloquium on Information Overload*.  

## Interactivity

Interactivity is a broad term within new media, one that was popuralized in the late 20th century as a machine's ability to respond to user input. It came to fashion as a way to describe human computer interaction (a relationship between a human and computer),
as opposed to how it was typically defined within sociology (a relationship between two people). This became especially prevalent as computers left the sphere of specialized scientific research, and became a household machine in the 80s.[^Quiring08interactivity] 
Since the term originated in sociology, it is hard to determine a single person or group who originally defined it within the context of new media.

Within digital Interactivity, there are two types: user-to-user interaction (ex. chat rooms, multi-player games) and user-to-system interaction (ex. writing an essay, single player games). There are also three basic dimensions to interactivity:[^Quiring08interactivity] 

1. ### As an attribute of technological systems
   Features that offer a potential for interaction, like hyperlinks.
   
2. ### As an attirbute of user perception
   The user realizing the "interactive potential" of the first dimension, by using said hyperlinks.
   
3. ### As an attribute of communication processes
   When the user achieves the second dimension and uses it properly, communication processes become interactive. For example, said hyperlink allows the user to send an email to someone, and that person responds to said email.

However, some researchers argue that we have not yet grasped what interactivity is, what it does, or its consequences on members of a social system.[^Bucy04society] They believe that beyond just being a technical property of media systems, it must have meaningful social relevance as a concept; and that different interactive technologies should not be classified as they exist within an unbiased theoretical framework.

The purpose of interactivity within our contemporary understanding is strongly linked to that of media richness theory. Proposed in 1986 by Richard Daft and Robert Lengel, it refers to the “ability of information to change understanding within a time interval”.[^Daft86richness] The theory proposes that different forms of media have different levels of communicational "richness", with face-to-face communication being the richest possible form. By imitating face-to-face communication, it is proposed that digital interactivity becomes more communicationally rich.[^Quiring08interactivity] Hence why interactive media is in an arms race to emulate real-world interaction, especially nowadays with AI chatbots or customer support alogorithms.

![Interactivity](images/interactivity.png)
*Image generated by crAIon with the prompt to DALL·E mini 'Make me an image of the keyword "interactivity", include someone interacting with a machine'*


[^Quiring08interactivity]: Quiring, Oliver, and Wolfgang Schweiger. May 2008. “Interactivity: A Review of the Concept and a Framework for Analysis.” Communications: The European Journal of Communication Research. https://doi.org/https://www.degruyter.com/document/doi/10.1515/COMMUN.2008.009/html.
[^Bucy04society]: Bucy, Erik P. 2004. “Interactivity in Society: Locating an Elusive Concept.” The Information Society 20 (5): 373–83. doi:10.1080/01972240490508063.
[^Daft86richness]: Richard L. Daft, Robert H. Lengel. 1986. "Organizational Information Requirements, Media Richness and Structural Design." Management Science 32(5):554-571.
https://doi.org/10.1287/mnsc.32.5.554

## **Interface Effect**

The *interface effect* is a term from Alexander R. Galloway’s book *The Interface Effect* (2012).[^1]
Instead of thinking of an interface like a website, app, or game screen; think of it as a neutral window 
we look through to access content, Galloway suggests that interfaces are guiding us in how we experience media.
In other words, they don’t just show us things, they shape what we see, how we act, and how we think about that media [^2]
Galloway’s point is that an interface isn’t just a tool. It’s a 'structure of possibilities and limits'.
It decides what is visible, what is clickable, what actions are easy or impossible, and even what feels “normal” or
expected when we use digital systems. Interfaces set the 'rules of engagement' without us necessarily noticing, and this 
is what he calls the *interface effect*, the influence the interface has on behavior and meaning.
For example, social media feeds, infinite scrolling, autoplay buttons, notifications, and recommended content don’t
just happen by accident. They are part of the interface design that guides your attention and decisions. The interface
makes some actions feel natural, like scrolling forever, and makes others harder, like finding your settings or deleting
an account. As one reviewer puts it, Galloway shows us that interfaces are cultural and political environments,not invisible 
backgrounds [^3].
This idea is significant in new media because it challenges the old assumption that interfaces are simple platforms. 
Instead, the interface is part of the *media experience itself* and shapes how digital culture works.

---
## **Bibliography**

[^1]: Galloway, Alexander R. 2012. *The Interface Effect*. Cambridge: Polity.
[^2]: “Alexander R. Galloway – The Interface Effect.” PZWIKI. https://pzwiki.wdka.nl/mediadesign/Alexander_R._Galloway_-_The_Interface_Effect.
[^3]: LA Review of Books. “The Next Level: Alexander R. Galloway’s *The Interface Effect*.” https://lareviewofbooks.org/article/the-next-level-alexander-r-galloways-the-interface-effect/(https://lareviewofbooks.org/article/the-next-level-alexander-r-galloways-the-interface-effect/).


‌

## Internet of Things (IoT)

The Internet of Things (IoT), sometimes referred to as the Internet of Objects.[^zeinab17IoTapplications] It refers to a network of physical objects embedded with sensors, software, and other technologies that enable them to connect and exchange data over the internet.[^rose15IoToverview] These objects, often called "smart devices," range from everyday consumer products like smart thermostats and wearable fitness trackers to industrial machines and urban infrastructure components.

The term "Internet of Things" was coined by Kevin Ashton in 1999.[^li15IoTsurvey] He described it as a system of interconnected objects that use radio-frequency identification (RFID) technology for communication.[^li15IoTsurvey] The terms "Internet" and "Things" refer to a globally interconnected network driven by sensory, communication, networking, and information processing technologies, which have redefined the concept of information and communications technology (ICT).[^li15IoTsurvey] IoT operates through four primary communication models that define how devices interact: Device-to-Device Communication, Device-to-Cloud Communication, Device-to-Gateway Model, and Back-End Data-Sharing Model.[^rose15IoToverview] These models enable IoT devices to collect large amounts of data, analyze inputs, and respond accurately and efficiently. A key advantage of IoT technology is its cost-effectiveness and low complexity, which contribute to reducing localization errors and improving real-time data processing.[^xia12IoT]

IoT has been widely adopted across various fields, including supply chain management, manufacturing, environmental monitoring, retail, smart shelf operations, healthcare, the food and restaurant industry, and travel and tourism.[^li15IoTsurvey] The IoT is expected to significantly contribute to addressing social challenges such as healthcare monitoring, daily life assistance, and traffic congestion management.[^li15IoTsurvey] Such that, both consumers and manufacturers could be benefit from IoT's hardware and software components as enhance lifestyle and efficiency.

The products of IoT like Internet-enabled appliances, home automation components, and energy management systems are advancing the concept of the "smart home” .[^zeinab17IoTapplications][^rose15IoToverview] While on a societal level, IoT improves infrastructure, enhances public transportation, reduces traffic congestion, and fosters safer and more engaged communities, which contributing to the development of "smart cities".[^zeinab17IoTapplications][^rose15IoToverview]  Moreover, IoT is transforming digital media by enabling seamless real-time data exchange, personalized content delivery, and interactive experiences across connected platforms, influencing entertainment, communication, and media consumption patterns.[^hallur21entertainment]

For example:
- Smart Homes: Users can control lighting, TVs, air conditioning, and central heating through mobile apps and smart home assistants like Amazon Alexa and Google Home.
- Smart Cities: Citizens can access real-time transportation and weather updates through IoT-connected apps that gather data from sensors across urban areas.
- Healthcare: IoT devices such as wearable fitness trackers and health monitoring tools, like the Apple Watch and Enflux Smart Clothing, provide users with personalized health insights and enable medical devices to deliver more precise and effective care.[^zeinab17IoTapplications]
- Entertainment: IoT has transformed entertainment experiences through smart TVs, streaming devices, and voice-controlled assistants that personalize media consumption. While VR headsets and IoT-connected gaming systems create immersive, real-time interactive experiences.[^hallur21entertainment]

Despite its advantages, IoT raises significant concerns regarding data privacy and security. Since IoT devices collect and analyze vast amounts of personal data, issues related to data flow security and user rights have become major challenges in IoT development.[^rose15IoToverview] The rapid evolution of IoT often outpaces the ability of legal, policy, and regulatory frameworks to keep up,[^rose15IoToverview] which making it difficult for governments and global institutions to establish universally accepted regulations. Addressing these concerns will be critical to ensuring IoT's continued growth while safeguarding user privacy and security.

![Internet of Things](images/internetofthingsIoT-wong.jpg)

Tool: ChatGPT. Prompt: “Creat a image that show several new media use through Internet of Things (IoT) network”


[^zeinab17IoTapplications]: Zeinab, Kamal Aldein Mohammed, and Sayed Ali Ahmed Elmustafa. 2017."Internet of things applications, challenges and related future technologies." _World Scientific News_ 67, no. 2 : 126-148.

[^rose15IoToverview]: Rose, Karen, Scott Eldridge, and Lyman Chapin. 2015. "The Internet of Things: An Overview." _The internet society (ISOC)_ 80, no. 15 : 1-53.

[^li15IoTsurvey]: Li, Shancang, Li Da Xu, and Shanshan Zhao. 2015. " The internet of things: a survey." _Information systems frontiers_ 17(2): 243-259.

[^xia12IoT]: Xia, Feng, Laurence T. Yang, Lizhe Wang, and Alexey Vinel. 2012."Internet of things." _International journal of communication systems_ 25, no. 9 : 1101.

[^hallur21entertainment]: Hallur, Giri Gandu, Sandeep Prabhu, and Avinash Aslekar. 2021. "Entertainment in era of AI, big data & IoT." _Digital Entertainment: The Next Evolution in Service Sector_ :87-109.

## Network Security  

Network security refers to the set of technologies, policies, and practices designed to protect digital communications, data integrity, and confidentiality from cyber threats. With the increasing reliance on interconnected systems, securing networks has become a critical concern for individuals, businesses, and governments. Effective network security involves multiple layers of defense to protect infrastructure, detect threats, and mitigate attacks [^huang2010].  

### Core Principles of Network Security  

Several fundamental principles guide the implementation of network security:  

- **Confidentiality** – Ensuring that sensitive data is accessible only to authorized individuals. Encryption techniques such as the **Advanced Encryption Standard (AES)** help safeguard data from unauthorized access [^zhao2024].  
- **Integrity** – Protecting information from unauthorized modification or tampering. Cryptographic hash functions and digital signatures ensure that data remains unaltered during transmission [^maasaoui2022].  
- **Availability** – Maintaining reliable access to network resources. Protection against **Distributed Denial of Service (DDoS)** attacks ensures that network services remain operational even under heavy cyber threats [^cheng2010].  

### Key Network Security Technologies  

Network security employs various technologies to protect systems and data, including:  

- **Firewalls** – Acting as a barrier between trusted and untrusted networks, firewalls filter incoming and outgoing traffic based on predefined security rules [^huang2010].  
- **Intrusion Detection and Prevention Systems (IDPS)** – These systems monitor network traffic for suspicious activities, alert administrators, and take preventive actions when necessary [^zhao2024].  
- **Virtual Private Networks (VPNs)** – Encrypting data transmissions over public networks to enhance privacy and security, especially in remote work environments [^maasaoui2022].  
- **Zero Trust Security** – A modern security approach where no entity—internal or external—is trusted by default, requiring continuous authentication and verification of users and devices [^cheng2010].  

### Challenges and Emerging Threats  

As network security evolves, new threats continue to emerge, requiring constant adaptation. Some of the most pressing challenges include:  

- **Ransomware and Malware** – Malicious software that encrypts or steals data, often demanding payment for its release [^zhao2024].  
- **Phishing Attacks** – Social engineering techniques used to trick users into revealing sensitive information, such as login credentials [^maasaoui2022].  
- **5G and IoT Vulnerabilities** – The expansion of **Internet of Things (IoT)** devices and **5G networks** introduces new attack vectors that require advanced security measures [^cheng2010].  

### Examples  

- **The WannaCry Ransomware Attack (2017)** – Exploited a vulnerability in Windows operating systems, affecting thousands of organizations globally, including healthcare and financial sectors [^maasaoui2022].  
- **SolarWinds Supply Chain Attack (2020)** – A sophisticated attack compromising enterprise networks by injecting malicious code into software updates [^cheng2010].  
- **DDoS Attack on GitHub (2018)** – One of the largest recorded DDoS attacks, which peaked at 1.35 terabits per second, demonstrating the need for robust network security defenses [^zhao2024].  


[^huang2010]: Huang, Scott C.-H., David MacCallum, and Ding-Zhu Du. 2010. *Network Security*. New York: Springer.  
[^zhao2024]: Zhao, Dongmei, Guoqing Ji, Yiling Zhang, Xunzheng Han, and Shuiguang Zeng. 2024. “A Network Security Situation Prediction Method Based on SSA-GResNeSt.” *IEEE Transactions on Network and Service Management* 21 (3): 3498–3509.  
[^maasaoui2022]: Maasaoui, Zineb, Anfal Hathah, Hasnae Bilil, Van Sy Mai, Abdella Battou, and Ahmed Ibath. 2022. “Network Security Traffic Analysis Platform - Design and Validation.” *IEEE International Conference on Computer Systems and Applications*.  
[^cheng2010]: Cheng, X., and S.C.-H. Huang. 2010. “Security Protocols and Cryptography in Network Security.” *Springer Science+Business Media*.  

## Internet

The Internet is a vast, decentralized network of interconnected computers that enables global communication, information sharing, and digital interaction. It is built on a foundation of standardized communication protocols, primarily the **Transmission Control Protocol/Internet Protocol (TCP/IP)**, which ensures the seamless exchange of data across diverse systems [^clark1988]. The Internet evolved from early research initiatives, notably the **ARPANET**, developed in the late 1960s to facilitate communication between academic and military institutions [^leiner2009]. Over time, it expanded into a ubiquitous infrastructure supporting a wide range of applications, from web browsing to cloud computing.  

### Core Architectural Principles  

The Internet's success is largely attributed to key design principles that have shaped its evolution:  

- **Packet-Switching** – Instead of relying on a dedicated communication channel, data is broken into packets that travel independently across the network before being reassembled at the destination. This ensures efficiency and fault tolerance [^abbate1999].  
- **End-to-End Principle** – The network itself remains simple, while intelligence and complexity are handled at the edges (i.e., the devices communicating over the network). This principle fosters flexibility and innovation by allowing new applications to be built without requiring changes to the core infrastructure [^saltzer1984].  
- **Layered Architecture** – The Internet is structured into multiple layers, including the physical, network, transport, and application layers. Each layer performs a specific function, enabling interoperability and scalability [^clark1988].  

### Impact on Society  

The Internet has transformed nearly every aspect of modern life, serving as the foundation for digital economies, communication platforms, and knowledge dissemination. Some key societal impacts include:  

- **Information Access** – With search engines and online databases, knowledge is more accessible than ever, revolutionizing education, research, and journalism [^leiner2009].  
- **Digital Communication** – Email, instant messaging, and social media have reshaped interpersonal and mass communication, making global interaction instantaneous and ubiquitous [^abbate1999].  
- **E-commerce & Digital Markets** – Online marketplaces such as Amazon, Alibaba, and Shopify have disrupted traditional commerce, enabling businesses of all sizes to reach global audiences [^leiner2009].  
- **Cloud Computing & Big Data** – The Internet supports vast computational and storage infrastructures, driving advancements in artificial intelligence, automation, and data analytics [^clark1988].  

### Challenges and Future Considerations  

Despite its benefits, the Internet faces significant challenges, including:  

- **Security & Privacy Risks** – Cyber threats, including hacking, data breaches, and surveillance, raise concerns about online safety and digital rights [^clark1988].  
- **Digital Divide** – While the Internet connects billions of people, disparities in access persist, particularly in developing regions with limited infrastructure [^abbate1999].  
- **Decentralization vs. Control** – Governments and corporations influence how the Internet operates, leading to debates over net neutrality, censorship, and monopolization [^saltzer1984].  

### Examples  

- **The World Wide Web** – Invented by Tim Berners-Lee in 1989, the Web is an application of the Internet that enables hyperlinked content, forming the foundation of modern websites and digital services [^leiner2009].  
- **Streaming & Social Media** – Platforms like Netflix, YouTube, and Twitter leverage Internet infrastructure to provide on-demand content and real-time interactions, shaping digital culture [^abbate1999].  
- **Blockchain & Decentralized Networks** – Emerging technologies like blockchain operate on Internet protocols to enable secure, transparent transactions without central authorities [^clark1988].  


[^clark1988]: Clark, David D. 1988. "The Design Philosophy of the DARPA Internet Protocols." *ACM SIGCOMM Computer Communication Review* 18 (4): 106–114.  
[^saltzer1984]: Saltzer, Jerome H., David P. Reed, and David D. Clark. 1984. "End-to-End Arguments in System Design." *ACM Transactions on Computer Systems* 2 (4): 277–288.  
[^abbate1999]: Abbate, Janet. 1999. *Inventing the Internet*. Cambridge, MA: MIT Press.  
[^leiner2009]: Leiner, Barry M., Vinton G. Cerf, David D. Clark, Robert E. Kahn, Leonard Kleinrock, Daniel C. Lynch, Jon Postel, Larry G. Roberts, and Stephen Wolff. 2009. "A Brief History of the Internet." *ACM SIGCOMM Computer Communication Review* 39 (5): 22–31. https://doi.org/10.1145/1629607.1629613.  


## Iteration
Iteration refers to the repeated execution of tasks or procedures derived from the Latin word *iteratio*, meaning "repetition".[^etymologyIteration] In design and engineering, it involves the ongoing process of revisiting a set of steps or operations with the intent of refining and improving a product, system, or idea.[^NormanEverydayDesign]
In New media, this practice is essential for fostering continuous development, innovation, and a responsive approach to user needs.

Historically, iteration has been integral to programming and has significant applications in mathematics, engineering, and design.
It became a cornerstone of computer programming in the mid-20th century with the introduction of loops in languages like **FORTRAN** and **ALGOL**, facilitating efficient task repetition crucial for early computational processes.[^johnBackusFotron]

The importance of iterative processes spans various fields, particularly in software development, user experience design, and artificial intelligence. 
Agile methodologies in software development emphasize iterative cycles or sprints, allowing for continuous improvement based on user feedback.[^cohn2006] 
In user experience design, iterative processes are employed to optimize digital interfaces, ensuring they meet diverse user needs.[^NormanEverydayDesign] 
In the field of AI, the iterative training of machine learning models increases accuracy as algorithms learn from more data over time.[^goodfellow2016] 
These methods shape contemporary digital culture, which is evident in social media platforms that encourage creativity through the remixing and adapting of content, fostering collaboration. 
In gaming, iterative design enables developers to refine gameplay and respond to player feedback through updates and new content.[^juul2005] 
Social media algorithms utilize iterative processes to continuously enhance and curate content based on user interactions, significantly influencing the online experience.

Notable examples that highlight the use of iteration include platforms like **TikTok** and **YouTube**, which use iteration to analyze user behavior and refine content recommendations. This ensures user feeds remain engaging, keeping users returning.
In **video games**, iteration is crucial for development. Designers release updates and patches based on player feedback and analytics, improving gameplay, balancing mechanics, and adding features that resonate with the community.
Similarly, software updates on our phones are a prime example of iteration. Companies like **Apple** and **Google** release regular updates to fix bugs, enhance security, and introduce new features, ensuring devices remain functional and up-to-date.

[^etymologyIteration]: Douglas Harper. 2018. Etymology of "Iteration" on Etymonline. October 19. Accessed March 7, 2025. https://www.etymonline.com/word/iteration.
[^NormanEverydayDesign]: Norman, Don. 2013. _The Design of Everyday Things: Revised and Expanded Edition_. New York: Basic Books. Chapter 6, "Design Thinking".
[^johnBackusFotron]: Backus, John. 1978. "The Early History of FORTRAN." ACM SIGPLAN Notices 13 (8): 165–180. https://doi.org/10.1145/359576.359579.
[^cohn2006]: Cohn, Mike. 2006. Agile Estimating and Planning. Upper Saddle River, NJ: Prentice Hall.
[^goodfellow2016]: Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. _Deep Learning_. Cambridge, MA: MIT Press.
[^juul2005]: Juul, Jesper. 2005. Half-Real: Video Games between Real Rules and Fictional Worlds. Cambridge, MA: MIT Press.

![Iteration](images/iteration-nisma.png)


**The above image was generated using Chatgpt( GPT-4o ) , the following prompts were written [Iteration-Prompts](https://chatgpt.com/share/67d3987f-ba3c-8008-a37f-cae18fd2b2d6)**


## JPEG

JPEG (Joint Photographic Experts Group)[^JPEG] is a file format commonly used to compress and store images. It is referred to as a 
"lossy" format because it compresses and alters the pixel data of the file and is therefore not a "lossless" transfer. It was created in
1992 by the aptly named Joint Photographic Experts Group, which is widely credited with the proliferation of digital imaging with its ease
of use and storage. 

When pictures are taken using the JPEG compression format, the image data is stored in a RAW[^RAW] file under the .TIFF (Tag Image File Format) file format. When the image is to be stored,
it is then compressed using JPEG and converted to a .JPEG file. When this image is displayed, it is then uncompressed and returned to the RAW format. The method that 
JPEG takes to compression using the DCT (Discrete Cosine Transform) method by discarding high-frequency information (such as intense and dense data packets).[^IEEE] 
By discarding this data, it then fills the gap using existing data leading to a lossy compression where high-frequency details are lost and filled in with random data, creating noise or grain. 

Though the data loss through JPEG compression can be severe in some cases, it is mostly considered to be a worthwhile sacrifice for the 
amount of space it saves. Even though more formats have since been released and popularized (such as Apple's HEIC format or the PNG format) 
the JPEG format remains the old and reliable of image compression. The JPEG's longevity can largely be attributed to the fact that it remains one of the
most efficient and loss-neutral compression formats in the modern day. Though some desire truly lossless compression, until this is achieved JPEG will still be used 
by photographers, editors, illustrators and digital creators all around the world, and not many formats have any hope of replacing its reach. 

The identity of the JPEG format as the gold standard for image compression in the modern age solidifies the idea of good image quality as an equal sacrifice of file size
over quality, or noisiness. Similarly to the idea of a "critical mass" point in data storage in the future, the JPEG and other compression methods represent 
the standards that will be adopted in order to expand and store as much data as we can possibly store in the most efficient way. Theories such as Bill Viola's "Data Space"[^DATA]
storage methods show alternate paths we could take to not exactly avoid the possibility of a critical mass poin in data storage, but to restructure data in a way that makes 
the wealth of data we have accessible in ways that don't necessitate more data to be stored. JPEG formats and subsequent innovations in compression represent a predictably human and 
anti-progressive way of dealing with problems, by cramming in more where we shouldn't.

[^JPEG]: Joint Photographic Experts Group. 2024. "JPEG - About JPEG". Retrieved from *Joint Photographic Experts Group: About*. 2025. https://jpeg.org/about.html.
[^IEEE]: Wallace, G. K. 1991. “The JPEG Still Picture Compression Standard.” *IEEE Transactions on Consumer Electronics* 38 (1): xviii–xxxiv. https://ieeexplore.ieee.org/document/125072.
[^RAW]: Bennett, Michael J. and Wheeler, F. Barry. 2010. "Raw as Archival Still Image Format: A Consideration"  *Digital Commons Published Works, 2023*.
https://digitalcommons.lib.uconn.edu/libr_pubs/23
[^DATA]: Viola, Bill. 1982. “Will There Be Condominiums in Data Space?” In *The New Media Reader*, edited by Noah Wardrip-Fruin and Nick Montfort, 460–66. Cambridge, MA: MIT Press, 2003.


## Large Language Model

Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.[^whatAlovelyIBM] 
**Application In The World**:[^itAjournal] Large language models, such as the Generative Pre-trained Transformer (GPT-3) (Floridi & Chiriatti, 2020), have made significant advancements in natural language processing (NLP) in recent years. These models are trained on massive amounts of text data and are able to generate human-like text, answer questions, and complete other language-related tasks with high accuracy.
**Role In The World**:[^whatAlovelyIBM] LLMs have become a household name thanks to the role they have played in bringing generative AI to the forefront of the public interest, as well as the point on which organizations are focusing to adopt artificial intelligence across numerous business functions and use cases.
 
###  The Role of Large Language Models (LLMs) in New Media

Large Language Models (LLMs) are reshaping new media by transforming content creation, personalization, and interaction. From digital journalism to entertainment, AI-driven tools are changing how information is produced and consumed.

One of the most significant impacts of LLMs is in content generation. News organizations use AI to summarize reports and draft articles, speeding up the publication process. Social media platforms and streaming services leverage AI to curate personalized feeds, ensuring users see content tailored to their interests. This level of customization enhances engagement but also raises concerns about filter bubbles and algorithmic bias.

Beyond automation, LLMs are making media more interactive. AI-powered chatbots provide customer support, assist in research, and even act as virtual companions. In entertainment, video games and interactive storytelling platforms use AI-generated dialogue to create dynamic, player-driven narratives. These technologies blur the line between scripted and real-time content, making media more immersive than ever.

Another major shift is the democratization of media production. AI-assisted tools enable independent creators to produce high-quality content without specialized skills. Bloggers, video producers, and social media influencers use LLMs for writing, editing, and multilingual translation, expanding their reach to global audiences. However, this accessibility also means misinformation can spread more easily, as AI-generated text and deepfakes become harder to distinguish from authentic content.

The rise of AI in journalism presents both opportunities and challenges. While LLMs assist reporters in analyzing large datasets and fact-checking information, they also raise concerns about accuracy and editorial control. The ability to generate convincing yet false narratives highlights the need for responsible AI governance in media.

As AI continues to evolve, its role in new media will grow more complex. While it enhances creativity and efficiency, it also poses ethical challenges that must be addressed. Balancing innovation with accountability will be crucial in ensuring that AI-driven media remains informative, fair, and trustworthy.


### Challenges and Opportunities[^solutionsForLLMs]
The pathway to cost optimization in GenAI applications with large language models is laden with both challenges and opportunities. These arise from the inherent complexities of the models and the evolving landscape of AI technologies. The following are the principal challenges and the accompanying opportunities in this domain:
- Computational demands
- Model complexity
- Data privacy and security
- Scalability
- Model generalizability and domain adaptation
- Evolving regulatory landscape


![LLMs](images/LLMs-bolin.jpg)
*Image created in OpenArt. Prompt: “programmer with LLMs product”*


[^whatAlovelyIBM]: IBM 2023. *What are large language models (LLMs)?* https://www.ibm.com/think/topics/large-language-models
[^itAjournal]: Shreyas Subramanian 2024. *Large Language Model-Based Solutions : How to Deliver Value with Cost-Effective Generative AI Applications*. https://learning-oreilly-com.lib-ezproxy.concordia.ca/library/view/large-language-model-based/9781394240722/f06.xhtml#head-2-41
[^solutionsForLLMs]: Enkelejda Kasneci,Kathrin Sessler 2023."ChatGPT for good? On opportunities and challenges of large language models for education" *Learning and Individual Differences 103*. https://www.sciencedirect.com/science/article/abs/pii/S1041608023000195

## Linux

***Linux*** is the name attributed to a collection of operating systems built on top the Linux kernel[^economist01authentic]. Linus Torvalds started developing the kernel in 1991[^steven11twenty] with the express purpose of creating an [open-source](/keywords/open-source.md) clone of [Unix](/keywords/unix.md). After being commonly packaged with GNU[^stallman25gnu], Linux quickly grew in size and user base currently making up 4% of all personal desktop operating systems in the world[^statcounter25desktop].

Thousands of versions of Linux exist [^atea25distrowatch], commonly referred to as Distros (distributions). Some popular Distros include; [Ubuntu](https://ubuntu.com/download), [Debian](https://www.debian.org/), [Mint](https://www.linuxmint.com/), [Fedora](https://fedoraproject.org/), [Arch](https://archlinux.org/), [Gentoo](https://www.gentoo.org/), and [NixOS](https://nixos.org/). Each distribution comes with individual suites of software usually built on the GNU library which has led to the name GNU/Linux[^stallman21linux] being a more widely adopted then simply Linux. These suites include Window Managers/Desktop Environments, package managers, and more.

Due to Linux's offered freedom, Linux has become a common tool among creatives. From the Raspberri PI engineers, to the young Chromebook students, anyone with a computer can have immediate access to a multitude of complex and usable tools. Linux is more then a tool, it is a community of like minded thinkers who share a passion for freedom of expression. Linux is **the** public paintbrush, the one to seperate user from state and enterprise. It is pure technological freedom, which infinite modubility[^rankin05linux]. 

![LinuxKeywordImage](/images/linux-dumont.png)

![Linux Logo](/images/Linux-Symbol-1933970093.png)

---

[^economist01authentic]: "Authentic hero; Computer technology; Linus Torvalds and Linux." *The Economist*, January 27, 2001, 3. _Gale OneFile: CPI.Q_ (accessed February 3, 2025). https://link-gale-com.lib-ezproxy.concordia.ca/apps/doc/A69842147/CPI?u=concordi_main&sid=bookmark-CPI&xid=0078b28d.
[^steven11twenty]: Steven Vaughan-Nichols, “Twenty Years of Linux According to Linus Torvalds,” *ZDNET*, April 13th 2011, (accessed February 2, 2025). https://www.zdnet.com/article/twenty-years-of-linux-according-to-linus-torvalds/.
[^stallman25gnu]: Stallman, Richard. 2021. “*GNU/Linux FAQ - GNU Project - Free Software Foundation*”, accessed February 2, 2025, https://www.gnu.org/gnu/gnu-linux-faq.html.
[^statcounter25desktop]: “*Desktop Operating System Market Share Worldwide*”, StatCounter Global Stats, accessed February 2, 2025, https://gs.statcounter.com/os-market-share/desktop/worldwide.
[^atea25distrowatch]: “*DistroWatch.Com: Put the Fun Back into Computing. Use Linux, BSD.*”, Atea Ataroa Limited, accessed February 2, 2025, https://distrowatch.com/.
[^stallman21linux]: Stallman, Richard. “*Linux and GNU - GNU Project - Free Software Foundation*”, accessed February 2, 2025, https://www.gnu.org/gnu/linux-and-gnu.html.
[^rankin05linux]: Rankin, Kyle. *Linux Multimedia Hacks*. 1st ed. Sebastopol, Calif.: O’Reilly, 2005. http://books.google.com/books?id=oZpQAAAAMAAJ.




## Lossless
Lossless compression offers a method to reduce file sizes without compromising any of the original content. 
For text, music, images, or videos, the principle remains consistent: minimize file size while preserving all details.
With images specifically, this compression technique ensures that every aspect of visual quality remains identical to the original. 
Unlike lossy compression, which discards some data to achieve smaller file sizes, lossless compression guarantees complete preservation of the original [^ester90image]. 
Lossless compression is especially valuable when dealing with sensitive or high-quality content, 
such as professional photography or archival data, where even the smallest loss in quality could be detrimental. 
Its ability to maintain the integrity of the original file makes it ideal for tasks that demand accuracy and clarity. 

When preserving moving images, it's important to use methods that keep the original quality intact. 
This means compressing the files without losing any details or clarity. By avoiding any loss of data, 
the digital copies stay true to the original, which is especially important for historical films or valuable content. 
This way, the material remains high-quality and can be easily stored and accessed in the future without worry about
losing quality over time [^besser01digital].

Compression becomes more challenging when dealing with noise, random errors that can appear in any file type. 
These irregularities complicate compression by introducing unnecessary information. 
The Rice algorithm, a lossless data compression method, is good at handling noisy data since it can compress files about 1.4 times better 
and 2-3 times faster than methods like GZIP [^pence09lossless].  Choosing the appropriate compression 
methodology would be able to optimize file size while maintaining quality across all media types.



[^pence09lossless]: Pence, W. D., R. Seaman, and R. L. White. 2009. “Lossless Astronomical Image Compression and the Effects of Noise.” Publications of the Astronomical Society of the Pacific 121, no. 878: 414–27. https://doi.org/10.1086/599023.
[^besser01digital]: Besser, Howard. 2001.“Digital Preservation of Moving Image Material?” The Moving Image: The Journal of the Association of Moving Image Archivists 1, no. 2: 39–55. http://www.jstor.org/stable/41167061.
[^ester90image]: Ester, Michael. 1990. “Image Quality and Viewer Perception.” Leonardo. Supplemental Issue 3. 51–63. https://doi.org/10.2307/1557895.

## Lossy

Lossy is an adjective that is used to describe the loss of data or the dissipation of energy. 
While the first known use of the word had been attributed to the second definition in 1948, in the writing of H.A. Leiter,
the loss of data is more commonly associated with file [compression](../main/glossary.md#compression) in the computation field.[^Merriam-Webster] [^OED]

Although many people associate lossy compression as a digital thing, it also occurs naturally in everyday life. 
Creatures, including humans, are constantly processing a large amount of information from their environment in order to survive. 
However, absorbing all of that data can be overwhelming and not particularly useful, as such, they have evolved to be able to “reduce resource demands by tracking a smaller number of features”.[^Marzen17] 
In other words, they are compressing information in a lossy way, removing redundancies, to be able to quickly react to a stimulus.

File compression itself involves the act of representing data into a series of bits and reconstructing it back to uncompress it. 
Lossy and lossless compression are two types of techniques that can be used to achieve this effect. 
While lossless compression allows us to obtain an identical copy of the original, 
lossy compression returns an approximation of the original data which results in a less accurate but smaller copy. [^Gupta17] 

Lossy Compression is very important in our current society as a large amount of data is constantly being shared all over the world through social media and other platforms. 
As such, we need to be able to reduce the size of this data to be able to quickly transfer information. 
Thus, “lossy techniques are widely used in image and video compression” for their compression and decompression speed 
as well as the fact that it is less important for these visual media to be pixel accurate. [^Gupta17]


[^Merriam-Webster]: Merriam-Webster.com Dictionary, s.v. “lossy.” accessed March 5, 2025. https://www.merriam-webster.com/dictionary/lossy.
[^OED]: “Lossy, Adj. Meanings, Etymology and More.” *Oxford English Dictionary*. Accessed March 6, 2025. https://www.oed.com/dictionary/lossy_adj. 
[^Marzen17]: Marzen, Sarah E., and Simon DeDeo. 2017. "The evolution of lossy compression." *Journal of The Royal Society Interface 14, no. 130* (2017): 20170166.
[^Gupta17]: Gupta, Apoorv, Aman Bansal, and Vidhi Khanduja. 2017. "Modern lossless compression techniques: Review, comparison and analysis." In *2017 Second International Conference on Electrical, Computer and Communication Technologies (ICECCT)*, pp. 1-8. IEEE, 2017. 

## Machine Learning

Machine Learning, also known as ML, is a field of study in artificial intelligence that enables computers to learn from and make decisions based on data, without explicitly being programmed for each task. [^wikipedia] Overtime as the computer is exposed to more data, it’s able to learn from experience and improve upon its own performance, simulating human-like intelligent behaviour. [^mitsloan]

To train a Machine Learning system, data is gathered and provided to the system as training data. Programmers allow for the system to analyze the data and train itself by recognizing patterns and forming predictions. Programmers can also later tweak the system's parameters or test the system using new data to help push the model towards more accurate results. [^mitsloan]

There are three primary types of machine learning:

- Supervised Learning: The most common type of modern machine learning, Supervised Learning incorporates labeled datasets. This allows for the model to search for specifically sought after patterns. [^mitsloan] Supervised Learning uses example input-output pairs to train the model and uses two sets of data, training data and testing data, which creates a workflow that ensures more accurate results. [^mahesh]

- Unsupervised Learning: Unsupervised Learning is used to locate patterns we aren’t specifically looking for. [^mitsloan] There is no correct answer as to what models trained with Unsupervised Learning may find, the model is left to its own devices to recognize patterns and structure within the given data. [^mahesh]

- Reinforcement Learning: Reinforcement Learning uses a trial and error approach combined with a reward system to train the model in recognizing patterns and guide it towards better decision-making. [^mitsloan]

Machine learning has become integral to new media, influencing how content is created, distributed, and consumed. From the algorithms social media uses to curate a personalized feed, to AI chatbots like ChatGPT that are dominating the online world, Machine Learning is a core part of modern technology. [^duggal]

[^duggal]: Duggal, Nikita. “Machine Learning Applications: Popular Machine Learning Applications and Examples.” *Simplilearn*, 2025. https://www.simplilearn.com/tutorials/machine-learning-tutorial/machine-learning-applications#popular_machine_learning_applications_and_examples.  

[^mahesh]: Mahesh, Batta. “Machine Learning Algorithms: A Review.” *ResearchGate*, 2019. https://www.researchgate.net/publication/344717762_Machine_Learning_Algorithms_-A_Review.  

[^mitsloan]: MIT Sloan. “Machine Learning Explained.” *MIT Sloan School of Management*, April 21, 2021. https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained.  

[^wikipedia]: “Machine Learning.” *Wikipedia*. Last accessed March 7, 2025. https://en.wikipedia.org/wiki/Machine_learning.  


## Meme

*Meme* is an abstract form of communication usually allowing the transfer of information from one to the other. The term was first proposed by Richard Dawkins in his book, The Selfish Gene. Memes in their most common form are comedic internet items shared publicly on social spaces.

There is no way to definitely prove what the first internet meme was. However, most scholars agree that this was likely the first *meme*. 

:-)

This simple emoticon representing a smiley face was first created by Scott Fahlman in 1982. However it was only by 1993 that using the term meme to represent this online communication was suggested[^godwin-meme].

Since :-) memes have vastly evolved, mainly inline with technology.

### 2000s
In the 2000s, most internet users would either communicate on forums based on their hobbies, or a very minor list of generalized forums such as the now defunct Something Awful. This meant memes were created in much more isolated environments and were often closely tied to the culture they originated from. [^shifman-digital]

However, white collar workers also had access to this technology and would also often share memes in email chains and the like.
#### Examples:
[Dancing Baby](https://knowyourmeme.com/memes/dancing-baby)
[All Your Bases are Belong to Us](https://knowyourmeme.com/memes/all-your-base-are-belong-to-us)
[O RLY](https://knowyourmeme.com/memes/o-rly)
[Pedobear](https://knowyourmeme.com/memes/pedobear)

### 2005s
The 2005s introduced YouTube and video sharing to world of internet memes. Additionally, generalized places of discussion were now becoming the norm with Reddit and Digg. The first social media website had also been launched, MySpace, which allowed users to tie their real life identity to their online one.
#### Examples:
[Rick Roll](https://knowyourmeme.com/memes/rickroll)
[Chocolate Rain](https://knowyourmeme.com/memes/chocolate-rain)
[Dramatic Chipmunk](https://knowyourmeme.com/memes/dramatic-chipmunk)
[Advice Dog](https://knowyourmeme.com/memes/advice-dog)

### 2010s
In the 2010s, the internet became mainstream. Due the advent of social media sites such as Facebook and an increase in computer and phone ownership, the general public now used the internet. This had a major effect on meme culture, seeing much more widespread use.
#### Examples:
[Rage Comics](https://knowyourmeme.com/memes/subcultures/rage-comics)
[Loss](https://knowyourmeme.com/memes/loss)
[David After Dentist](https://knowyourmeme.com/memes/david-after-dentist)
[Bad Luck Brian](https://knowyourmeme.com/memes/bad-luck-brian)

### 2015s
This era is quite important to talk about. In 2016, Donald Trump won the election. Most political analysts attribute a major factor of this win to Trumps online presence[^moreno-monsters]. After this election memes were no longer seen as simple jokes shared around. Memes were now seen as political power, dogwhistle and indoctrination tactics.

Internet memes of this era also began self regurgitation. As memes have now become a cultural artifact, old memes carry new meaning in modern use. Memes as a concept were challenged as information-less memes became prominent.[^post-memes]
#### Examples:
[Donald Trump Bing Bong](https://www.youtube.com/watch?v=niSLZDVApdQ)
[Video Games Appeal to the Male Fantasy](https://knowyourmeme.com/memes/video-games-appeal-to-the-male-fantasy)
[MLG Edits](https://www.youtube.com/watch?v=vr9O0uR4-Kw)
[E](https://knowyourmeme.com/memes/lord-marquaad-e)


### 2020s and on
As years go by, the percentage of people born after the internet and widespread meme culture grows and grows. Memes have become a common way of communication, with certain groups of children primarily communicating in the media they consume online.
#### Examples:
[Erm, What the Sigma?](https://knowyourmeme.com/memes/erm-what-the-sigma)
[Misheard Lyrics](https://knowyourmeme.com/memes/misheard-song-lyric-speech-bubbles)
[JD Vance Face Edits](https://knowyourmeme.com/memes/jd-vance-edited-face-photoshops)
[Undertime Slopper](https://knowyourmeme.com/memes/sites/undertime-slopper)


### Chinese Meme Culture

We cannot discuss internet meme culture without discussing Chinese Meme Culture. Modern Chinese culture is deeply tied to internet meme culture. Though not as thoroughly apparent to western society, china went through a visual renaissance[^poplin-china] with the use of memes. In china, memes are commonly spread with anti establishment and counter culture messaging. 

For [example](https://b23.tv/veOrYve), a current modern trend is for random individuals to essentially 'trauma dump' while using a comedic filter.


![MemesKeyword](/images/memes-dumont.jpg)

---

[^shifman-digital]:Shifman, Limor. 2014. Memes in Digital Culture. Cambridge, Massachusetts: The MIT Press. http://site.ebrary.com/id/10776345.

[^godwin-meme]:Godwin, Mike. 1994. “Meme, Counter-meme.” _WIRED_, October 1, 1994. https://www.wired.com/1994/10/godwin-if-2/.

[^herbert-culture]:Herbert, Katherine. 2019. “Limor Shifman, Memes in Digital Culture.” *The International Journal of Community and Social Development* 1 (1): 92–94. https://doi.org/10.1177/2516602618806389.

[^post-memes]:*Post Memes*. 2019. Brooklyn, NY: punctum books. https://doi.org/10.21983/P3.0255.1.00.

[^moreno-monsters]:Moreno Almeida, Cristina, and British Academy. 2024. *Memes, Monsters, and the Digital Grotesque*. Oxford: Published for the British Academy by Oxford University Press. https://doi.org/10.5871/bacad/9780197267714.001.0001.

[^poplin-china]:Poplin, Justine. 2024. *Memes, Myth and Meaning in 21st Century Chinese Visual Culture*. Singapore: Palgrave Macmillan. https://doi.org/10.1007/978-981-99-2181-2.

## Microblogging

Microblogging[^micro2blogging] is a combination of instant messaging and content production. With a microblog, you share short messages with an online audience to improve engagement. 


Each microblogging website is significant and optimized for search engine optimization.[^micro3science] These services encourage high-quality link development by allowing users to express their thoughts in a few words or a phrase that may be connected to a variety of other websites currently operating on the internet. Microblogs are short (under 300 words) blog postings that include photos, GIFs, links, infographics, videos, and audio snippets. Despite the fact that the word “microblogging” is new, the practice is not. Many of us are microblogging without even realizing it. Let us illustrate this with an example. You make an instructional video or publish a whitepaper based on research. You write a brief synopsis that covers the major points and include an embedded link to the resource while uploading it. That is a microblog, right there! It is like a hybrid of instant chatting and regular blogging. 

**Even the synopsis we do every week for reading is kind of microblogging too!**

### Types of Microblogging Platforms

Most[^micro2blogging] brands already use microblogging platforms in their content plans.
As more customers look to develop relationships with companies, short and frequent social posts are critical. Additionally, microblogging appeals to the mobile browsing community. Examples of microblogging platforms include:


- Twitter: One of the best-known channels in the microblogging world. Twitter is a quick and  - convenient way to share short posts, GIFs, article links, videos and more.
- Pinterest: Companies on Pinterest link to products, articles and other useful information for audiences. Descriptions allow for quick content connections.
- Instagram: A visual form of microblogging, Instagram allows organizations to share stories and snaps as part of an online narrative.
- Facebook: One of the most popular social platforms, Facebook is an effective microblog channel. Users share text, live videos and more to connect with customers.


### Microblogging and New Media: Redefining Communication in the Digital Age

Based on what we know about microblogging, Microblogging is a core component of new media, transforming how information is produced, shared, and consumed. Platforms like Twitter, Tumblr, and Mastodon facilitate real-time communication, enabling users to post short, engaging updates. Unlike traditional media, microblogging emphasizes immediacy, interactivity, and decentralized content creation.  

A key aspect of microblogging’s impact is its role in news dissemination. Journalists and citizens alike use these platforms to report breaking news, provide live commentary, and engage in discussions. While this fosters a more democratic flow of information, it also raises concerns about misinformation and the rapid spread of unverified content.  

Microblogging has also revolutionized digital activism. Movements such as #MeToo and #BlackLivesMatter gained momentum through viral hashtags, mobilizing support and amplifying marginalized voices. The speed and accessibility of microblogging allow for widespread participation, though this can sometimes lead to performative activism rather than sustained engagement.  

In marketing and entertainment, microblogging fosters direct interaction between brands, celebrities, and audiences. Engaging content can quickly gain traction, reinforcing the role of algorithms in shaping visibility and influence. However, this also raises challenges related to echo chambers and algorithmic bias.  

Despite its benefits, microblogging poses challenges regarding content moderation and platform governance. The balance between free expression and responsible regulation remains a crucial debate. As AI-driven moderation tools evolve, ethical concerns surrounding censorship and digital rights persist.  

Ultimately, microblogging is a defining force in new media, reshaping communication, activism, and public discourse in an increasingly digital world.

### Major Impact of Microblogging Widely Use

Hate[^micro4hate] speech constitutes a major problem on microblogging platforms, with automatic detection being a growing research area. Most existing works focus on analyzing the content of social media posts. Our study shifts focus to predicting which users are likely to become targets of hate speech. 


[^micro2blogging]: sproutsocial https://sproutsocial.com/glossary/microblog/
[^micro3science]: Soumi, Dutta. 2006. *Data Analytics for Social Microblogging Platforms*, 3-38.https://www.sciencedirect.com/topics/computer-science/microblogging
[^micro4hate]: Sahrish, khan. 2024."Predicting the victims of hate speech on microblogging platforms" Heliyon Volume 10(23). https://www.sciencedirect.com/science/article/pii/S240584402416642X




## Middleware

Middleware is a type of software that can best be viewed as a layer between different systems and the outcome of their collaboration. Examples of these different systems are applications, databases, and operating systems. Examples of the outcomes of these systems’ collaboration are single sign-on (SSO) and API management. In other words, it simplifies the integration between various components to provide efficient and reliable services. [^1citation]

Middleware first made its appearance in a 1968 NATO Software Engineering report. Originally, the term designated the software that sat between application programs and service routines.[^2citation] Then, in the 1980s and 1990s, the exponential growth of the internet led to new middleware technologies being designed to support distributed systems. Today, middleware continues to be a term used to describe a wide range of services that are essential for building complex applications that are distributed.

By far, the most important role of middleware is in cloud computing. The foundation for modern cloud-native architecture is middleware. It allows developers to safely and efficiently scale applications across hybrid environments. The main advantage of middleware is that it creates reliable core application features rather than handling complex system interactions that are error prone. The main disadvantage is that the initial setup and maintenance of middleware systems can be complex.[^3citation]

In the 21st century, computing continues to evolve, thus the role of middleware is more important than ever. Done properly, middleware is a vital service that enable applications to communicate an across a wide range of platforms. Incorrectly chosen, middleware can slow down the process by producing bottleneck issues.[^4citation]

![Middleware](images/middleware-kerasias.png)

[^1citation]: Fersi, G. 2015. "Middleware for Internet of Things: A Study." In _2015 International Conference on Distributed Computing in Sensor Systems_, 230-235. Fortaleza, Brazil: IEEE. https://doi.org/10.1109/DCOSS.2015.43.
[^2citation]: Naur, Peter, and Brian Randell, eds. 2001. _Report on a Conference Sponsored by the NATO Science Committee: NATO Software Engineering Conference 1968. Garmisch, Germany: NATO, 1969_. Edited and reformatted by Robert M. McClure, Arizona.
[^3citation]: Youngblood, G. Michael. 2004. "Middleware." In _Handbook of Computer Networks and Distributed Systems: Middleware_, edited by Diane J. Cook and Sajal K. Das, 101-127. Hoboken, NJ: Wiley.
[^4citation]:Daily Mail. 2018. "Hawaii Man Gets Trapped in Gap Between Two Buildings." _Daily Mail_, March 21, 2018. https://www.dailymail.co.uk/news/article-5542715/Hawaii-man-gets-trapped-gap-two-buildings.html.

## MP3

MP3 (MPEG-1 Audio Layer 3) is a widely used digital audio format that has revolutionized the way people store, distribute, and consume music. MP3 is a lossy audio compression format that significantly reduces file size while maintaining acceptable sound quality by discarding audio data that is not sensitive to human hearing. This efficient compression makes MP3 key to the rise of digital music, online distribution, and portable media players, and has profoundly influenced contemporary digital culture and media consumption practices.[^brandenburg99mp3]

The importance of MP3 in new media is that it changed the economic model of the music industry, making file sharing and music streaming platforms possible. It is directly related to the evolution of peer-to-peer (P2P) networks, digital rights management (DRM), and streaming services such as Spotify and Apple Music.[^morris15selling]

The MP3 format was developed by the Moving Picture Experts Group (MPEG) under the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC). Its core figure was Karlheinz Brandenburg, who achieved efficient compression by optimizing perceptual coding techniques to remove parts of the audio signal that are imperceptible to the human ear.[^brandenburg94iso]

MP3 became widely known in the late 1990s, with the rise of the Internet and peer-to-peer file sharing services such as Napster (launched in 1999), which enabled the mass dissemination of digital music. This triggered a major conflict between copyright holders and consumers, and fundamentally changed the music industry's thinking about digital rights management and profit models.[^morris15selling2]

![MP3 Collage](/images/MP3-Wu.jpg)




[^brandenburg99mp3]: Brandenburg, Karlheinz. 1999. "MP3 and the Future of Digital Audio." Journal of the Audio Engineering Society 47(7/8): 558–566.
[^morris15selling]: Morris, Jeremy Wade. 2015. Selling Digital Music, Formatting Culture. Oakland: University of California Press.
[^brandenburg94iso]: Brandenburg, Karlheinz, and Gerhard Stoll. 1994. "ISO/MPEG-1 Audio: A Generic Standard for Coding of High-Quality Digital Audio." Journal of the Audio Engineering Society 42(10): 780–792.
[^morris15selling2]: Morris, Jeremy Wade. 2015. Selling Digital Music, Formatting Culture. Oakland: University of California Press.
[GitHub](../main/glossary.md#github), [Wiki](../main/glossary.md#wiki), [Affordance](../main/glossary.md#affordance)

## Multimedia

Multimedia refers to the integration of multiple media to deliver a richer message than one single form of media.[^Li21multimedia] By manipulating text, images, 
data and videos through technology (both modern and traditional forms) and including user interaction, one may allow more dimension for expression.

Earlier forms of multimedia included newspapers, one of the first mass communication mediums and one in which included text, imagery, and graphics. Over time, hardware used to create multimedia continued to improve, from rudamentary devices like phonograms
and wooden box cameras to CDs and Instant Cameras. Eventually, all the content captured by hardware went from analog media to digital media, marking a shift in how we could share and interact with media. This continuous improvement in hardware and shift in media allowed increased experimentation and expression using multimedia tools. Nowadays, with new digital media, users are now actively engaged with their "social ecosystem" rather than passively consuming media.[^Li21multimedia] 

More modern applications of Multimedia are thought to date back to the early 1960's, with interactive installations such as Ivan Sutherland and Bob Sproull's "The Ultimate Display".[^history30ultimate] The proposed installation involved a computer interpreting beams of light and outputting them into a small screen on a helmet.[^brown17virtual] The Ultimate Display was effectively the ancestor of modern virtual reality technologies, combining creative computation, physics and imagery to generate virtual objects. With the evolution of software and hardware in the 20th and 21st centuries, interactive programs like video games and social media emerged as some of the most commercial and prevalent forms of multimedia entertainment.

[^Li21multimedia]: Li, Ze-Nian, Mark S. Drew, and Jiangchuan Liu. 2021. Fundamentals of Multimedia. Cham, Switzerland: Springer

[^history30ultimate]: History Timelines. 2025. “Multimedia: History Timeline.” Accessed January 30, 2025. https://historytimelines.co/timeline/multimedia-. 

[^brown17virtual]: Brown, Johnathan, Elisa White, and Akshya Boopalan. 2017. “Looking for the Ultimate Display: A Brief History of Virtual Reality.” Boundaries of Self and Reality Online, March 3, 2017. https://www.sciencedirect.com/science/article/abs/pii/B9780128041574000128#:~:text=A%20computer%20interpreted%20the%20beams,Sturman%20%26%20Zeltzer%2C%201994).

![Multimedia](images/Multimedia-Aidan.png)

## Multiplatform

Multiplatform is a term used to describe cross compatibility with multiple devices. Within a new media context, this refers to software that is able to run on multiple different devices, for example Windows, iOS, Android and [Linux](../main/glossary.md#Linux) are all forms of software that are able to run on different forms of [hardware](../main/glossary.md#hardware), such as [computers](../main/glossary.md#computer) and phones. Multiplatform consumption has been around for decades, when people used to use radio as background noise for when they were reading the newspaper or magazines.[^chan18consumption]

The usage of multiplatform environments have greatly increased the diversity of content that has been distributed as well as the volume of content that is presently available.[^champion15diversity] Currently, we are able to access a plethora of content and media due to multiplatform compatibility. More content is being distributed across multiplatform channels and software due to the fact that many people have different means of access points and different modes of technology.

Due to the fact that modern technologies are constantly improving, there is a necessity for multiplatform softwares, such as with mobile devices, where there are differing and multiple types of mobile phones and tablets. Each operating platform within a mobile device requires separate standards, programming languages and distribution markets, which poses a challenge on software developers.[^corral12mobile] Thus, is it crucial to have a multiplatform software that allows for developers to be able to extend their reach towards different users.

![Multiplatform](images/multiplatform_armstrong.jpg)
*Tool: [OpenArt](https://openart.ai/home). Prompt: "Create an image that shows the various themes of multiplatform within a new media context."*

[^chan18consumption]: Chan-Olmsted, Sylvia M., and Min Xiao. 2018. "A Consumption Perspective." _Handbook of Media Management and Economics_: 317. Accessed March 15, 2025.

[^champion15diversity]: Champion, Katherine. 2015. "Measuring content diversity in a multi-platform context." _The Political Economy of Communication_ 3 (1): 39-56. Accessed March 15, 2025.

[^corral12mobile]: Corral, Luis, Alberto Sillitti, and Giancarlo Succi. "Mobile multiplatform development: An experiment for performance analysis." _Procedia Computer Science_ 10: 736-743. Accessed March 15, 2025.

## Natural Language Processing (NLP)


Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and human language.[^NLPIBM] It involves developing algorithms and models that enable computers to interpret, analyze, and generate human language in a way that is both meaningful and contextually relevant. By leveraging techniques from computational linguistics and machine learning, NLP facilitates tasks such as text classification, language translation, sentiment analysis, and conversational agent development.

The field of NLP has undergone significant evolution over the past decades. Early approaches were largely rule-based, relying on hand-crafted linguistic rules to process language.[^speechandlanguageprocessing] Over time, statistical methods were introduced, which paved the way for more scalable approaches. In recent years, the emergence of deep learning—especially transformer-based architectures like BERT has revolutionized NLP by enabling models to capture nuanced contextual information in language data.[^bert]

Applications of NLP span a wide range of areas. For instance, machine translation systems like Google Translate use deep [neural networks](../main/glossary.md#neural-networks) to improve language translation, making it more accurate. Virtual assistants like Siri and Alexa rely on NLP to understand and respond naturally to spoken language. In business, sentiment analysis tools process text from social media, reviews, and surveys to extract insights on public opinion and consumer sentiment.[^speechandlanguageprocessing] Most recently, generative AI chatbots are one of the most significant advancements in NLP, as they act as a culimnation of decades of research and development into computational linguistics.

![Natural language processing](/images/naturallanguageprocessing-vlamis.png)[^deepmind]

[^deepmind]: Cockx, Wes and Google DeepMind. 2023. *An Artist’s Illustration of Artificial Intelligence.* Pexels. https://www.pexels.com/photo/an-artist-s-illustration-of-artificial-intelligence-ai-this-illustration-depicts-language-models-which-generate-text-it-was-created-by-wes-cockx-as-part-of-the-visualising-ai-project-l-18069694/.
[^bert]: Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. https://doi.org/10.48550/arXiv.1810.04805.
[^speechandlanguageprocessing]: Jurafsky, Dan, and James H. Martin. 2009. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. 2nd ed. Prentice Hall Series in Artificial Intelligence. Upper Saddle River, N.J: Pearson Prentice Hall.
[^NLPIBM]: Stryker, Cole, and Jim Holdsworth. n.d. “What Is NLP (Natural Language Processing)? | IBM.” IBM. Accessed February 19, 2025. https://www.ibm.com/think/topics/natural-language-processing.

## Net Neutrality

Net neutrality is the principle that all data on the internet should be treated equally by internet service providers (ISPs), without discrimination or preferential treatment based on the type of content, user, website, or platform. [^wu03discrimination] It ensures that ISPs cannot block, throttle, or prioritize certain types of internet traffic, maintaining an open and level playing field for all online activities. [^schewick10internet]

### Significance
Net neutrality is a cornerstone of digital democracy and innovation in new media. It ensures that the internet remains a public resource where users have equal access to information and services, fostering creativity, competition, and free expression. [^kimball22battle] Without net neutrality, ISPs could control what users see and do online, potentially stifling innovation and limiting access to diverse viewpoints. [^mcchesney13disconnect]
However, the debate over net neutrality also highlights tensions between corporate power, government regulation, and public interest. As Danny Kimball notes in Net Neutrality and the Battle for the Open Internet, the fight for net neutrality is not just about technical regulations but about preserving the internet as a space for democratic discourse and cultural production. [^kimball22battle]

### Relation to Current Technologies and Media Practices
Net neutrality is deeply intertwined with contemporary digital technologies and practices:
- **Streaming Services:** Without net neutrality, ISPs could throttle streaming platforms like Netflix or YouTube, forcing them to pay for faster access or degrading their performance. [^wu03discrimination]
- **Emerging Technologies:** Technologies like 5G, IoT (Internet of Things), and cloud computing rely on an open internet to function effectively. Net neutrality ensures that these innovations are not hindered by ISP interference. [^schewick10internet]
- **Social Media and Content Creation:** Platforms like TikTok, Instagram, and YouTube depend on equal access to bandwidth. Without net neutrality, ISPs could prioritize their own content or charge creators for faster upload speeds, disadvantaging smaller creators. [^kimball22battle]

### Historical Origins
The concept of net neutrality is rooted in the design principles of the early internet and the regulatory traditions of telecommunications:
- **End-to-End Principle:** The internet was built on the end-to-end principle, which emphasizes that intelligence and functionality should reside at the edges of the network (users and applications) rather than in the core (ISPs). This principle, as explained by Barbara van Schewick in Internet Architecture and Innovation, ensures that the network remains neutral, allowing for permissionless innovation and equal access for all users. [^kimball22battle]
- **Common Carrier Tradition:** The idea of treating all data equally draws from the common carrier principle in telecommunications, which required phone companies to treat all calls equally without discrimination. This tradition influenced early debates about how broadband internet should be regulated. [^wu03discrimination]

The term "net neutrality" itself emerged in the early 2000s as broadband internet became more widespread, and concerns grew about ISPs potentially abusing their control over infrastructure to discriminate against certain types of traffic. [^schewick10internet]

### Key Figures
- **Tim Wu:** He coined the term "net neutrality" in his 2003 paper "Network Neutrality, Broadband Discrimination." Wu argued that ISPs should not be allowed to discriminate between different types of internet traffic, drawing parallels to the common carrier principle in telecommunications. His work laid the theoretical foundation for net neutrality advocacy. [^wu03discrimination]
- **Barbara van Schewick:** A leading scholar on internet architecture, van Schewick’s book *Internet Architecture and Innovation* provides a detailed analysis of how the internet’s design fosters innovation and why net neutrality is essential to maintaining this ecosystem. [^schewick10internet]
- **Free Press:** A media advocacy group that played a pivotal role in mobilizing public support for net neutrality through campaigns like Save the Internet. Their efforts were instrumental in shaping the public discourse around net neutrality and pushing for regulatory action.
- **FCC and Tom Wheeler:** The Federal Communications Commission, under Chairman Tom Wheeler, led the effort to reclassify broadband under Title II of the Communications Act in 2015, enabling the enforcement of net neutrality rules. This decision marked a significant victory for net neutrality advocates. [^mcchesney13disconnect]

### Influence on Contemporary Digital Culture and Society
Net neutrality has profound implications for digital culture and society:
- **Democracy and Free Expression:** An open internet ensures that diverse voices can be heard, fostering democratic discourse and cultural diversity. [^mcchesney13disconnect]
- **Economic Innovation:** By preventing ISPs from picking winners and losers, net neutrality allows startups and small businesses to compete on equal footing with established players. [^schewick10internet]
- **Global Impact:** The principles of net neutrality have influenced internet governance debates worldwide, with countries like India and the European Union adopting their own net neutrality regulations. [^kimball22battle]

![Net Neutrality](images/netneutrality-lapierrefurtado.png)
*ChatGPT was used to generate this image. Prompt: "Generate an image that represents the New media keyword "net neutrality", make it thoughtful and contemplative, based on the style of The Medium is The Massage by Marshall McLuhan and Quentin Fiore; and The Extreme Self by Hans Ulrich Obrist, Shumon Basar, and Douglas Coupland. Drawing inspiration from Babbage, Hayles, Lovelace, McLuhan, Nelson, Turing, Wiener, and others, expand on the media keywords and create a critical collage. This is not just an illustration of the concept, but it is a critical visual commentary, where the concept is presented and a commentary is provided."*

[^wu03discrimination]: Wu, Tim. 2003. “Network Neutrality, Broadband Discrimination.” SSRN Electronic Journal 2. https://doi.org/10.2139/ssrn.388863.
[^schewick10internet]: Barbara van Schewick. 2010. Internet Architecture and Innovation. The MIT Press EBooks. The MIT Press. https://doi.org/10.7551/mitpress/7580.001.0001.
[^kimball22battle]: Kimball, Danny. 2022. Net Neutrality and the Battle for the Open Internet. Ann Arbor, Michigan: University of Michigan Press. https://doi.org/10.3998/mpub.10067550.
[^mcchesney13disconnect]: McChesney, Robert W. 2013. Digital Disconnect: How Capitalism Is Turning the Internet against Democracy. The New Press. https://doi.org/10.3998/mpub.10067550.

## Neural Networks

A neural network is a sophisticated form of machine learning and subset of deep learning, designed to emulate the structure and functionality of the human brain[^thisisacitation1]. These systems, often referred to as “Artificial Neural Networks” (ANN) [^thisisacitation2], are inspired by the biological processes of neurons in the brain. They are composed of dense interconnected layers of nodes consisting of an input layer, multiple hidden layers for processing information, and an output layer[^thisisacitation3].

Each node within these layers is assigned a specific weight and threshold. When the input to a node surpasses its threshold, the node becomes activated, transmitting a signal to the next layer of nodes mirroring the way synapses fire in the human brain thus allowing a neural net to tackle complex problems and learn from data. Additionally, due to their interconnected nature, they are able to process vast amounts of information, recognize patterns, and make decisions with remarkable accuracy[^thisisacitation3].

If there are any students in this class who are further interested in how this functions mathematically, as I will not be covering this, they may consult the paper “Activation Functions in Artifcial Neural Networks: A Systematic Overview” by  Johannes Lederer[^thisisacitation4], outlining the neuron activation function and overall architecture as mathematical principles. 

The most common type of neural network, as in the architecture described above, is the Feedforward Neural Networks (FNN) [^thisisacitation1][^thisisacitation5]. FNNs are the simplest form of neural networks, characterized by their unidirectional flow of information, moving exclusively from the input layer, through the hidden layers, and finally to the output layer. This makes them particularly well suited for tasks such as classification and other linear problems. Another prominent example is the Convolutional Neural Network (CNN)[^thisisacitation1][^thisisacitation5], which is specifically designed for pattern identification and recognition using matrix manipulation and other algebraic principles. CNNs excel in tasks involving image and video analysis, detecting similarities, differences, and intricate patterns within data.

Their strengths become apparent in the context of new media art through this distinct aptitude for patterns. They can analyze datasets of existing artworks to identify styles, and features, enabling them to generate new pieces that either mimic or innovate upon existing. More importantly, neural networks can optimize art to align with aesthetic ideals, effectively mirroring the human perceptual process of identifying and evaluating sensorily pleasing qualities. Even without explicit training on visual art, CNNs can produce art-like outputs that exhibit fundamental principles like symmetry, composition, and more. By studying these outputs, researchers gain insights into how art itself is conceptualized in the mind, bridging the gap between models and human creativity. Thus, they not only have the ability to generate, but also provide a deeper understanding of the human cognitive process[^thisisacitation7].

The final aspect to discuss is the training process for neural networks. While difficult to explain, briefly, they are trained using large datasets, where the model iteratively refines its internal parameters, such as weights and biases, through a dominant design paradigm known as backpropagation if multilateral flow exists[^thisisacitation1]. This works by calculating the error between the neural network’s predictions and the actual target outputs and using the discrepancy to adjust its parameters, gradually minimizing errors and improving its accuracy over time, an iterative cycle of learning and refinement[^thisisacitation2]. 

To conclude, since the first trainable neural network was demonstrated by Frank Rosenblatt in 1957[^thisisacitation6], neural networks have evolved into a technology with widespread applications. Today, they are integral to advancements in fields such as facial and speech recognition, financial and weather forecasting, or simply providing accurate predictions that inform decision-making[^thisisacitation1]. Their versatility and power continue to drive innovation, making them a cornerstone of modern artificial intelligence.

![Neural Network](images/neuralnetwork-abdulaziz.png)

(Image created in Blender, edited in Adobe Illustrator)

[^thisisacitation1]: IBM. n.d. "What Are Neural Networks?" IBM. https://www.ibm.com/think/topics/neural-networks.
[^thisisacitation2]: Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. *Deep Learning*. MIT Press. https://www.deeplearningbook.org/.
[^thisisacitation3]: Haykin, Simon. 1999. *Neural Networks: A Comprehensive Foundation*. 2nd ed. Prentice Hall. https://github.com/anishLearnsToCode/books/blob/master/machine-learning/Neural-Networks-A-Comprehensive-Foundation-Simon-Haykin.pdf.
[^thisisacitation4]: Lederer, Johannes. 2021. “Activation Functions in Artificial Neural Networks: A Systematic Overview.” arXiv preprint arXiv:2101.09957. https://arxiv.org/abs/2101.09957
[^thisisacitation5]: Sharkawy, Abdel-Nasser. 2020. "Principle of Neural Network and Its Main Types: Review." *Journal of Advances in Applied & Computational Mathematics*. https://www.researchgate.net/publication/343837591_Principle_of_Neural_Network_and_Its_Main_Types_Review.
[^thisisacitation6]: MIT News. 2017. "Explained: Neural Networks." *MIT News*. https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414.
[^thisisacitation7]: Evans, Owain. 2019. "Sensory Optimization: Neural Networks as a Model for Understanding and Creating Art." arXiv preprint. https://arxiv.org/abs/1911.07068.

## Non-Fungible Tokens (NFTs)


Non-Fungible Tokens (NFTs) are unique digital assets that use a blockchain technology to be verified. They represent a specific item or content like digital art, music, or virtual real estate. It’s different from cryptocurrencies like Bitcoin or Ethereum [^ethereum.org], because they can be exchanged very easily from person to person but NFTs can’t. NFTs are different and can’t be exchanged like that because each token has unique attributes and different values. NFT’s importance in the world of new media can be seen through the way it has revolutionized the way digital content is made and sold by adding a mechanism for artists to monetize their work directly without need for other parties [^nature.com]. This invention has inspired the creators by giving them more control over their own intellectual property and has brought new ways for digital ownership. In the area of new media, NFTs have inspired a wave of virtual galleries and digital collectibles and has re-designed the way digital culture and commerce works. NFTs use blockchain technology to show the authenticity and ownership of digital assets. They have been used with many platforms, including:

Digital Art Platforms: Artists use NFTs to sell their digital artworks on platforms like OpenSea and Rarible, keeping the royalties through smart contracts.

Gaming: In-game assets, like characters and items, are seen as NFTs, players can own, trade, and monetize their virtual possessions.

Virtual Real Estate: Platforms like Decentraland and The Sandbox allow users to buy, sell, and develop virtual lands as NFTs, making new virtual economies.


The idea for NFTs became popular when ERC-721 standard on the Ethereum blockchain in 2018 was introduced. Early experiments with NFT assets date back to 2012 with projects like Colored Coins on the Bitcoin blockchain [^aisnet.org]. But it was the ERC-721 standard that gave a framework for NFTs to be adopted all over the world. The blockchain game CryptoKitties, launched in 2017, is also credited with bringing mainstream attention to NFTs by allowing players to collect and trade unique digital cats. NFTs have also drastically affected digital culture and society by redefining art ownership, they have gone against the normal ideas of art distribution, and allowed for digital artists to gain recognition and revenue. NFTs have also decentralized economies where creators and consumers engage directly. There is the issue concerning the environmental impact of blockchain technologies used on minting NFTs. Some examples of famous NFTs are Beeple's “Everydays: The First 5000 Days” which is a digital artwork sold as an NFT for $69 million at Christie's auction house in 2021 [^wsj.com]. Another one was CryptoPunks which was one of the earliest NFT projects, featuring 10,000 unique 24x24 pixel art characters, which have become highly desired digital collectibles. And lastly, Bored Ape Yacht Club is a collection of 10,000 unique ape NFTs that grant owners exclusive access to virtual events and communities, which shows how they can combine digital art and social experiences.[^chat.openai.com]



[^ethereum.org]: Entriken, William, Dieter Shirley, Jacob Evans, and Nastassia Sachs. “_ERC-721 Non-Fungible Token Standard._” Ethereum Improvement Proposals, 2018. (https://eips.ethereum.org/EIPS/eip-721).  
[^nature.com]: Nadini, Matthieu, Laura Alessandretti, Flavio Di Giacinto, Mauro Martino, Luca Maria Aiello, and Andrea Baronchelli. “_Mapping the NFT Revolution: Market Trends, Trade Networks, and Visual Features._” Scientific Reports 11, no. 1 (2021): 20902. (https://doi.org/10.1038/s41598-021-00053-8).  
[^aisnet.org]: Regner, Ferdinand, André Schweizer, and Nils Urbach. “_NFTs in Practice – Non-Fungible Tokens as Core Component of a Blockchain-Based Event Ticketing Application._” In _Proceedings of the 40th International Conference on Information Systems (ICIS)_, Munich: Association for Information Systems, 2019. (https://aisel.aisnet.org/icis2019/blockchain_fintech/blockchain_fintech/1/).  
[^wsj.com]: Zimmer, Ben. “_'Fungible': The Idea in the Middle of the NFT Sensation._” The Wall Street Journal, April 16, 2021. (https://www.wsj.com/articles/fungible-the-idea-in-the-middle-of-the-nft-sensation-11618595061).  
[^chat.openai.com]: ChatGPT. Response to “_Fix my spelling mistakes and make it read more professional,_” March 8, 2025. (https://chat.openai.com).  


## Object recognition

Object recognition is a subgenre of computer vision and artificial intelligence (AI) that allows machines to analyse, identify and classify objects through various media types such as images or videos. The process relies on deep learning models such as CNNs, convolutional neural networks, to detect objects with high accuracy [^Zhao2019]. Object recognition is regularly used in autonomous vehicules, security systems, and medical machines. [^Jiao2019] 


1-	Image Preprocessing: To improve the quality of the image, it will be using techniques such as noise reduction, contrast adjustments and resizing. [^Jiao2019]
2-	Extraction: Once it locks on an object, the system then identifies the unique features of the objects such as edges, textures and shapes to classify. [^Zhao2019]
3-	Model Training: Deep learning models such as YOLO or Faster R-CNN are trained using large datasets (COCO or ImageNet) in order to recognize object patterns. [^Liu2019]
4-	Detection and classification: The trained model takes in the new images, processes them by detecting the object then assigning them into categoris based on probability-based classification techniques. [^Zhao2019]


Many industries rely on object recognition. Autonomous vehicles for example would not exist without the implementation of object recognition. Slef-driving cars rely on object recognition to detect pedestrians, road signs, blockades and other vehicles in real-time. [^Zhao2019] Same goes for medical imaging. The technologies used in the medical field rely on object recognition to spot tumors, fractures and many more problems through X-Rays, MRIs and CT scans. [^Jiao2019] Surveillance systems uses facial and object recognition to enhance security. [^Jiao2019]


[^Zhao2019]: Zhao, Zhong-Qiu, Peng Zheng, Shou-Tao Xu, and Xindong Wu. 2019. “Object Detection with Deep Learning: A Review.” IEEE Transactions on Neural Networks and Learning Systems 30 (11): 3212–32. https://doi.org/10.1109/TNNLS.2018.2876865.
[^Liu2019]: Liu, Li, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietikäinen. 2019. “Deep Learning for Generic Object Detection: A Survey.” International Journal of Computer Vision 128 (2): 261–318. https://doi.org/10.1007/s11263-019-01247-4.
[^Jiao2019]: Jiao, Licheng, Fan Zhang, Fang Liu, Shuyuan Yang, Lingling Li, Zhixi Feng, and Rong Qu. 2019. “A Survey of Deep Learning-Based Object Detection.” IEEE Access 7. https://doi.org/10.1109/ACCESS.2019.2939201.



## Online Community
Creating online communities means using the internet to bring people together, helping them interact, and making them feel like they belong to a group. 
These communities exist on websites, social media platforms, and other digital spaces where people can talk to each other, share ideas, and work together. 
Whether it’s through discussions, sharing content, or supporting each other, these online spaces allow people with similar interests to connect and build relationships, no matter where they are in the world[^bell05creating].

When learning how online communities function, researchers discovered three primary interaction patterns: direct reciprocity (people helping others with the expectation of future returns),
indirect reciprocity (contributing without immediate expectations, trusting in community support down the line),
and preferential attachment (newcomers gravitating toward established members).
Interestingly, the results reveal that most online communities thrive through both direct and indirect reciprocity,
challenging previous assumptions that digital interactions primarily follow popularity-driven power-law patterns[^faraj11network].

Research shows extreme-right groups gaining momentum through online communities where they connect with ideologically aligned people.
These groups respond to real-world events like political shifts and social movements by intensifying discussions, organizing collective action,
and reinforcing shared beliefs[^biluc20growing].
The study demonstrates how internet platforms can accelerate these groups' growth and influence by facilitating the spread of their viewpoints while nurturing a powerful sense of community and shared purpose.

![Online Community](images/online-community-sherwin.png)

<sub>OpenAI. (2025). Prompt: "Create an image that depicts an online community". DALL·E. </sub>


[^bell05creating]: Bell, Steven J. 2005. “Creating Community Online.” American Libraries 36, no. 4: 68–71. http://www.jstor.org/stable/25649539.
[^biluc20growing]: Bliuc, Ana-Maria, John Betts, Matteo Vergani, Muhammad Iqbal, and Kevin Dunn. 2020. “The Growing Power of Online Communities of the Extreme-Right: Deriving Strength, Meaning, and Direction from Significant Socio-Political Events ‘in Real Life.’” International Centre for Counter-Terrorism. http://www.jstor.org/stable/resrep25261.
[^faraj11network]: Faraj, Samer, and Steven L. Johnson. 2011. “Network Exchange Patterns in Online Communities.” Organization Science 22, no. 6: 1464–80. http://www.jstor.org/stable/41303137.

## Open Source

The term “open source” refers to the source code of a project that is under an open source licence and is freely available for anyone to inspect, modify and redistribute [^opensource]. For source code to be considered open source, it must adhere to the Open Source Definition (OSD) created by the Open Source Initiative (OSI). This definition was written by Bruce Perens and acts as a bill of rights for users by certifying content as open source [^bruceperens].

The current criteria outlined by the Open Source Definition are as such:

The license must allow for free redistribution of the source code/content.
The content must include source code and allow for redistribution in form of source code.
The license must allow for modifications and for derived works to be redistributed under the same terms as the original license.
The license may require derived works to be distributed under a different name or version number to maintain the integrity of the original project.
The license must not discriminate against any group of people.
The license must not restrict usage in any field or endeavor.
The license must apply to all cases of use without the need of additional licenses.
The license must not be limited to particular software distribution.
The license must not impose restrictions on any other software used in the redistribution of the content.
The license must not be specific to any software, style or interface. [^bruceperens]

The concept of collaboratively developing a project with source code freely available to modify and redistribute was first introduced through the GNU project at MIT in the early 1980’s. In early 1998 the term “open source” was coined by Bruce Perens and Eric Raymond in response to the source code of the Netscape web browser being released [^cristinagacek]. This soon led to the establishment of the Open Source Initiative and the official Open Source Definition to create clear guidelines as to what qualifies as open source. 

A well-known example of open source software is Linux, an open source operating system based on the Linux kernel released in 1991 by Linus Torvalds [^linux]. Linux quickly gained popularity and replaced software with much more restrictive licenses, especially as the World Wide Web emerged and the internet became more mainstream [^ericramond]. Linux demonstrated the benefits of open source software due to its rapid development, and the open source license encouraged developers to feel comfortable while contributing to it.

Although the term originated from a software context and is most commonly used in said context, the open source concept extends onto many other fields such as art, education and research. Open Source recognizes developers as a part of  the user community [^cristinagacek], and emphasises the importance of collaboration and community in innovation.

![opensourceimage](images/opensource-kelly.jpg)

[^cristinagacek]: Gacek, Cristina, and Budi Arief. "The Many Meanings of Open Source." IEEE Software 21, no. 1 (January 2004): 34–40.

[^bruceperens]: Perens, Bruce. "The Open Source Definition." In Open Sources: Voices from the Open Source Revolution, edited by Chris DiBona, Sam Ockman, and Mark Stone, 171–188. Sebastopol, CA: O'Reilly Media, 1999.

[^ericramond]: Raymond, Eric S. The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary. O'Reilly Media, 1999.

[^linux]: "Linux." Wikipedia. February 2, 2025. https://en.wikipedia.org/wiki/Linux.

[^opensource]: "Open Source Software." Wikipedia. February 2, 2025. https://en.wikipedia.org/wiki/Open_source_software.

## Operating System (OS)

 An operating system is a fundamental software layer, often intrinsic to a piece of hardware, that manages hardware resources and essential services for computer programs. Operating systems essentially serve as an intermediary between users, applications/programs, and computer hardware. In new media contexts, operating systems are crucial for managing time-dependent data and supporting multimedia applications with specific performance requirements. [^candtsolution2024]
### Historical Evolution
The first portable operating system was developed in 1969 by Ken Thompson and Dennis Ritchie at Bell Labs, called UNIX. Since then, operating systems have evolved through four major generations. From UNIX's basic single-task capabilities, through IBM's OS/360 multitasking system, to MS-DOS (1981), and finally to modern Windows-based systems (1993 onwards) capable of handling many more tasks, with increased speed and efficiency. Each generation brought significant advances in functionality and user accessibility.
### Challenges and Evolution 
Operating systems have evolved from simple systems to sophisticated platforms capable of handling variable bit rate media, complex user interactions, as well as globally distributed multimedia applications. Current development focuses on balancing resource management with the unpredictable demands of modern multimedia applications; all while maintaining consistent performance and quality in an ever changing technological landscape (hardware and software). [^logan2024]
### Significance in New Media
- Operating systems provide efficient management of multimedia streams with precise timing requirements (Think user interactivity & real time collaboration softwares)
- Provides quality guarantees, essential for new and computationally expensive media applications, allowing people to create digital media with a level of trust and security in the robustness and foundation of their operating system.
- Supports distributed multimedia systems and real-time media processing
- Essentially, operating systems provide a framework for people to create and interact with new media. It is the interface between the unseen, inner mechanisms of a computer system, and the flexible world of software development and new media. 
### Key Examples
- Distributed multimedia systems for distance learning, managing simultaneous audio, video, and interactive whiteboard data 
- Real-time media processing systems requiring precise synchronization
- Modern storage systems optimized for multimedia content delivery [^plagemann2000]



[^plagemann2000]: Plagemann, Thomas, Vera Goebel, Pål Halvorsen, and Otto Anshus. 2000 "Operating System Support for Multimedia Systems." *Computer Communications 23, no. 3 *: 267-289. https://doi.org/10.1016/s0140-3664(99)00180-2.

[^logan2024]: Logan, Luke, Jay Lofstead, Xian-He Sun, and Anthony Kougkas. 2024 "An Evaluation of DAOS for Simulation and Deep Learning HPC Workloads." *SIGOPS Operating Systems Review 58, no. 1*

[^candtsolution2024]: C&T Solution Inc. 2024. "What is an Operating System (OS)?" *C&T Solution Inc.*. https://www.candtsolution.com/news_events-detail/what-is-an-operating-system/.

## Pantone

In 1963, Lawrence Herbert created the Pantone Matching System. Working for a commercial printing company 
named Pantone, he came up with a way to make colours more accurate in printing.[^KiernanPottedHistory] By having specific names 
and codes for each colour, artists and designers could choose the specific colour they wanted and have it stay 
consistent across their work.

The way Pantone colours work also increases the quality of print. Instead of using CMYK to create different 
colours, Pantone mixes the ink and has the colour ready. With CMYK printing, it’s typically an overlay of small 
dots crossing over each other to create the wanted colour. However, with Pantone colours, there aren’t those 
small dots, the colour can be printed evenly everywhere, making designs look more crisp. Additionally, Pantone 
provides a wide variety of colours, some of which are impossible to create with CMYK. For example, there are 
fluorescent or metallic inks.[^wageningen]

 In the 1980s Pantone was integrated into a variety of different software to help creators have more accurate
 colour-picking options. Companies such as Xerox Corporation, Adobe Systems Inc. and Canon Inc. implemented 
 the colours so work on screen could translate more effectively to physical mediums.[^pantoneWebsite] 


The Pantone Matching System doesn’t just affect printing on paper but is also used for all types of colour 
work. Pantone colours can be used in textiles, plastics, coatings and paints. Because of the accurate 
colour matching across multiple mediums, branding using colour was simplified. Companies and products could 
claim a specific Pantone colour and use it across their branding, making them more recognizable to consumers. 
Examples of such branding can be seen in Coca Cola red, Barbie pink and even countries have chosen 
specific Pantone colours for their flags. [^davisHistoryPantone]

![Pantone](images/pantone-wong.png)
*Tool: Dall-e. Prompt transcript: "please create an image of the concept of pantone colours and how their invention influenced the design world. Prioritize inks and colourful." "less dramatic please" "No, I want it colourful and fun and engaging but not with religious undertones"*. [Prompt link](https://chatgpt.com/share/67cf51ac-1900-800d-b04e-95bd151ed4ba)


[^pantoneWebsite]: “About Pantone”, Pantone, accessed March 8 2025, https://www.pantone.com/about-pantone?srsltid=AfmBOook_lr6H-HCkeBTFO3NIz-Wk0zztUeb1KL7jAISTQtmY23C7jsb
[^davisHistoryPantone]: Emma Davis. “The History Behind Pantone colours” *4Over4*. 2014. Accessed March 8, 2025. https://www.4over4.com/content-hub/stories/the-history-behind-pantone-colours?srsltid=AfmBOopz7FZPOXlmA_lg96mRrgdoHjZDMQlDHsddZzG9FHtveth58hGX
[^KiernanPottedHistory]: Steven Kiernan, “Pantone: A Potted History.” *Printing World* 17, no.10 (2008): 68-69. Accessed March 8, 2025. https://web-p-ebscohost-com.lib-ezproxy.concordia.ca/ehost/pdfviewer/pdfviewer?vid=1&sid=3ae1dfec-067b-4908-aef4-de1ce9d6fa81%40redis
[^wageningen]:Mark van Wageningen, *Type and colour : How to Design and Use Multicoloured Typefaces.* trans. Marie Frohling ( New York: Princeton Architectural Press, 2019), 118.




## Pattern Recognition

Pattern Recognition (PR) is the field focused on the automatic identification and classification of meaningful regularities within complex or noisy data using mathematical, statistical, and computational techniques. [^liu] It involves the discovery of patterns through computer algorithms, which allow for the classification of data into categories based on learned regularities. The goal is to build models that can accurately recognize and categorize new, unseen data by leveraging patterns derived from a training set, enabling machines to interpret and process information in dynamic and complex environments. [^bishop]

The four prominent approaches to pattern recognition include template matching, statistical classification, syntactic or structural matching, and neural networks. Template matching involves comparing patterns to predefined templates and measuring their similarity, though it can struggle with distorted patterns. Statistical classification represents patterns in a multidimensional space to establish decision boundaries based on probability distributions, often using techniques like discriminant analysis. The syntactic approach treats patterns as compositions of simpler subpatterns, akin to sentences in a language, allowing for both classification and structural description. Neural networks, inspired by biological systems, learn complex nonlinear relationships between inputs and outputs, making them effective for classification tasks and feature extraction, with common models including feed-forward networks and self-organizing maps. [^dutt]

Pattern recognition is widely utilized across various fields to analyze and interpret data.In healthcare, it aids in diagnosing diseases by recognizing patterns in medical images, such as identifying tumors in X-rays or MRIs. In finance, algorithms detect fraudulent transactions by analyzing spending patterns to flag anomalies. In the realm of security, facial recognition systems enhance surveillance by matching faces in real-time against databases. Additionally, pattern recognition plays a crucial role in natural language processing, enabling applications like sentiment analysis and speech recognition. In autonomous vehicles, it helps in interpreting sensor data to recognize road signs and obstacles. Overall, these applications highlight the versatility and importance of pattern recognition in modern technology and data analysis. [^bishop]

![Pattern Recognition](images/pattern_recognition_aneder.jpg)

[^liu]: Liu, Jie, Jigui Sun, and Shengsheng Wang. “Pattern Recognition: An Overview.” IJCSNS International Journal of Computer Science and Network Security 6 (June 2006). http://paper.ijcsns.org/07_book/200606/200606A10.pdf. 

[^dutt]: Dutt, Vinita, Vikas Chadhury, and Imran Khan. “Different Approaches in Pattern Recognition.” Computer Science and Engineering 1, no. 2 (August 31, 2012): 32–35. https://doi.org/10.5923/j.computer.20110102.06. 

[^bishop]: Bishop, Christopher M. Pattern recognition and machine learning. New York: Springer, 2016.
## PHP

**PHP (Hypertext Preprocessor)** is a widely used, open-source scripting language designed primarily for web development. It is embedded into HTML to create dynamic web pages and can interact with databases, manage sessions, and process forms. 

PHP was created in 1993 by **Rasmus Lerdorf** as a set of **Common Gateway Interface (CGI)** scripts to track visitors to his online resume. Initially called "Personal Home Page Tools," it evolved into **PHP/FI (Personal Home Page / Forms Interpreter)** in 1995. By 1997, it became the fully-fledged PHP language. PHP 3, released in 1998, marked its first major version, and it has continuously evolved with regular updates to improve performance, security, and functionality.


In the digital age, new media encompasses various forms of online communication, including websites, social media platforms, streaming services, and interactive content. PHP (Hypertext Preprocessor) is crucial in enabling these digital experiences by serving as a powerful scripting language for web development. Its ability to process dynamic content, manage databases, and facilitate user interactions makes it indispensable in the evolving landscape of new media.

One of PHP’s primary applications in new media is within **Content Management Systems (CMS)**. Platforms such as WordPress, Drupal, and Joomla utilize PHP, allowing users to create, manage, and publish digital content effortlessly. These systems enable individuals and businesses to maintain an online presence without requiring extensive programming knowledge. The flexibility and scalability of PHP-based CMS platforms contribute significantly to the proliferation of digital media.

Additionally, PHP has been pivotal in developing **social media platforms**. Facebook, one of the largest social networks, was initially built using PHP, highlighting its ability to handle vast amounts of user-generated content and real-time interactions. Although Facebook has since modified its backend, PHP remains a cornerstone in many social networking sites, prioritizing user engagement and content sharing.

The language is also essential in **streaming services**, where it helps deliver video, audio, and other interactive media. Websites like YouTube, Twitch, and other multimedia platforms rely on backend scripting to manage content distribution, handle user logins, and provide seamless streaming experiences. PHP’s integration with databases like MySQL ensures that media content is efficiently stored, retrieved, and delivered to users.

Furthermore, **e-commerce and forum-based websites** benefit from PHP’s capabilities. Platforms like Etsy, WooCommerce, and various online marketplaces use PHP to manage transactions, user authentication, and dynamic product displays. Community forums, including those designed for problem-solving and knowledge sharing, also leverage PHP to facilitate discussions and user interactions.

In conclusion, PHP remains a foundational technology in new media, driving the interactivity and functionality of modern websites and digital platforms. Its versatility, efficiency, and database compatibility make it a preferred choice for developers looking to create engaging online experiences. As new media evolves, PHP’s role in shaping the digital landscape will likely persist, ensuring dynamic and interactive web applications for users worldwide.

![PHP](images/PHP.png)

[^amanatidis16php]: Amanatidis, Theodoros, and Alexander Chatzigeorgiou. 2016. “Studying the Evolution of PHP Web Applications.” _Information and Software Technology_ 72 (April 1): 48–67. https://doi.org/10.1016/j.infsof.2015.11.009.  

[^lockhart15php]: Lockhart, Josh. 2015. _Modern PHP: New Features and Good Practices._ O’Reilly Media, Inc.  

[^wikipedia25php]: Wikipedia. 2025. “PHP.” _Wikipedia_, February 9, 2025. https://en.wikipedia.org/w/index.php?title=PHP&oldid=1274907653.  



## Pinch, zoom, and swipe



**1983**: Myron Kruger pioneered a form of pinch-to-zoom using suspended cameras and computers, not touchscreens [^mallery].\
**1991**: Pierre Wellner demonstrated the pinch gesture with hanging video cameras in his Digital Desk at Rank Xerox EuroPARC [^mallery].\
**1991**: Dean Rubine at Carnegie Mellon University developed a custom multi-touch system and named the pinch gesture "rotate-scale-translate" [^mallery].\
**1992**: The "Starfire" short film by Sun Microsystems featured the pinch gesture in a futuristic "Sun Video Collaboration Booth" [^mallery].\
**1998**: John Elias and Wayne Westerman founded Fingerworks, creating touchpads that led to Apple's iGesturePad after acquisition in 2005 [^mallery].\
**1998**: Touch-sensitive sensors appeared in Moog musical instruments, and the Lemur, the first screen-based multi-touch product, was developed [^mallery].\
**2004**: Neonode, a Swedish phone manufacturer, patented the Swipe function three years before Apple [^mick].\
**2006**: Jeff Han, a research scientist from NYU, was a key developer of the pinch-to-zoom feature [^mallery].\
**2007**: Steve Jobs demonstrated the pinch-to-zoom feature on the iPhone [^mallery].\
**2010**: The pinch-to-zoom technology was implemented in Android products [^mallery].

### Functions

There are various ways to use the pinch-to-zoom function on iPhones and Android multi-touch devices. First, we see the two-finger pinch which usually requires two hands, making it difficult for users on the go [^garg]. There are different ways to accomplish this, either by holding the phone with one hand and pinching with the other, supporting the device with both hands and using both thumbs to navigate the pinch-to-zoom function, or to hold the phone with one hand and using the thumb and index to complete this action [^garg]. Interactions such as the double-tap gesture enables users to quickly zoom in and out, but does not provide a direct zoom into the desired area of the screen and must be repositioned manually [^garg]. The swipe function enables users to quickly switch between pages, images or profiles, reacting much faster than using the drag function [^jeong]. 

New one-handed alternatives have been implemented to introduce additional gestures to control this function. VolumeZoom is a touchless function utilizing the volume-up key to zoom in, while volume-down zooms out [^garg]. GyroZoom uses devices’ real-time gyroscope sensor to activate the function, tilting the phone clockwise lets users zoom in, while anti-clockwise zooms out, using the angle of rotation to control the amount of zoom applied [^garg]. In 2014, designers developed an enhanced zooming technique called Pinch-To-Zoom-Plus (PZP), a method that helps reduce clutching during panning operations [^avery]. This function was developed based on research showing that zooming followed a predictable ballistic velocity curve, demonstrating that users have a tendency to pinch the center of the screen [^avery]. Additional research showed that tap-and-drag was favored to pinch-to-zoom, being quicker and more efficient based on the number of gestures involved [^farhad]. Furthermore, a team working at the Institut fur Informatik in Germany explored using the spiral gesture as a zooming mechanism, tracing inwards to zoom in and vice versa [^artinger]. 

[^artinger]: Artinger, Eva, Martin SchanzenbachF, Florian Echtler, Simon Nestler, Tayfur Coskun, and Gudrun Klinker. Beyond pinch-to-zoom: Exploring alternative multi-touch gestures for map interaction. Accessed March 16, 2025. https://www.researchgate.net/publication/266255635_Beyond_Pinch-to-Zoom_Exploring_Alternative_Multi-touch_Gestures_for_Map_Interaction. 
[^avery]: Avery, Jeff, Mark Choi, Daniel Vogel, and Edward Lank. “Pinch-to-Zoom-plus: An Enhanced Pinch-to-Zoom That Reduces Clutching and Panning.” Proceedings of the 27th annual ACM symposium on User interface software and technology, October 5, 2014, 595–604. https://doi.org/10.1145/2642918.2647352. 
[^farhad]: Farhad, Manoel, and I. Scott MacKenzie. “Evaluating Tap-and-Drag: A Single-Handed Zooming Method.” Lecture Notes in Computer Science, 2018, 233–46. https://doi.org/10.1007/978-3-319-91250-9_18. 
[^garg]: Garg, Saurabh, and I. Scott MacKenzie. “Comparison of Touch and Touchless Zoom Control Methods for Single-Handed Mobile Interaction.” Lecture Notes in Networks and Systems, 2021, 71–78. https://doi.org/10.1007/978-3-030-79816-1_9. 
[^jeong]: Jeong, Heejin, and Yili Liu. “Effects of Touchscreen Gesture’s Type and Direction on Finger-Touch Input Performance and Subjective Ratings.” Ergonomics 60, no. 11 (April 8, 2017): 1528–39. https://doi.org/10.1080/00140139.2017.1313457. 
[^mallery]: Mallery, Sam. “A Visual History of Pinch to Zoom.” Sam Mallery, August 20, 2015. https://www.sam-mallery.com/2012/09/a-visual-history-of-pinch-to-zoom/. 
[^mick]: Mick, Jason. Analysis: Neonode patented swipe-to-unlock 3 years before Apple. Accessed March 16, 2025. https://freerepublic.com/focus/f-chat/2849087/posts. 

## Post-Humanism 
Emerging in the late 20th and 21st centuries, posthumanism critiques Eurocentric humanism, which positions humans at the centre of the universe, and explores the interdependence of all matter—humans, animals, plants, and technology.[^K23Post]

Post-humanism can also be connected to the science fiction portrayal of superheroes who use technology to enhance their abilities, blurring the line between human and non-human [^Paul23Trans]. 
These technologies—ranging from artificial intelligence (AI) to advanced robotics—are shaping society by providing access to amounts of information and expanding human intelligence.
As these technologies evolve, they challenge traditional human capabilities, fundamentally altering the nature of consciousness itself. 

Post-humanism is significant in the context of current technologies because innovations like AI are transforming how people think and process information, accelerating cognitive abilities in ways that are impossible. 
The rise of AI, for instance, allows humans to interact with and analyze information faster, opening up new avenues for creativity and problem-solving.
This shift is fundamentally reshaping what it means to be human, pushing beyond the biological limits and re-evaluating human capacity in the digital age. 

As post-humanism becomes more intertwined with technological development [^Scott16The], it challenges the categorization of human vs. non-human
and raises questions about fairness, equality, and the ethics of technological enhancements. 

![post-humanism](images/post-humanism-sanchez.jpeg)

[^Paul23Trans]:Matthewa, Paul. 2023. Transparent Minds in Science Fiction: An Introduction to Alien, AI and Post-Human Consciousness. Cambridge, Uk: Open Book Publishers. https://books.openbookpublishers.com/10.11647/obp.0348.pdf 
[^Scott16The]: Jeffery, Scott. 2016. Posthuman Body in Superhero Comics: Human, Superhuman, Transhuman, Post/Human. New York: Palgrave Macmillan. https://doi.org/10.1057/978-1-137-54950-1.
[^K23Post]: Nayar, Pramod K. 2023. Post-Humanism. Oxford Bibliographies: Oxford University Press. https://www.oxfordbibliographies.com/display/document/obo-9780190221911/obo-9780190221911-0122.xml 

## Proprioception

In definition, proprioception is often described as the “sixth sense”, providing unique input beyond the five traditional senses. It is perceiving our body's position and movement in space without relying on visual or tactile cues. The term was intiially coined by English neurophysiologist Charles Scott Sherrington, to describe the positioning of body segments in space. Sherrington built upon existing literature and derived the word from the Latin "proprius," meaning one's own, and perception, thus literally designating one's own perception [^1]. This unconscious awareness enables us to comprehend how the parts of our body and mind relate to each other as well as how they interact with the environment. Unlike the five senses, it is less consciously discerned but plays a crucial role in embodiment, self-understanding and sensory interpretation [^2][^3]. 

From a metaphysical standpoint, proprioception blurs the lines between the mind and body. Dualistic frameworks outlined by classical philosophy separate the mind from the body, but proprioception demonstrates an integrated model where bodily awareness and one’s mental state are interdependent [^4]. Philosopher Maurice Merleau-Ponty argues that perception is not a passive vessel for external stimuli, but rather an active bodily engagement with the world [^5]. This said, If I know my hand’s position through an immediate, pre-reflective sense, then my mental state is inseparable from my physical state. Thus the mental and physical are not only correlated, but co-constitutive. Further discussions on the role of proprioception on the self are perhaps beyond the scope of this keyword, though classmates can consult “Perception and the Physical World” by D.M Armstrong [^6] for further explorations on the influence of the senses. 

Keeping this in mind, In traditional art, the viewer’s role is often passive, simply observing. New media however, often demands active participation, leveraging proprioception explicitly, engaging the body as both medium and participant. Proprioception’s significance continues to grow as artists seek immersive and interactive experiences that engage the user's entire body through various mediums and technologies, including VR & AR, interactive installations, or more general haptic technologies. New media experiences often respond to the user’s physical body in real-time, relying heavily on integrating proprioceptive information with the visual and auditory stimuli, or incorporating haptic feedback into digital environments, allowing developers to create more realistic, intuitive, and engaging interactions [^2][^7] Haptic devices provide tactile feedback, simulating the sense of touch and allowing users to interact with virtual objects in a more tangible way as the sense of touch provides crucial information about the body's interaction with its surroundings [^8]. Thus, the body’s state shapes the artwork, while the artwork reshapes the participant’s awareness of their body. This mirrors Merleau-Ponty’s idea of perception as a bodily dialogue with the world, with the inclusion of a responsive technological layer giving tangible output in the form of art.

To expand and finish, in class, we examined "Osmose" by Char Davies, which immerses users in virtual environments controlled by their movements through VR. Their proprioceptive sense of balance and motion dictated their navigation through ethereal, dreamlike spaces. The artwork does not simply represent a static idea, it responds to the user’s body, blending mental immersion with physical action. This creates a unified experience between output and input, mediated by technology, forming a continuous, hybrid self that includes the technological environment [^9]. We can define proprioception through a posthuman lens, If proprioception reveals a mind-body unity, art like the “Cloud Core Scanner” by Agnes Meyer-Brandis [^10], where participants "feel" cloud textures via haptic feedback, extends that unity into a symbiosis. One's bodily awareness informs the artwork, which reshapes awareness in turn, mediated by sensors and code. This aligns with Donna Haraway’s Cyborg Manifesto [^11], where boundaries between human and machine dissolve, and proprioception becomes a shared property to be explored.

![Proprioception](images/proprioception-abdulaziz.png)

Initial images made with Fluxdev in SwarmUI (trying something new), glowing insides inpainted with Juggernaut base SDXL in Fooocus.

Example positive prompt: cinematic still, blindfolded person, walking a tightrope, waves emanating from his body, dark atmospheric background, rim lighting, proprioceptive awareness visualization, cinematic composition, emotional, harmonious, vignette, 4k epic detailed, shot on kodak, 35mm photo, sharp focus, high budget, cinemascope, moody, epic, gorgeous, film grain, grainy

Example positive prompt for inpainting: glowing nervous system, gold, man, nerve endings

Example negative prompt (N/A in FluxDev, preset): (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3) anime, cartoon, graphic, (blur, blurry, bokeh), text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured

[^1]: Sherrington, Charles Scott. *The Integrative Action of the Nervous System*. New York: Charles Scribner's Sons, 1906.
[^2]: Gandevia, S. C. "Proprioception." *Current Biology*, vol. 10, no. 22, 2000, pp. R838-R841.
[^3]: Proske, U., and R. C. Gandevia. "The Proprioceptive Senses: Their Roles in Signaling Body Shape, Body Position and Movement, and Muscle Force." *Physiological Reviews*, vol. 89, no. 1, 2009, pp. 123-161
[^4]:Schneider, Susan. 2020. "The Mind-Body Problem: Dualism and Identity Theory." Stanford Encyclopedia of Philosophy (Winter 2020 Edition), edited by Edward N. Zalta. https://plato.stanford.edu/entries/mind-identity/.
[^5]: Merleau-Ponty, Maurice. 2012. Phenomenology of Perception. Translated by Donald A. Landes. New York: Routledge.
[^6]: Armstrong, D.M., 1961, Perception and the Physical World, London: Routledge.
[^7]: Biocca, Frank A., and Mark R. Levy. *Communication in the Age of Virtual Reality*. Hillsdale, NJ: Lawrence Erlbaum Associates, 1995.
[^8]: Okamura, A. M., et al. "Review of Haptic Interfaces for Surgical Training and Teleoperation." *IEEE/ASME Transactions on Mechatronics*, vol. 8, no. 1, 2003, pp. 2-16
[^9]: Dyson, Frances. 2009. “Immersion.” In Sounding New Media: Immersion and Embodiment in the Arts and Culture, 107–135. Berkeley: University of California Press
[^10]: Meyer-Brandis, Agnes. 2010. "Cloud Core Scanner – In the Troposphere Lab." Exhibition at Schering Stiftung, Berlin, January 15 – February 27.
[^11]: Haraway, Donna. 1985. “Manifesto for Cyborgs: Science, Technology, and Socialist Feminism in the 1980s.” Socialist Review, no. 80: 65–108.

## QR Code
 
QR code(quick-response code) is a type of two-dimensional code, first invented by a Japanese company Denso Wave in 1994[^wiki25Denso]. Compared to a regular barcode, a QR code has a faster reading speed and a higher storage capacity. In terms of recognition, a QR code does not require precise alignment with the scanner like a one-dimensional barcode does. It has a maximum error correction capability of 30%, meaning that even if up to 30% of the QR code is damaged, it can still be recognized. Today, QR codes are used in various fields such as payment, identity verification, logistics management, and marketing promotion. 


### History 
1994 : Denso Wave publicly announced QR Codes 

2002: Due to BSE (commonly known as the “mad cow disease”), QR gained attention and was applied in the manufacturing industry and supply chains.

2017: Both Apple's iOS 11 and Android 8.0 have incorporated QR code scanning as a native system feature.

2020: Due to COVID-19, QR codes have been used for payments and accessing menus to reduce physical contact. In some countries, QR codes are also used for COVID-19 contact tracing.
[^Uniqode24Paul]


### Uses 
#### Payment 
A QR code can store information such as payment account details, transaction amounts, and order information.When a user or merchant scans the QR code, the payment system quickly reads and decodes the embedded information, then sends the transaction request to the bank or digital wallet for identity verification, deduction, and transfer.In China, 95.7% of Chinese users prefer the QR code payment method.This new payment method has also facilitated the development of local e-commerce in China.[^QRT2025Ricson]

#### Restaurant ordering 
QR Code ordering is a method of placing food orders using QR code technology. Customers scan the QR code on the table, browse the menu, select dishes, and complete the payment directly on their smartphones, without the need for traditional paper menus or waiter intervention. QR Code ordering enhanced guest safety during the pandemic. It not only speeds up the service but also reduces the workload on staff, allowing them to focus on enhancing customer service elsewhere.At the same time, by reducing the use of paper menus, its application also helps protect the environment.[^Ovidius2024Anca]

#### QR code ticket 
QR Code Ticket uses QR code technology to function as an electronic ticket. This type of ticket contains all the necessary attendee information in a scannable format, and is commonly used in various scenarios such as events, travel, and entertainment, including concerts, movies, sports events, and train or flight tickets.

#### Supply chain management 
QR codes can be used in supply chain management to provide digital identification for products, goods, equipment, or documents, making data collection, tracking, management, and sharing more convenient and efficient. In terms of security features, QR codes have characteristics such as data immutability, fair and secure access, total transparency, enhanced traceability, and real-time validation.[^AICCSA2023Munir]


![qr-code](/images/qr-code-example.png)[^researchG2013ruil]


[^wiki25Denso]: Wikipedia. 2025. "Denso." last modified on 7 February 2025. https://en.wikipedia.org/wiki/Denso.

[^Uniqode24Paul]: Uniqode. 2025. "QR Code History." Last modified March 7, 2025. https://www.uniqode.com/blog/qr-code-basics/qr-code-history.

[^QRT2025Ricson]: QR Code Tiger. 2022. "QR Code Statistics 2022 Q1." Last modified March 7, 2022. https://www.qrcode-tiger.com/qr-code-statistics-2022-q1#toc_overall_qr_code_growth.

[^Ovidius2024Anca]: Anca Popescu, Nicoleta Andreea Neacsu. 2024. "QR Code Menus: Exploring the Role of QR Codes in Enhancing Operational Efficiency and Customer Engagement in Restaurants and Cafes." *Ovidius University Annals, Economic Sciences Series* 24(1). https://concordiauniversity.on.worldcat.org/search/detail/10455650155?queryString=qr%20CODE%20menu&bookReviews=off&newsArticles=off&databaseList=&clusterResults=true&groupVariantRecords=false.

[^AICCSA2023Munir]: Munir Majdalawieh. 2023. "Supply Chain Management Using Blockchain and QR Codes." in *2023 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA)* December 04-07. IEEE. https://concordiauniversity.on.worldcat.org/search/detail/10203810730?queryString=Building%20Trust%20in%20Supply%20Chains%3A%20The%20Blockchain-QR%20Code%20Advantage&bookReviews=off&newsArticles=off&databaseList=&clusterResults=true&groupVariantRecords=false.

[^researchG2013ruil]: Ruilopezia, A. 2013. "Examples of Potential Uses of QR Codes in Natural History Collections." *ResearchGate*. https://www.researchgate.net/figure/Examples-of-potential-uses-of-QR-codes-in-Natural-History-collections-A-Ruilopezia_fig1_258337578.
## Quantum computing
Quantum computing is a branch of computing that challenges classical computation by leveraging principles of quantum mechanics to process information. Qubits are the core and can be both 0 and 1 simultaneously. Due to this, it can perform complex calculations exponentially faster than traditional computation. [^1citation]

Another key concept is entanglement, which is a phenomenon where qubits become the state of one instantly affects the state of another. Distance here has no impact of their relation, so it makes this very powerful. [^2citation]
At its origins, quantum computing was originated by  Richard Feynman in 1982 [^3citation]. 

However, it wasn’t until later in the late 20th century, that Shor’s algorithm demonstrating how quantum computers could efficiently factor large numbers[^4citation].Today, companies like IBM, Google, and startups such as Rigetti Computing are pioneering real-world quantum hardware and software applications [^5citation].

Despite its promise, quantum computing faces challenges today like decoherence. Building stable quantum computers requires overcoming these hurdles [^6citation] . 
In the 21st century, computing continues to evolve,  and quantum computing is vital to that evolution. When designed and implemented effectively, it has the potential to redefine what is computationally possible.
 
[^1citation]:  Nielsen, Michael A., and Isaac L. Chuang. 2010. _Quantum Computation and Quantum Information_. 10th ed. Cambridge: Cambridge University Press. ↩
[^2citation]: Einstein, Albert, Boris Podolsky, and Nathan Rosen. 1935. "Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?" _Physical Review_ 47 (10): 777–780. https://doi.org/10.1103/PhysRev.47.777. ↩
[^3citation]: Shor, Peter W. 1994. "Algorithms for Quantum Computation: Discrete Logarithms and Factoring." In _Proceedings of the 35th Annual Symposium on Foundations of Computer Science_, 124–34. Santa Fe, NM: IEEE. https://doi.org/10.1109/SFCS.1994.365700. ↩
[^4citation]: Feynman, Richard P. 1982. "Simulating Physics with Computers." _International Journal of Theoretical Physics_ 21 (6): 467–488. https://doi.org/10.1007/BF02650179. ↩
[^5citation]: Arute, Frank, et al. 2019. "Quantum Supremacy Using a Programmable Superconducting Processor." _Nature_ 574 (7779): 505–510. https://doi.org/10.1038/s41586-019-1666-5. ↩
[^6citation]: Preskill, John. 2018. "Quantum Computing in the NISQ Era and Beyond." _Quantum_ 2 (79): 1–20. https://doi.org/10.22331/q-2018-08-06-79. ↩

## Quantum Cryptography

Quantum cryptography has a goal to secure communications and uses principles of quantum mechanics to do so. This opposes the traditional way of using computational mathematical abilities. Quantum cryptography provides information-theoretic security based on the fundamental laws of physics [^1citation].

Quantum Key Distribution (QKD) is the basis of this type of cryptography. It’s a technique that enables two parties to share encryption keys in a way that is provably secure against any computational attack. In other words, it’s a system that allows quantum-encoded information to be sent, and it can also alert the communicating parties if there are any issues [^2citation].

The idea of quantum cryptography originated in the early 1970s, but it was formally introduced in 1984 by Charles H. Bennett and Gilles Brassard [^3citation]. Since then, this field has expanded into real-world applications, including the Micius satellite, which enabled the first intercontinental quantum-secured video call [^4citation].

Despite its promise, quantum cryptography faces challenges, including photon loss in optical fibers. However, ongoing research into quantum repeaters and satellite-based is expected to address these limitations[^5citation]. 

Image prompt  :A Vogue editorial featuring two individuals dressed in elaborate medieval attire, standing against a clean white backdrop. The individuals hold striking green keys, glowing with subtle power and mystery. Their outfits blend classic medieval elegance with high-fashion elements, incorporating rich fabrics, gold accents, and detailed embroidery. The keys are a symbol of royalty and secrecy. Use hyperslats and intricate detailing to enhance the textures of the clothing and keys. Render the scene in 32k resolution, emphasizing the sharpness of fabrics, the light glinting off the keys, and the regal atmosphere. The lighting should be dramatic yet soft, highlighting the contrast between the medieval garments and the clean, modern backdrop. The overall aesthetic should feel timeless and futuristic, combining historical beauty with cutting-edge fashion, all in ultra-high definition

[^1citation]: Bennett, Charles H., and Gilles Brassard. 1984. "Quantum Cryptography: Public Key Distribution and Coin Tossing." In _Proceedings of IEEE International Conference on Computers, Systems, and Signal Processing_, 175–179. Bangalore, India: IEEE.
[^2citation]: Ekert, Artur K. 1991. "Quantum Cryptography Based on Bell’s Theorem." _Physical Review Letters_ 67 (6): 661–663. https://doi.org/10.1103/PhysRevLett.67.661.
[^3citation]: Gisin, Nicolas, Grégoire Ribordy, Wolfgang Tittel, and Hugo Zbinden. 2002. "Quantum Cryptography." _Reviews of Modern Physics_ 74 (1): 145–195. https://doi.org/10.1103/RevModPhys.74.145.
[^4citation]: Liao, Sheng-Kai, Wen-Qi Cai, and Juan Yin et al. 2017. "Satellite-to-Ground Quantum Key Distribution." _Nature_ 549 (7670): 43–47. https://doi.org/10.1038/nature23655.
[^5citation]: Pirandola, Stefano, Ulrik L. Andersen, and Leonardo Banchi et al. 2020. "Advances in Quantum Cryptography." _Advances in Optics and Photonics_ 12 (4): 1012–1036. https://doi.org/10.1364/AOP.361502.


## Ransomware 

Ransomware is a type of malware which prevents you from accessing your device and the data stored on it, usually by encrypting your files. A criminal group will then demand a ransom in exchange for decryption.[^UKsecurity]
**The birth of ransomware**: Historically, ransomware dates back to an original piece of malicious code, known as AIDS, written in 1989 by Joseph Popp.[^Allan]
**How does ransomware work**: 
 - Data access
 - Data encryption 
 - Demanding ransom

Ransomware is highly relevant to new media because digital platforms, online services, and media companies are frequent targets of cyberattacks. The relationship between ransomware and new media can be explored through several key points:

### How Ransomware Connects to New Media
Digital Dependency: New media relies on cloud storage, digital publishing, and online platforms, all of which are vulnerable to ransomware attacks.
Media Companies as Targets: News organizations, streaming services, and content platforms store valuable data, making them prime targets.
Disinformation & Cybersecurity Risks: Hackers may use ransomware to control or alter digital narratives, censor journalism, or manipulate public opinion.

**Where Ransomware is Used in New Media**
News Organizations: Attackers have targeted news agencies (e.g., The Guardian was hit by ransomware in 2022), disrupting operations.
Streaming & Entertainment Platforms: Netflix and other content providers face ransomware threats, with hackers leaking unreleased movies and shows.
Social Media & Influencers: High-profile social media accounts can be hijacked for ransom, causing reputational damage.
Gaming & Digital Art: Game developers and NFT creators have suffered ransomware attacks, losing valuable digital assets.

**Why It’s Relevant**
Threat to Press Freedom: Journalists can be silenced through ransomware attacks, undermining free speech.
Economic Impact: New media companies can lose millions due to downtime and ransom payments.
Evolving Cybersecurity Landscape: The rise of AI and digital platforms increases the risks of cyberattacks.

**Ransomware Examples**
   - SamSam : SamSam first appeared in 2015. It mainly targets healthcare businesses.
   - CryptoLocker :The Trojan Horse virus known as CryptoLocker spreads via unidentified attachments in staff emails. With CryptoLocker, only users of Microsoft® Windows® are in danger; Mac™ users are unaffected. Once your files get encrypted, a countdown timer starts.
   - Ryuk: Ryuk is one of the most harmful ransomware variants due to the extreme ransom amount demanded. Millions of dollars may be required to restore data after a Ryuk attack. Like other viruses, Ryuk spreads via phishing. After infecting a system, it begins shutting down operations on the victim’s computer.
   - Cerber : When Cerber initially debuted in 2016, it was incredibly profitable for attackers, earning them $200,000 in just July of that year. To infiltrate networks, it made use of a Microsoft® vulnerability.


**How to defend against cyber threats**:[^cyberCanada] 
1.	Cyber defence planning: There are several approaches you can take to enhance the protection of your networks and devices. The following list of items provides details on several security controls you can implement to effectively enhance your cyber security posture.
   1.1 Develop your backup plan: Develop and implement a backup plan for your organization. A backup is a copy of your data and systems that can be restored in the event of an incident.
   1.2 Develop your incident response plan: Developing an incident response plan for your organization is the keystone to your cyber defence strategy. You should also consider developing a disaster recovery plan for your business. Through these two plans, your organization considers major events that could cause an unplanned outage and require you to activate your recovery response. Your incident response plan helps you detect and respond to cyber security incidents. Your disaster recovery plan focuses on how the organization recovers and resumes critical business functions after an incident.
   1.3 Develop your recovery plan
   1.4 Manage user and administrator accounts

2. Cyber security controls
   2.1 Establish perimeter defences
   2.2 Implement logging and alerting
   2.3 Conduct penetration testing
   2.4 Segment your networks
   2.5 Constrain scripting environments and disable macros
   2.6 Patch and update
   2.7 Create an application allow list
   2.8 Use protective domain name system (DNS)
   2.9 Apply password management
   2.10 Use email domain protection

![Ransomware](images/ransomware-bolin.png)



[^UKsecurity]: *National Cyber Center UK*. https://www.ncsc.gov.uk/ransomware/home#section_4
[^Allan]: Allan Liska & Timothy Gallo 2016. *Ransomware: Defending Against Digital Extortion*. https://learning-oreilly-com.lib-ezproxy.concordia.ca/library/view/ransomware/9781491967874/preface01.html#_using_code_examples
[^cyberCanada]: Canadian Centre for Cyber Security. 2021. “Ransomware Playbook (ITSM.00.099).” Canadian Centre for Cyber Security, November 30, 2021. https://www.cyber.gc.ca/en/guidance/ransomware-playbook-itsm00099.

## Responsive Design

Responsive design is also known as Responsive Web Design (RWD). It is a web design technique to apply to websites to make them scalable and accessible for multiple devices. This allows developers to make website content adaptable for all screen sizes. 
The technique automatically adjusts the components displayed on the web page to the screen size. The term Responsive design was coined by web developer Ethan Marcotte in 2010[^GardnerRWD], he believed that a webpage should satisfy all users’ needs by having the pages adjust to their users. He published an article where he stated the top three important factors in creating a scalable website are flexible images, fluid grids and media queries[^NebelingRWD]. Responsive web design is applied through HTML5 and CSS3 web development languages. Flexible images that are either scalable on their own or they resize. A fluid layout utilizes a scalable grid layout to be resized based on the user’s screen size. Media queries allow the website to know which CSS3 properties it should apply to elements when the screen is resized. 


Responsive design helps users have a seamless browsing experience on a web page. This practice prioritizes user experience by changing the placements of elements on a webpage in response to the screen size. It remains a very accessible and usability-friendly practice. Responsiveness enforces affordance practices by allowing users to view the same website on different devices and still perform intended actions through different approaches. Responsive web design optimizes the user experience [^W3SchoolsRWD].
The RWD technique is innovative and has changed the history of web development for the better. 

Most websites hosted online follow responsive design best practices. Here are two examples: https://github.com and https://google.com.

![Responsive Design](/images/responsive-design-bodika.png) 

[^GardnerRWD]: Gardner, Brett S. 2011. "Responsive Web Design: Enriching the User Experience." Web Design Blog 11 (1): 15. https://www.webdesignblog.gr/wp-content/uploads/2012/03/5.pdf#page=15.
[^NebelingRWD]: Nebeling, M., Norrie, M.C. (2013). Responsive Design and Development: Methods, Technologies and Current Issues. In: Daniel, F., Dolog, P., Li, Q. (eds) Web Engineering. ICWE 2013. Lecture Notes in Computer Science, vol 7977. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-39200-9_47
[^W3SchoolsRWD]: W3Schools. 2024. "HTML Responsive Web Design." W3Schools. Accessed March 1, 2025. https://www.w3schools.com/html/html_responsive.asp.

## RGB
RGB colorspace stands for “Red, Green Blue” colorspace, which is considered an “additive” color model. It is considered additive due to the range of colors being created by adding wavelengths of the main colors together in different proportions. [^zelazko225rgbcolor]
This colorspace is usually used in screens and digital devices. Although different color depths exist for different screens, the most common way of referring to RGB colors is the HEX notation, characterized by 6 hexadecimal (0 to F) values. [^christensson19rgb] This notation is used due to the 24-bit color (256 values per color) being the nearest to the color spectrum visible to humans, making two hexadecimals per color perfectly represent those colors. [^christensson19rgb]

The idea of RGB colorspace was created because of a previous finding by Thomas Young in 1802, who proposed that the human eye had three cone cells. The colors corresponding to those cone cells were later discovered in 1851 by Hermann von Helmholtz. From there, in 1861, James Clerk Maxwell proposed the RGB color model, demonstrating it with photography plates of an image (of a tartan ribbon) that were taken with different filters. [^rhyne17applying] From then on, the RGB colorspace has been what we consider as the "light" colorspace. Other species who have different color cones in their eyes will see color differently than us, therefore would necessitate a different colorspace model. 

Although RGB is universal, the colors are display dependent, meaning that the colors for a same hexadecimal may appear differently on different screens. [^ibraheem12understanding] This is mostly due to variations in the manufacturing of the different layers of the screen, as well as the display technology used (LED vs OLED). [^benq22why]

The theory of RGB is very useful in new media, as the use of screens and lights are very common. Utilising different colors and color mixes in new media will necessitate a basic knowledge in RGB colorspace, as well as some knowledge of CMYK (Cyan, Magenta, Yellow, Black), and other models such as HSI (Hue, Saturation, Intensity), used for digital illustration, or RYB (Red, Yellow, Blue), used for painting. RBG and HSI have also been used together in Deep learning to optimize image denoising (using two colorspaces to add more data and therefore more definition to the denoising)[^Deng24rgb]

![RGB](images/RGB-godfroy.png)
Colorwheel image taken from a screenshot of the procreate colorwheel tool.

[^zelazko225rgbcolor]: Zelazko, Alicja. 2025. “RGB Colour Model | Description, Development, Uses, Science, & Facts.” Encyclopedia Britannica, https://www.britannica.com/science/RGB-color-model.

[^christensson19rgb]: Christensson, Per. 2019. “RGB,” https://techterms.com/definition/rgb#google_vignette.

[^ibraheem12understanding]: Ibraheem, Noor, Mokhtar Mohammed Hasan, Rafiqul Zaman Khan, and Pramod Kumar Mishra. 2012. “Understanding Color Models: A Review.” *ARPN Journal of Science and Technology 2* (3): 265–275.

[^benq22why]: BenQ. 2022. “Why Don’t Colors Look the Same Across Different Devices?” https://www.benq.com/en-ca/knowledge-center/knowledge/why-dont-colors-look-the-same-across-different-devices.html.

[^rhyne17applying]: Rhyne, Theresa-Marie. 2017. "Applying Color Theory to Digital Media and Visualization." *Boca Raton, FL : CRC Press, Taylor & Francis Group* : p.4

[^Deng24rgb]: K. Deng, P. Wang and Y. Qian, "RGB Images Enhancing Hyperspectral Image Denoising with Diffusion Model," ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp. 2960-2964

## Search Engine Optimization


Search Engine Optimization, also known as SEO, optimizes websites to help users find content faster and more efficiently. By developing SEO, websites are pushed to the top of the Search Engine Results Page (SERP), which allows users to engage with website content promptly. SEO also helps improve website traffic since users typically visit the top pages from their search query [^mailchimpSEO]. SEO is fundamental for web development since users rarely click on links found on the following result pages. 

Search Engine Optimization helps search engines understand the content on a website so it can process the information to help people discover content. SEO is widely used in new media for digital marketing purposes [^zilincanSEO]. To allow businesses to thrive and gain a lot of engagement with their clients they must be easily found through search engines. It helps to increase the customer base and website credibility as well. There are two types of SEO: on page and off-page optimization [^santhoshSEO]. On page delves into modifying the content and structure of a webpage like the schema and source code and adding content-specific keywords[^zilincanSEO]. Off-page SEO delves into gathering a user base from outside platforms to link to the targeted web page.

Search Engine Optimization is important in new media, since it shapes how users interact with digital content and determines what the user base sees. It structures content through SERPs and defines how it is interpreted within the digital space of the search engine[^mailchimpSEO].  SEO dictates how content is ranked, who can access it, and how algorithms influence its visibility and interpretation.

![Search Engine Optimization](images/search-engine-optimization-bodika.png) [^imageSEO]

[^mailchimpSEO]: Mail Chimp. “What Is SEO in Digital Marketing?” Mailchimp. Accessed January 29, 2025. https://mailchimp.com/marketing-glossary/seo/#:~:text=SEO%20means%20Search%20Engine%20Optimization,search%20engines%20rank%20them%20better.

[^zilincanSEO]: Zilincan, Jakub. 2015. “Search Engine Optimization.” In Proceedings of the CBU International Conference, 3:506–10. Accessed January 29, 2025. https://doi.org/10.12955/cbup.v3.645

[^santhoshSEO]:Santhosh, R.2018. “A Survey on Search Engine Optimization and Google's Search Engine Algorithms”. International Journal of Distributed and Cloud Computing 6, no. 1: 11. Accessed January 29, 2025. https://concordiauniversity.on.worldcat.org/atoztitles/link?genre=article&issn=23216840&title=International%20Journal%20of%20Distributed%20%26%20Cloud%20Computing&volume=6&issue=1&date=20180101&atitle=A%20Survey%20on%20Search%20Engine%20Optimization%20and%20Google%27s%20Search%20Engine%20Algorithms.&spage=11&pages=11-18&ID=doi:&id=pmid:&sid=EBSCO:Computers%20%26%20Applied%20Sciences%20Complete&au=Santhosh%2C%20R

[^imageSEO]:WIOR Magnifying Glass. 2025. "WIOR Magnifying Glass, 30X Handheld Magnifier." Amazon. Accessed February 8, 2025. https://www.amazon.com/WIOR-Magnifying-Magnifier-Magnification-Inspection/dp/B07RQ576YT

## Semantics
As a word “semantics” was first used by **Michel Bréal**, a French philologist (a language historian), in 1883. He studied how languages are organized, how languages change as time passes, and the connections within languages.[^footSemantics]
Semantics has expanded beyond linguistics into computer science, artificial intelligence, and digital marketing, transforming how information is structured and interpreted in the digital space.
In digital marketing and search engine optimization [(SEO)](keywords/search-engine-optimization.md), “semantic keywords” have gained prominence. Unlike traditional keywords that rely on exact matches, semantic keywords relate to the main topic, enabling search engines to grasp context.
This shift toward semantic search enhances the accuracy and relevance of search results, improving user experience. The **Semantic Web** exemplifies this application, relying heavily on semantic principles to optimize information retrieval.

The **Semantic Web**, proposed by **Tim Berners-Lee**, was designed to make data machine-readable, inspired by **Vannevar Bush’s** 1940s **"memex"** concept of a universal, searchable library [^leeHall2006]. 
This vision aimed to integrate metadata and ontologies into web content, allowing **AI-powered agents to understand relationships between data points autonomously**. **Modern search engines**, such as Google, leverage these advancements through **semantic indexing**,
ensuring more accurate search results by focusing on meaning rather than exact keyword matches [^Guha2003]. This shift was exemplified by Google’s 2013 Hummingbird update, which emphasized understanding user intent over literal word matching, marking a pivotal advancement in SEO.
Beyond search, semantic technologies have redefined social media and AI applications. Platforms like Facebook and Twitter use semantic algorithms to analyze user-generated content, interactions, and preferences, enabling targeted advertising and personalized content feeds [^kapanipathi2001]. 
These technologies illustrate how semantics bridges the gap between human communication and machine understanding.

In practical applications, semantic search and keyword optimization play a crucial role in digital marketing and content visibility. Semantic keywords capture the broader context of a topic, allowing for enhanced information retrieval and ranking in search engines.
Ultimately, semantics has become a fundamental pillar of the digital age, influencing how AI, search engines, social media, and content marketing function in an interconnected world.
By embedding meaning into data, semantic technologies enable more intuitive, efficient, and intelligent information processing.


[^footSemantics]: Foote, Keith D. 2023. "A Brief History of Semantics." DATAVERSITY. Last modified February 22, 2023. Accessed March 13, 2025. https://www.dataversity.net/brief-history-semantics/.
[^leeHall2006]: Shadbolt, Nigel, Tim Berners-Lee, and Wendy Hall. 2006. "The Semantic Web Revisited." IEEE Intelligent Systems 21 (3): 96–101. https://doi.org/10.1109/MIS.2006.62.
[^kapanipathi2001]: Sheth, Amit, and Pavan Kapanipathi. 2016. "Semantic Filtering for Social Data." IEEE Internet Computing 20 (4): 74–78. https://doi.org/10.1109/MIC.2016.86.
[^Guha2003]: Guha, Ramanathan, Rob McCool, and Eric Miller.2003.  _"Semantic search"_. In Proceedings of the 12th international conference on World Wide Web,https://dl.acm.org/doi/epdf/10.1145/775152.775250


                                                    

## Social Networking

Social Networking is the practice of building a digital network of people through multiple platform connections. It refers to someone’s online presence. This term was coined by an Australian sociologist named John Arundel Barnes through his work in “Class and Committees in a Norwegian Island Parish.” A social network uses different social platforms to connect people through digital spaces. They facilitate online communication directly within the application, to allow people from across the globe to connect easily [^InvestopediaSN].  

The most universal social network is [Facebook](facebook.com), an online scrapbook of what’s going on in a person's life, it allows users to find people who they know to grow their social network. According to research conducted by Hampton et al. (2011) “Facebook is, by far, the most popular SNS. Of those who use a SNS, almost all use Facebook (92%).” [^HamptonSN] Since Facebook is used for personal relationships and to give life updates.
A professional social network such as [LinkedIn](linkedin.com), is meant to showcase their career and educational achievements to make connections with professionals and catch up with the latest trends in their industry. This is tied to how Weaver and Morrison (2008) stated: “Social networking is the logical extension of our human tendencies toward togetherness, whether that socialization is down the hall or across the world.” [^WeaverSN]

In the context of new media theory, social networking is highly tied to interactivity and decentralization. It facilitates communication and the spread of information. From a decentralized standpoint, it replaces dated forms of communication with typical information outlooks. Social networking has changed the way people communicate online and helps to build new relationships with people who live anywhere in the world. It promotes diversity and intersectionality through a social landscape.

[^HamptonSN]: Hampton, Keith N., Lauren Sessions Goulet, Lee Rainie, and Kristen Purcell. 2011. Social Networking Sites and Our Lives. Vol. 1. Washington, DC: Pew Internet & American Life Project.
[^InvestopediaSN]: Kenton, Will. n.d. “What Is Social Networking?” Investopedia. Accessed March 16, 2025. https://www.investopedia.com/terms/s/social-networking.asp#:~:text=Social%20networking%20uses%20internet%2Dbased%20social%20media%20platforms%20to%20connect,recognition%20and%20encourage%20brand%20loyalty.
[^WeaverSN]: Weaver, Alfred C., and Benjamin B. Morrison. 2008. “Social Networking.” Computer 41 (2): 97–100.

## Software

Various software have become an indispensable factor of our everyday lives, being the driving force behind everything from how we communicate to how one may monitor their fitness progress. ‘Software’ itself refers to the general concept of operating information that a computer may use in order to execute tasks, such as computer programs. Some examples include databases like Microsoft SQL or Oracle Database, search engines like Google Chrome or Explorer and word processors including Microsoft Word. However, it is often easy to think of ‘software’ as a product that is simply meant to be bought and used when “With Microsoft’s rise to dominance in the software industry, it’s easy to think of software primarily as a product, something that is developed, packaged, and sold.In fact, shrinkwrapped PC applications represent only a small fraction of the total software in use,” [^OReilly34]. Open-source softwares are an incredibly significant portion of the software industry, as they allow for experimentation and collaboration between software engineers that can lead to important developments in order to keep up with our ever-quickening technological world. Barry Boehm states that these developments are ever increasing both due to “Gordon Moore’s Law (transistor and integrated circuit density and performance doubles roughly every 18 months), plus the continuing need for product differentiation and tornado like processes for new technology introduction [Geoffrey Moore, 1995]. Global connectivity also accelerates the ripple effects of technology, marketplace, and technology changes” [^Boehm5]. Moreover, there are far more areas of software development that are affected by technological advancements in order to keep up. As Meir Lehman and Juan C. Fernandez-Ramil explain in  Software Evolution and Feedback: Theory and Practice “Evolution phenomena in software-related domains are not confined to programs and related artefacts such as specifications, designs and documentation. Applications, definitions, goals, paradigms, algorithms, languages, usage practices, the sub-processes and processes of software evolution and so on, also evolve” [^LehmanRamil14]. In order for software developers to keep up with our rapidly developing world, it is important to understand the various influences and considerations that go into producing an effective software product.









[^Boehm5]: Boehm, Barry. 2005. Some Future Trends and Implications for Systems and Software Engineering Processes, Wiley InterScience, https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8cca145c899fd2a90b59fbcfaf025e6937ef3e3e
 [^LehmanRamil14]: Madhavji, Nazim H. 2010. Juan Carlos Fernández Ramil, and Dewayne E. Perry. Software evolution and feedback: Theory and practice. Chichester, England: John Wiley & Sons.
[^OReilly34]: O'Reilly, Tim. 1999. Lessons from open-source software development. Commun. ACM 42, 4 (April 1999), 32–37. https://doi.org/10.1145/299157.29916



## Stable Diffusion

Stable Diffusion is a deep-learning model designed for generating high-quality images from text prompts. Developed by Stability AI and released in 2022, it is an open-source latent diffusion model (LDM) that enables users to create photorealistic and artistic images with greater accessibility. Compared to earlier AI image generators, Stable Diffusion offers improved efficiency, control, and customization, making it a significant tool in digital art and media production.[^rombach22high]

Stable Diffusion operates using diffusion models, which gradually refine an image from noise through iterative denoising. Unlike Generative Adversarial Networks (GANs), which often struggle with mode collapse and high computational costs, diffusion models generate more consistent and high-resolution results. [^ho20denoising] This technology has led to widespread applications in digital art, game development, and AI-assisted design, shaping the future of new media and creative industries.

Stable Diffusion is particularly important in new media studies due to its impact on AI-generated content, digital storytelling, and interactive media. As an open-source tool, it enables artists and developers to create customized AI models, democratizing access to advanced image-generation technology. Moreover, its ability to synthesize high-fidelity visuals from simple text descriptions aligns with contemporary generative media trends, allowing for new forms of creative expression and digital content production. [^mccormack23ai]

Applications in Digital Media: 
AI-Generated Art – Used by artists to create original artworks, concepts, and illustrations.
Game Development – Generates textures, assets, and backgrounds dynamically.
AI-Enhanced Storytelling – Used in film, animation, and virtual world creation.

The breakthrough of Stable Diffusion lies in its migration of the diffusion process from high-dimensional pixel space to a compressed latent space, enabling efficient operation on consumer-grade GPUs. Training begins with processing billions of text-image pairs (e.g., LAION-5B dataset): textual descriptions are encoded into semantic features via CLIP, while images are compressed into 64×64 latent representations using a variational autoencoder (VAE), reducing computational load by 97%.

The core architecture employs an enhanced U-Net network, where an encoder-decoder structure with residual connections processes multi-scale features. Text conditioning is integrated through cross-attention mechanisms within the latent space. A cosine noise scheduler governs the diffusion process, prioritizing global structure learning in early training phases and fine detail refinement in later stages[^ho20denoising].

The training pipeline comprises two phases: initial pre-training uses mean squared error loss to teach noise prediction for image reconstruction, followed by fine-tuning with human-preference-guided reinforcement learning (e.g., Proximal Policy Optimization/PPO) to enhance aesthetic quality. While full training demands 150,000 compute hours on clusters of 64-256 A100 GPUs, open-source platforms like Hugging Face have democratized access through distributed training frameworks.


[^rombach22high]: Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. "High-Resolution Image Synthesis with Latent Diffusion Models." *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. https://arxiv.org/abs/2112.10752.

[^ho20denoising]: Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. "Denoising Diffusion Probabilistic Models." *Advances in Neural Information Processing Systems* 33: 6840–6851. https://arxiv.org/abs/2006.11239.

[^mccormack23ai]: McCormack, Jon, Oliver Bown, Simon Colton, and Mark d’Inverno. 2023. "AI and Creativity: A Critical Examination." *Journal of Artificial Intelligence Research* 76: 143–167. https://doi.org/10.1613/jair.1.13544.

## Streaming

Streaming is a term used to describe the transmission of data across multiple networks on a given device or [computer](../main/glossary.md#computer). The idea of streaming was first seen within the novel *Looking Backward* by Edward Bellamy. It presents a "music room" where music is transfered and transported by phone lines seamlessly into every home.[^harvey14station]

Nowadays, streaming takes on multiple forms, including music, video, live, and on-demand. Streaming websites such as YouTube and Twitch have blown up in popularity, and the entertainment and movie industry has also followed suit, with large corporations such as Netflix and Disney resorting to the creation of streaming services for the general public to use to engage with their movies or TV series.[^fagerjord19mapping] 

Streaming has transformed the landscape of the digital realm. It allows easy and quick accessibility for new media consumers of all types, ranging from music to TV. In fact, streaming services have displaced physical sales and digital downloads as the largest source of commercial music revenue in the US.[^chen17customer]

[^harvey14station]: Harvey, Eric. 2014. "Station to station: The past, present, and future of streaming music." _Pitchfork_. Accessed March 7, 2025.

[^fagerjord19mapping]: Fagerjord, Anders, and Lucy Kueng. 2019. "Mapping the Core Actors and Flows in Streaming Video Services: What Netflix Can Tell Us about These New Media Networks." _Journal of Media Business Studies_ 16 (3): 166–81. Accessed March 7, 2025.

[^chen17customer]: Chen, Yi‐Min, Hsin‐Hsien Liu, and Yu‐Chun Chiu. 2017. "Customer benefits and value creation in streaming services marketing: a managerial cognitive capability approach." _Psychology & Marketing_ 34 (12): 1101-1108. Accessed March 7, 2025.

## Subversion

Subversion in thought and action and technical choice in New Media, refers to the intention to undermine, challenge or question established structures, dogmas or conventions in the field and in society. Just like in any art form or knowledge production strategy, it may become necessary to employ the use of subversion as an attempt to sabotage, derail and refocus collective conversations around social justice, oppression and the erasure of marginalized voices, and to affect the material conditions conducive for action towards liberation and a new non-exploitative technological paradigm.
Such employment of the technique, manifested as blasphemous, provocative, profane, satirical, ironic, questioning, combative or simply alternate and divergent language and style, is the defining quality of such works and articulations as Donna Harraway’s Cyborg Manifesto[^Harraway], The VNS Matrix Cyberfeminist Manifesto[^VNS], and the vast array of resources available on The Anarchist Library[^AnLi].


[^Harraway]: Haraway, D. (1991). A Cyborg Manifesto: Science, Technology, and Socialist-Feminism in the Late Twentieth Century. In Simians, Cyborgs and Women: The Reinvention of Nature (pp. 149-181). New York: Routledge.

[^VNS]: VNS Matrix. n.d. The Cyberfeminist Manifesto for the 21st Century. Accessed February 19, 2025. https://vnsmatrix.net/projects/the-cyberfeminist-manifesto-for-the-21st-century.

[^AnLi]: The Anahist Library. n.d. Special Archive. Accessed February 19, 2025. https://theanarchistlibrary.org/special/index.

## Surveillance Capitalism
Surveillance capitalism is an economic model that emerged with the rise of big tech companies. Coined by Shoshana Zuboff, the term describes an economic system in which personal data is extracted and sold to influence human behaviour [^zuboff2023]. Unlike traditional capitalism, which transforms natural resources into commodities, surveillance capitalism commodifies the human experience itself. It turns behavioural data into a product for profit [^zuboff2023].

### History
Surveillance capitalism began in the early 2000s, largely driven by Google. Initially, Google collected user search data to refine its search algorithms. However, following the 2001 dot-com bust, the company faced pressure to generate revenue and realized that user data had untapped economic value [^zuboff2019]. Google’s breakthrough came when it started using behavioural surplus to create prediction products for targeted advertising. This marked a turning point, transforming Google from a search engine into a data business [^zuboff2019].

Zuboff also ties the expansion of surveillance capitalism to post-9/11 mass surveillance policies. Laws like the U.S. Patriot Act encouraged data collection, creating an environment where companies like Google could expand their data extraction practices with no oversight [^techtarget]. This development set the stage for the rise of big tech giants like Facebook, Amazon, and Apple to adopt data-driven business models [^techtarget].

### Digital Culture
Surveillance capitalism is deeply ingrained into modern digital infrastructure. While it relies on technologies such as machine learning, sensors, and data analytics, it is not the technology itself but the economic model behind it that defines its impact [^zuboff2019]. In this system, companies do not simply use data to enhance user experiences but rather to manufacture prediction products that advertisers and other clients purchase [^zuboff2019]. The model has expanded beyond advertising, affecting industries such as finance, healthcare, and even government [^zuboff2019].

One of the defining aspects of surveillance capitalism is the role of humans in it. Users are not customers but rather raw material for data extraction. Unlike a traditional market transaction, where consumers have choice and agency, surveillance capitalism operates without user consent or awareness [^zuboff2023]. The process is designed to keep individuals unaware of the extent of data collection and manipulation [^techtarget].

The influence of surveillance capitalism extends beyond targeted ads as it affects privacy, autonomy, and democracy. By continuously tracking user behaviour, companies not only predict but also shape human actions. They influence political opinions, consumer behaviour, and social interactions [^techtarget].

Additionally, surveillance capitalism normalizes a world where privacy is no longer a given. The increasing integration of smart devices, IoT technology, and AI-driven systems means that more aspects of human life are being recorded and monetized [^techtarget]. According to Zuboff, this shift risks our fundamental rights, as people lose control over their digital lives [^techtarget]. Understanding and addressing surveillance capitalism is essential for shaping a more ethical digital future.

![Surveillance Capitalism](/images/surveillancecapitalism-chakrabarty.png)

Prompt: can you give me an image that represents the essence of surveillance capitalism

[^zuboff2023]: Zuboff, Shoshana. 2023. *The Age of Surveillance Capitalism.* In *Social Theory Re-Wired: New Connections to Classical and Contemporary Perspectives*, edited by Wesley Longhofer and Daniel Winchester, 3rd ed. New York, NY: Routledge.

[^zuboff2019]: Zuboff, Shoshana. 2019. *Surveillance Capitalism and the Challenge of Collective Action.* *New Labor Forum, 28(1)*, 10-29. https://doi.org/10.1177/1095796018819461 

[^techtarget]: Barney, Nick. 2022. "Surveillance Capitalism." *TechTarget* https://www.techtarget.com/whatis/definition/surveillance-capitalism#:~:text=Big%20tech%20companies%20like%20Amazon,of%20things%20(IoT)%20devices.

## Technological Determinism

Technological determinism is the idea that society's transformation is heralded by technological innovation. In more detailed terms, the belief that any social change is determined by technological development, communications,
media etc, forming a very convenient "before and after" narrative structure.[^Smith94dilemma]
The term is widely thought to be coined by sociologist Thorstein Veblen, as a link between technology and society. 
However, an antithetical belief called social determinism argues that instead, it is technology that is influenced by social changes. Some also argue that both may be influencing one another with a mutual "push and pull",
where societyis formed by the technological change, and the technological change is then shaped by the society.[^Hauer17newmedia]

A pertinent theoretical example of technological determinism comes in the form of Christopher Columbus, adding to the "collective western lore" of deterministic narrative structure. For the better part of modern history, Europe had no knowledge on the "new world", until the invention of the compass, which allowed Columbus to colonize the new world.
Afterwards, new navigational equipment is invented as a result of the successful colonization, as a "precondition". Because of the inherent tangibility (and lack of abstraction) of historical technology, a certain efficacy is artificially formed within the memory of history that places emphasis on the consequences of the object.[^Smith94dilemma]

A subsection of the theory dictates that the consequences of technology are the result of poor use from people, as opposed to the nature of technological development. American writer Alvin Toffler described the age we live in as as a "third wave"
in which the continuous automation and development of technology has led to the acceleration of life's pace, leading to a major impact on the psyche of humans.[^Hauer17newmedia]

![Technological Determinism](images/technological-determinism.png)

[^Hauer17newmedia]: Hauer, Thomas. April 2017. “Technological Determinism and New Media.” International Journal of English Literature and Social Sciences 2, no. 2. https://doi.org/10.24001/ijels. 
[^Smith94dilemma]: Smith, Merritt Roe, and Leo Marx. 1994. "Does technology drive history?: The dilemma of technological determinism". Cambridge, Mass: MIT Press. 

## Training  

Training is the process of developing skills or abilities to improve performance for tasks. It can be both formal and informal, and take place in almost any setting, like in an educational or personal environment. Training is very important for most professional level jobs, athletic performances, and for skill requiring activities, and training allows people to change their ways to be able to use the newer technology or challenges that are required in their life. In the field of new media, training has a major role in building up a digital artist and designer’s skills that are needed for their job or passion during this time of new emerging AI tools and platforms. With these fast evolutions of technologies, constant training makes sure that professionals can stay up to date in a competitive industry[^1].  

Training can take many forms, here are some examples:

- **Online Courses and Tutorials**: Platforms like Coursera, Udemy, and LinkedIn Learning provide courses on digital design, coding, and multimedia production[^2].  
- **Workshops and Bootcamps**: Intensive, hands-on training sessions help people develop practical skills in areas like UX/UI design, game development, and digital storytelling.  
- **Corporate Training Programs**: Many tech companies invest in employee training to keep teams updated on the latest advancements in AI, cybersecurity, and digital marketing[^3].  
- **Interactive Learning Through AI and VR**: AI tutors and VR simulations offer new ways to make training more immersive and personalized.  

The idea of a structured training plan has its history rooted from the idea of apprenticeships, where people would learn trades through hands-on experience under the guidance of a master. In modern times, training has been implemented more into more formal settings, like in the education system and in corporations.[^4] The increase of online learning has made training more accessible, and has allowed for people to learn globally and to acquire specific skills from anywhere in the world. Training has made a change to digital culture by how it has given a huge amount of online resources to people, which made the idea of being a self taught worker feasible and made way for them to reshape the normal ways of making a career for themselves. But the digital divide is a concern because the access to quality training depends on whether the internet has unbiased available training and if they cost a lot of money or not.

### Paradigmatic Examples  

- **Google Career Certificates**: A program offering industry-recognized training in IT support, UX design, and data analysis.  
- **Codecademy**: An interactive learning platform for coding and programming languages.  
- **YouTube Tutorials**: A widely used resource for informal training in skills such as video editing and animation.  
- **MIT OpenCourseWare**: Free courses from MIT, making high-level education accessible to learners worldwide.  


[^1]: "AI and the Future of Workforce Training." *Center for Security and Emerging Technology*, Georgetown University. (https://cset.georgetown.edu/publication/ai-and-the-future-of-workforce-training/)
[^2]: “The Rise Of Online AI.” _Forbes_, 2020. ([https://www.forbes.com/digital-learning](https://www.forbes.com/sites/ilkerkoksal/2020/05/02/the-rise-of-online-learning/))  
[^3]: “How AI is Revolutionizing Training.” _TechCrunch_, 2025. ([https://techcrunch.com/ai-training-impact](https://techcrunch.com/sponsor/fluency/the-ai-revolution-using-artificial-intelligence-to-unlock-massive-time-savings/))  
[^4]: Lauterbach, Uwe. "Apprenticeship: History and Development of the German Model." *pedocs*, 2009. (https://www.pedocs.de/volltexte/2009/1730/pdf/Lauterbach_Uwe_Apprenticeship_History_and_Developement_of_D_A.pdf)  

## UNIX

UNIX is a multiuser, multitasking operating system developed in the late 1960s and early 1970s at AT&T's Bell Labs by Ken Thompson, Dennis Ritchie, and others. Known for its simplicity, portability, and powerful capabilities, UNIX has been influential in the development of many modern operating systems.[^raymond03art]

UNIX was created at AT&T's Bell Labs in the late 1960s by Ken Thompson, Dennis Ritchie, and others. The name “UNIX” is a pun on “Multics,” an earlier operating system project, reflecting its creators' intent to develop a simpler, more streamlined system.[^salus94quarter]

In new media studies, UNIX is significant because it provides a stable and flexible platform for various digital media applications. Its design principles have influenced the development of tools and systems that support multimedia processing, web hosting, and content creation. The UNIX philosophy of building simple, modular tools that can be combined in complex ways aligns with the collaborative and participatory nature of new media.[^mcilroy78unix]

Many contemporary operating systems, such as Linux and macOS, are derived from or influenced by UNIX. These systems are widely used in media production, web development, and other digital content creation fields. The command-line interface and scripting capabilities of UNIX allow for efficient processing and manipulation of media files, making it a preferred choice for professionals in the industry.

[^raymond03art]: Raymond, Eric S. 2003. _The Art of Unix Programming_. Addison-Wesley.

[^salus94quarter]: Salus, Peter H. 1994. _A Quarter Century of Unix_. Addison-Wesley.

[^mcilroy78unix]: McIlroy, M. D. 1978. "Unix Time-Sharing System: Foreword." _The Bell System Technical Journal_ 57 (6): 1899–1904.

## Unsupervised Machine Learning 

Unsupervised machine learning refers to a branch of artificial intelligence where algorithms analyze and interpret unlabeled datasets independently, without human supervision.[^kassambara17unsupervised_machine_learning]
These algorithms utilize raw data to identify patterns, similarities, and differences, enabling exploratory data analysis, customer segmentation, and image recognition.[^usama19unsupervised_machine_learning]

A fundamental technique within unsupervised learning is clustering, which categorizes data based on shared characteristics or distinctions.[^usama19unsupervised_machine_learning]
Clustering algorithms, such as exclusive, overlapping, hierarchical, and probabilistic methods, group data points into clusters based on their inherent similarities.[^gentleman08unsupervised_machine_learning]
Exclusive clustering, exemplified by the K-means algorithm, assigns each data point to a single cluster, whereas overlapping clustering allows for membership in multiple clusters with varying degrees of affiliation.[^gentleman08unsupervised_machine_learning]

Hierarchical clustering, whether agglomerative or divisive, organizes data into hierarchical structures based on similarity measures, forming dendrograms to visualize clustering processes. 
Probabilistic clustering, exemplified by Gaussian Mixture Models (GMMs), estimates data point assignments to probability distributions, facilitating soft clustering and density estimation tasks.[^kassambara17unsupervised_machine_learning]

Unsupervised learning, though devoid of target values, empowers machines to autonomously discern underlying structures within unlabeled data. 
Recent advancements, particularly in deep unsupervised learning, have bolstered the field's capacity for nuanced interpretation and analysis.[^usama19unsupervised_machine_learning] 
Additionally, the integration of unsupervised learning with other AI disciplines, such as reinforcement learning, augments adaptive and intelligent system capabilities, fostering innovation across diverse sectors.[^gentleman08unsupervised_machine_learning]

As unsupervised learning evolves, its application in uncovering hidden insights and structuring complex datasets continues to expand, underscoring its pivotal role in contemporary AI research and development.[^usama19unsupervised_machine_learning]


[^gentleman08unsupervised_machine_learning]: Gentleman, R., and V. J. Carey. 2008. “Unsupervised Machine Learning.” In Bioconductor Case Studies, by Florian Hahne, Wolfgang Huber, Robert Gentleman, and Seth Falcon, 137–57. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-77240-0_10.

[^kassambara17unsupervised_machine_learning]: Kassambara, Alboukadel. 2017. Practical Guide to Cluster Analysis in R: Unsupervised Machine Learning. Edition 1. Multivariate Analysis 1. Erscheinungsort nicht ermittelbar: STHDA.

[^usama19unsupervised_machine_learning]: Usama, Muhammad, Junaid Qadir, Aunn Raza, Hunain Arif, Kok-lim Alvin Yau, Yehia Elkhatib, Amir Hussain, and Ala Al-Fuqaha. 2019. “Unsupervised Machine Learning for Networking: Techniques, Applications and Research Challenges.” IEEE Access 7: 65579–615. https://doi.org/10.1109/ACCESS.2019.2916648.

## User Experience (UX)

User Experience (UX) design is crafting digital products or services that provide meaningful and relevant experiences to users. It involves a combination of elements such as usability, accessibility, and pleasure, to improve user satisfaction.[^hamidi23introductionUIUX]  The concept of UX extends beyond the visual appearance of a product. It focuses on the overall experience of interacting with it to consider the full range of user needs, behaviors, and emotions.[^hamidi23introductionUIUX] 

The term "User Experience" was coined by Don Norman in the early 1990s while working at Apple’s Advanced Technology Group.[^hernandez19origins] [^alves14state] As the head of the “User Experience Architect’s Office,” Norman sought to shift the design focus from just usability and interface design to a broader, more holistic view of the entire user experience.[^hernandez19origins] In the years that followed, UX became increasingly central to digital product development as the complexity of human-computer interaction (HCI) grew with the rise of personal computing, mobile apps, and the internet.[^alves14state] 

Related to the new media, UX design is essential to modern technology, especially in websites, mobile applications, and interactive products. With the increasing prevalence of digital devices, UX has become integral to ensuring users can interact with products effortlessly and intuitively.[^orlova16UXdesign] As new technologies such as artificial intelligence (AI), machine learning, and virtual reality (VR) continue to evolve, the field of UX design is adapting to meet new challenges.[^hamidi23introductionUIUX] For example, the integration of voice interfaces (e.g., Siri, Alexa) in digital products represents a growing trend in UX design, where voice commands and natural language processing become central to user interactions.[^hamidi23introductionUIUX]

Some of the most recognizable examples of effective UX design can be observed in platforms like Google and Facebook, where user-centric design ensures seamless navigation and engagement. Google's minimalist homepage and Facebook's intuitive user flows highlight how UX design can simplify complex processes and enhance user satisfaction.[^basuki22effects] Similarly, mobile apps like Uber and Airbnb demonstrate how UX design can improve usability by addressing user needs in real-time, creating efficient and enjoyable experiences that increase user retention.[^levy21UXstrategy]
The rise of augmented reality (AR) and virtual reality (VR) further emphasizes the significance of UX design.[^hamidi23introductionUIUX] In these environments, designers are tasked with creating immersive, interactive experiences that not only engage users but also consider their psychological and physical responses to virtual spaces. These new technologies challenge traditional UX paradigms and provide opportunities for designers to push the boundaries of user interaction.

UX design has significantly influenced contemporary digital culture by shaping how users interact with products and services. Companies like Apple, Google, and Amazon have made exceptional user experiences a cornerstone of their brands,[^hamidi23introductionUIUX] which sets high standards for user expectations. As technology becomes more integrated into daily life, the demand for user-centered, accessible, and engaging experiences continues to grow, which makes UX design a crucial factor in the success of digital products.

[^hamidi23introductionUIUX]: Hamidli, Nasrullah. 2023. "Introduction to UI/UX design: key concepts and principles." _Preuzeto_ 28: 2024.

[^hernandez19origins]: Hernández-Ramírez, Rodrigo. 2019. "On the origins and basic aspects of user-centered design and user experience." _Emotional Design in Human-Robot Interaction: Theory, Methods and Applications_: 71-92.

[^alves14state]: Alves, Rui, Pedro Valente, and Nuno Jardim Nunes. 2014. "The state of user experience evaluation practice." _Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational_: 93–102.

[^orlova16UXdesign]: Orlova, Mariia. 2016. _User experience design (UX design) in a website development: website redesign._

[^basuki22effects]: Basuki, Ribut, Zeplin Jiwa Husada Tarigan, Hotlan Siagian, Liem Satya Limanta, Dwi Setiawan, and Jenny Mochtar. 2022. "The effects of perceived ease of use, usefulness, enjoyment and intention to use online platforms on behavioral intention in online movie watching during the pandemic era." _Int J Data Netw Sci_ 6(1):253–262.

[^levy21UXstrategy]: Levy, Jaime. 2021. _UX strategy: How to devise innovative digital products that people want_.

## User Interface (UI)
A user interface (UI) is the medium through which humans interact with devices. It includes physical tools like keyboards and touchscreens as well as visual elements such as icons, buttons and animations. Some more concrete examples include a computer mouse, remote control and virtual reality headsets.[^hashemi24ui]

### History
Early humans used hieroglyphs to communicate. They are universally understood symbols and modern UI design uses this practice with its emphasis on icons and emojis to indicate an action. The typewriter was a pivotal innovation during the age of the machine as it introduced a tactile user interface. This later influenced the design of computer keyboards and digital text interfaces.[^reidworld] However, more concrete work on the user interface began with the development of a graphical user interface (GUI) based on Douglas Engelbart’s work on interactive computing in 1968. He was inspired by calls for technological advancements to do social good after World War II. Xerox PARC further refined GUI concepts, leading to commercial adaptations like Apple's Macintosh and Microsoft Windows. These innovations made computing from a command-line system that was only accessible to experts into a more intuitive and visual experience for everyday users. However, commercial interests began to influence user interface design. The priority is ease of use and market dominance over the original vision of deeper human-computer interactions.[^barnes10history]

### Contemporary UIs
UIs are essential in today's increasingly digital world because they determine how users interact with technology. UIs are directly tied to modern digital experiences. With the shift from Web 1.0 to Web 3.0, interfaces have adapted to allow more participation and personalization. Now, UI has advanced to natural user interfaces (NUI), which include voice commands, motion gestures and even biological signal recognition. This allows for more human-like interactions with technology. UI is evolving in industries like gaming, mobile devices and smart applications as human needs change.[^sharma21ui] It does so based on user experience which is another facet of human-computer interaction.[^hashemi24ui]

![User Interface](images/userinterface-chakrabarty.png) [^dell25keyboard][^tisanti16mouse][^rethin23humam][^wongios][^kamushken20figma][^elius20macintosh][^dev13microsoft]

[^dell25keyboard]: Dell. 2025. “Dell Wireless Keyboard KB500 Product Image.” Dell Canada. Accessed February 9, 2025. https://www.dell.com/en-ca/shop/dell-wireless-keyboard-kb500/apd/580-aklj/pc-accessories.

[^tisanti16mouse]: TiSanti. 2016. "Old Computer Mouse." iStock. Accessed February 9, 2025. https://www.istockphoto.com/photos/old-computer-mouse.

[^rethin23humam]: Rethinagiri, G. M. 2023. "Human-Computer Interaction with Computer Vision: Challenges and Innovations." Medium. Accessed February 9, 2025. https://medium.com/@gmrethinagiri1507681/human-computer-interaction-with-computer-vision-challenges-and-innovations-b99c24c8e6ff.

[^rr20mouse]: RR Auction. 2020. "Ground-breaking Engelbart 3-button computer mouse goes to auction." New Atlas. Accessed February 9, 2025. https://newatlas.com/computers/first-computer-mouse-engelbart-sale/.

[^wongios]: Wong, Lauren. n.d. "iOS: The User Interface Design Journey." Sush Labs. Accessed February 9, 2025. https://www.sushlabs.com/blog/ios-the-ui-design-journey.

[^kamushken20figma]: Kamushken, Roman. 2020. "Best Figma Plugins for 2020 Which Deserve Your Attention." Medium. Accessed February 9, 2025. https://kamushken.medium.com/best-figma-plugins-for-2020-which-deserve-your-attention-d542cc56aee0.

[^elius20macintosh]: Eliuseev, Dmitrii. 2020. "The 1984 Apple Macintosh: How Does It Look Today?" UX Design. Accessed February 9, 2025. https://uxdesign.cc/the-1984-apple-macintosh-how-does-it-look-today-d08dde79da05.

[^dev13microsoft]: DevToolsGuy. 2013. "An Introduction to the Microsoft 'Modern UI' Design Philosophy." Infragistics Blog. Accessed February 10, 2025.

[^hashemi24ui]: Hashemi-Pour, Cameron, and Fred Churchville. 2024. “User Interface (UI).” TechTarget. https://www.techtarget.com/searchapparchitecture/definition/user-interface-UI.

[^reidworld]: Reid, Danielle. n.d. “The World Is Our Interface: The Evolution of UI Design.” Toptal. https://www.toptal.com/designers/ui/touch-the-world-is-our-interface.

[^barnes10history]: Barnes, Susan B. 2010. "User Friendly: A Short History of the Graphical User Interface." _Sacred Heart University Review_ 16 (1): 4.

[^sharma21ui]: Sharma, Vatsal, and A. Kumar Tiwari. 2021. "A study on user interface and user experience designs and its tools." _World Journal of Research and Review (WJRR)_ 12, (6): 41-45.

## User

A user is an individual who interacts with products, systems, or platforms not just for their basic functions but as part of their broader activities, goals, and experiences. Users bring their own preferences, histories, and lifestyles into their interactions with technology, shaping how they evaluate, use, and even inspire the development of new products or features. Their needs, perspectives, and the context in which they use technology play a key role in shaping digital tools and their evolution.[^margolin97user]

In the digital world, users do more than passively consume content—they actively engage with platforms, tools, and media. They create, share, and interact with digital content, influencing trends, brands, and the way media is consumed. This participatory role is a key aspect of the user reflects a change in power dynamics, where users shape content consumption patterns, influence advertising strategies, and redefine the digital ecosystem.[^rashtchy07user]

Users contribute content through photos, posts, and comments while also generating data through actions like clicking, tagging, and networking. However, they often have little control or awareness over how their contributions and personal data are collected and used by technology companies, often for commercial purposes.[^reyman13user]

Beyond content creation, users play a key role in shaping the systems they interact with. Their feedback and satisfaction influence how applications, platforms, and tools evolve. The way users engage with a system—based on their expectations, experiences, and needs—drives improvements and updates. This continuous feedback loop helps refine technology to better match user behaviors and demands. While measuring user satisfaction can be complex, understanding how users interact with systems is essential for creating more effective and user-friendly applications.[^etezadi91user]



[^etezadi91user]: Etezadi-Amoli, Jamshid, and Ali F. Farhoomand. “On End-User Computing Satisfaction.” MIS Quarterly 15, no. 1 (March 1991): 1. https://doi.org/10.2307/249428.

[^margolin97user]: Margolin, Victor. “Getting to Know the User.” Design Studies 18, no. 3 (July 1997): 227–36. https://doi.org/10.1016/s0142-694x(97)00001-x. 

[^rashtchy07user]: Rashtchy, Safa, Aaron M. Kessler, Judith C. Tzeng, Nathaniel H. Schindler, and Paul J. Bieber. The User Revolution: The new advertising ecosystem and the rise of the internet as a mass medium. Minneapolis, MN, MN: PiperJaffray, Investment Research, 2007. 

[^reyman13user]: Reyman, Jessica. “User Data on the Social Web: Authorship, Agency, and Appropriation.” College English 75, no. 5 (May 1, 2013): 513–33. https://doi.org/10.58680/ce201323565. 
## Virtual Environment

Virtual environments are the technology which facilitates interactions between users and computer-generated artificial environments.[^ve3] These interactive computer displays are designed to create the illusion that the user has been transported to a different location.[^ve1] In a virtual world, the user can use their senses and their body in a similar way to the real world. They can turn their head to see their surroundings, trace sounds to the direction they are coming from, and use their hands to pick up virtual objects [^ve2]. Virtual environments are thus the most natural form of interaction between humans and computers.[^ve2] 

![Virtual environment](images/virtualenvironment-vigliensoni-genai.jpg)
*Tool: [Stable Diffusion Web](https://stablediffusionweb.com). Prompt: “Image of a virtual environment. It is a place where humans and computers interact naturally.”*

[^ve1]: Ellis, Stephen R. 1994. “What Are Virtual Environments?” _IEEE Computer Graphics and Applications_ 14 (1): 17–22. https://doi.org/10.1109/38.250914.
[^ve2]: Mine, Mark R. 1995. “Virtual Environment Interaction Techniques.” _UNC Chapel Hill Computer Science Technical Report TR95-018_.
[^ve3]: Youngblut, Christine, Rob E. Johnston, Sarah H. Nash, Ruth A. Wienclaw, and Craig A. Will. 1996. _Review of Virtual Environment Interface Technology_. Alexandria, VA: Institute for Defense Analyses. Accessed January 20, 2025. https://apps.dtic.mil/sti/tr/pdf/ADA314134.pdf.
[^reschke07wallpaper]: Reschke, Michael. 2007. “800x600 Wallpaper Blue Sky.png.” Wikimedia Commons. December 27. https://commons.wikimedia.org/wiki/File:800x600_Wallpaper_Blue_Sky.png.
## Virtual Reality (VR)

Virtual reality (VR) is a computer-simulated technology that generates a three-dimensional virtual environment. Users obtain an immersive sensory simulation experience — primarily visual — through motion trackers and 3D display screens. At current technological levels, standard VR environments are implemented via virtual reality headsets or multi-projection systems, creating realistic virtual worlds by combining images, sound effects, and other sensory inputs. Auditory and visual feedback are typically employed to maximize user immersion, with some devices additionally incorporating haptic feedback or force response mechanisms. Through the introduction of sophisticated algorithms, users can freely navigate within the virtual space, observe their surroundings, and interact with virtual objects.


### Applications 

Virtual reality (VR) technology can be applied in the education sector to provide students with more immersive and interactive learning experiences. Through immersive simulations and interactive elements, VR stimulates students' learning interest, making it more effective at enhancing educational outcomes compared to traditional tools like electronic courseware or online courses. Well-designed VR programs not only capture students' attention in classroom settings but also encourage voluntary engagement in home environments, thereby amplifying learning effectiveness.
However, VR educational software also faces limitations. Current VR equipment remains cost-prohibitive, and its physical space requirements exceed those of traditional educational setups. Additionally, there is a shortage of VR content resources, and the development costs for educational VR software are high. Designing VR programs is inherently more complex than standard curriculum development. While creating intricate and engaging virtual environments, developers must address the risk of students becoming distracted during use, potentially deviating from the primary educational objectives.[^Virvou2008CE]


Virtual Reality (VR) technology has been extensively adopted across entertainment sectors such as gaming, film, social networking, concerts, and theme parks, with gaming representing the most significant application area. By utilizing VR headsets (e.g., Oculus Quest, HTC Vive, PlayStation VR) combined with controllers, motion-sensing devices, or full-body tracking systems, players can immerse themselves in game content with unprecedented depth. VR gaming enables natural interactions through gesture recognition, voice commands, and physical movements, allowing free exploration within three-dimensional environments. Taking Half-Life: Alyx as an example, VR systems capture real-time player motions and translate them into precise in-game feedback. Compared to traditional screen-based gaming, VR delivers heightened immersion, expanded creative possibilities, enriched entertainment value, and superior audiovisual aesthetics.[^SDB2017P]


VR technology delivers multifaceted value in commercial applications, effectively attracting potential customers, enabling dynamic product demonstrations, and significantly boosting brand exposure. Its core business implementations can be categorized into four key domains: product design & prototyping, employee training & skill simulation, virtual marketing & customer experience enhancement, and remote collaboration & virtual meeting innovation. A prime example is the initiative by Scotland's Museum of Lead Mining. By deploying VR marketing strategies to engage potential visitors and elevate brand awareness, the museum introduced an immersive virtual mine tour accessible across multiple devices including smartphones and computers. Performance metrics reveal a 12% user interaction rate for their VR campaign, far surpassing both the service industry average (0.13%) and cross-sector benchmarks (0.27%). Notably, this initiative drove substantial brand recognition growth, with the museum's Facebook followers increasing by 17% because of the campaign.[^kj2023Business]


![Fork the repo](/images/VR-education-example.PNG)
A teacher uses Virtual Reality to teach math.[^viewhub2021Youtube] 

![Fork the repo](/images/VR-game-example.PNG)
VR game: Half-Life: Alyx[^valve2019Youtube]




[^Virvou2008CE]: Virvou, Maria, and George Katsionis. 2008. "On the Usability and Likeability of Virtual Reality Games for Education: The Case of VR-ENGAGE." *Computers & Education* 50 (1): 154–178. https://concordiauniversity.on.worldcat.org/search/detail/5902295202?queryString=On%20the%20Usability%20and%20Likeability%20of%20Virtual%20Reality%20Games%20for%20Education&databaseList=&clusterResults=true&groupVariantRecords=false&newsArticles=off&bookReviews=off

[^SDB2017P]: Shelstad, William J., Dustin C. Smith, and B. Chaparro. "Gaming on the Rift: How Virtual Reality Affects Game User Satisfaction." *Proceedings of the Human Factors and Ergonomics Society Annual Meeting* 61, no. 1 (2017): 2072-2076. https://concordiauniversity.on.worldcat.org/search/detail/7164089971?queryString=Gaming%20on%20the%20Rift%3A%20How%20Virtual%20Reality%20Affects%20Game%20User%20Satisfaction&bookReviews=off&newsArticles=off&databaseList=&clusterResults=true&groupVariantRecords=false

[^kj2023Business]: Kostyk, Alena, Jie Sheng. "VR in Customer-Centered Marketing: Purpose-Driven Design." *Business Horizons* 66, no. 3 (2023): 225-236. https://concordiauniversity.on.worldcat.org/search/detail/9771306204?queryString=VR%20business&bookReviews=off&newsArticles=off&databaseList=&clusterResults=true&groupVariantRecords=false&format=Artchap&subformat=Artchap%3A%3Aartchap_artcl&subformat=Artchap%3A%3Aartchap_chptr&subformat=Artchap%3A%3Aartchap_digital&changedFacet=format

[^viewhub2021Youtube]: "Screenshot from 'Math Teacher's Virtual Reality Class In Half-Life Alyx,' YouTube, published October 6, 2021, https://www.youtube.com/watch?v=R3g9jrqjOZs"

[^valve2019Youtube]: "Screenshot from 'Half-Life: Alyx Announcement Trailer' Youtube, published November 21, 2019, https://www.youtube.com/watch?v=O2W0N3uKXmo"
## Virtuality

Virtuality refers to how we live, work, and interact through digital technology, like the internet, video games, social media, and other online platforms. Today, many parts of our lives such as work, relationships, entertainment, and even self-care happen online. This virtual world is shaped by different technologies, such as gaming software, social networks, and online communication tools. These technologies create new ways for people to interact and experience life. For example, in video games like World of Warcraft, players connect through various digital tools like blogs, forums, and chat rooms, making the game more than just the software itself [^nardi15virtuality]. Virtuality also explores how these technologies impact society, including how they change activism, work, and entertainment; emphasizing the role of digital technology in shaping how we work, connect, and have fun.

Virtual worlds like online games and platforms are spaces where artists experiment with creating and showcasing art in digital environments. These spaces let artists explore new ways of interacting with their audience and blending physical and virtual worlds. Over time, virtual art has been presented in galleries and exhibitions, encouraging collaboration and new forms of expression [^doyle15art]. Artists use these virtual spaces to reimagine art, create immersive experiences, and explore social engagement in unique ways that wouldn't be possible in the physical world.

Virtuality is the idea that something can have an effect or be experienced even if it isn't physically real. For instance, in a meeting where most committee members aren't physically there, the meeting can still happen because everyone is "virtually" present, maybe through video calls or other digital tools. Even though the people aren't in the same room, their presence is still felt, and the meeting can go on as planned [^norton72what]. Virtuality is powerful because it can fill in for what’s missing in the real world and still allow things to function as if they were real. It helps us continue with important activities, even if the real-world conditions aren't perfect.

![Virtuality](images/virtuality-sherwin.png)

[^doyle15art]: Doyle, Denise. 2015. “Art, Virtual Worlds and the Emergent Imagination.” *Leonardo* 48 (3): 244–50. http://www.jstor.org/stable/43832994.

[^nardi15virtuality]: Nardi, Bonnie. 2015. “Virtuality.” *Annual Review of Anthropology* 44: 15–31. http://www.jstor.org/stable/24811646.

[^norton72what]: Norton, Richard. 1972. “What Is Virtuality?” *The Journal of Aesthetics and Art Criticism* 30 (4): 499–505. https://doi.org/10.2307/429465.

## Virtualization

Virtualization is the process of creating a software-based representation of physical computing resources, enabling multiple virtual systems to operate simultaneously on a single hardware device. This technology essentially decouples hardware from software, allowing resources to be abstracted, shared, and allocated more efficiently[^1].

Virtualization is significant in new media because it restructures how computing infrastructure is managed and utilized. By separating software from hardware dependencies, virtualization enables flexibility in terms of resource allocation and system management. This abstraction layer transforms computing resources into scalable, portable utilities that can be provisioned on demand[^2].

In current media practices, virtualization serves as the foundational technology underlying cloud computing, allowing for dynamic scaling of resources across data centers. It enables content delivery networks, streaming services, and digital media platforms to efficiently distribute computational resources according to demand. Virtualization also facilitates software development by allowing programmers to test applications across multiple operating systems simultaneously[^3].

Some real world examples include desktop virtualization (enabling remote access to personalized workspaces), and network virtualization (creating segmented virtual networks on shared physical infrastructure). These implementations demonstrate how virtualization optimizes resources while improving reliability and availability[^1].

The concept of virtualization dates back to the 1960s when IBM developed CP/CMS (Control Program/Cambridge Monitor System), which evolved into VM/370, allowing multiple users to run seemingly isolated systems on shared mainframe hardware. However, the modern resurgence of virtualization is largely attributed to VMware's founding in 1998, which popularized virtualization for commodity x86 hardware[^2].

[^1]: Douglis, F., and Krieger, O. 2013. "Virtualization." IEEE Internet Computing 17 (2): 6-9. https://doi.org/10.1109/MIC.2013.27
[^2]: Campbell, S., and Jeronimo, M. 2006. "An Introduction to Virtualization." Intel. https://www.intel.com/intelpress/sum_vpio.htm
[^3]: Pearce, M., Zeadally, S., and Hunt, R. 2013. "Virtualization: Issues, Security Threats, and Solutions." ACM Computing Surveys 45 (2): 17:1-17:39. https://doi.org/10.1145/2431211.2431216

## Wearable Technology

Wearable technology refers to electronic devices designed to be worn on the user's body. These technological devices are typically integrated into jewelry, watches, accessories, clothing or medical devices. More advanced examples of wearable technology include Google Glasses, VR headsets and Microsoft's HoloLens, while more common everyday examples include hearing aids, smartwatches and bluetooth earphones [^techtarget]. The wearability of these devices allows for a more efficient method of collecting and tracking user data. These devices are seamlessly integrated into the users daily lives, making data such as sleep, health, movement, location and interaction conveniently trackable [^ximeiqu].

Wearable technology is particularly integral to new media by integrating artificial intelligence and internet connectivity into daily life, and thus contributing to the broader concept of smart living. Technologies such as the Google Glasses allow for users to search the internet or reply to emails through voice command, and through tracking your daily routine, the glasses can notify you of potential inconveniences such as a late train [^ximeiqu]. Wearable Technology's contribution to smart health is also crucial to the development and implementation of smart cities, which concerns the optimization of several different areas of daily living, such as health [^lydiaizu]. However, this collection of personal data raises ethical concerns in relation to new media, as large corporations gain access to personal user information and data, raising questions pertaining to hyper-surveillance.

[^techtarget]: Yasar, Kinza, and Ivy Wigmore. "Wearable Technology." *TechTarget*. November 2023. https://www.techtarget.com/searchmobilecomputing/definition/wearable-technology.

[^ximeiqu]: Qu, Ximei, Jiahuan Wang, and Rong Miao. "Application of Wearable Technology in Education." *Open Access Library Journal* 8, no. 11 (2021): 1-11. https://doi.org/10.4236/oalib.1107630.

[^lydiaizu]: Izu, Lydia, Brenda Scholtz, and Ifeoluwapo Fashoro. "Wearables and Their Potential to Transform Health Management: A Step towards Sustainable Development Goal 3." *Sustainability* 16, no. 5 (2024): 1850. https://doi.org/10.3390/su16051850.


## Wiki


Wikis are web-based, collaborative software that enables people to modify content by directly editing pages online.[^ebersbach05wiki] In educational contexts, they facilitate student engagement in a collaborative learning environment.[^parker07wiki]

The term "Wiki Wiki Web" was inspired by the "Wiki Wiki Shuttle" at Honolulu International Airport, as named by Ward Cunningham.[^anderson10wiki] In Hawaiian, "Wiki" means "quick" or "fast."

Wikis differ from traditional websites in that they are collaborative platforms that allow multiple users to create, edit, and organize content. Unlike traditional websites, wikis enable easy navigation between pages through interlinking and often have features such as revision history and discussion pages to track changes and facilitate communication among users. Wikis are commonly used for knowledge management, project collaboration, and intranet applications, and they can be utilized to display information, store corporate records, or serve as knowledge bases for various subjects. While traditional websites are typically front-end oriented, wikis act as a hybrid between an information system and an information displayer, allowing for quick and easy content creation and updates by users with minimal technical skills.[^hester09analysis]


There are numerous platforms for running wikis. Among the most popular are  [MediaWiki](https://www.mediawiki.org/), [DokuWiki](https://www.dokuwiki.org/dokuwiki), and [GitHub](../main/glossary.md#GitHub).

[^anderson10wiki]: Anderson, Melanie O., and Jon R. Serra. 2010. “Is that a Wiki in Your Classroom?.” In *Proceedings of the 2010 Association of Small Computers Users in Education (ASCUE) Conference*.

[^ebersbach05wiki]: Ebersbach, Anja, Markus Glaser, and Richard Heigl. 2005. “The Wiki Concept.” In *Wiki - Web Collaboration*, 1-24. Berlin: Springer.

[^parker07wiki]: Parker, Kevin, and Joseph Chao. 2007. “Wiki as a Teaching Tool.” *Interdisciplinary Journal of E-Learning and Learning Objects* 3 (1): 57–72.

[^hester09analysis]: Hester, Andrea J. 2009. Analysis of factors influencing adoption and usage of knowledge management systems and investigation of wiki technology as an innovative alternative to traditional systems. PhD diss., University of Colorado at Denver.

## XML

**XML (Extensible Markup Language)** is a flexible, text-based format for storing and transporting data. It uses a set of rules for encoding documents in a human-readable and machine-readable format.

The **World Wide Web Consortium (W3C)** developed XML in the late 1990s to address the need for a universal data format on the Internet. It was officially released in 1998 as a simplified version of **SGML (Standard Generalized Markup Language)**, aiming to make data more portable across platforms and applications. Over time, XML became integral for web services, configuration files, and data storage.

In the evolving landscape of new media, **XML (Extensible Markup Language)** plays a crucial role in efficiently structuring, organizing, and transmitting data. As digital communication and multimedia platforms expand, XML provides a standardized format that enables seamless integration across different systems, making it an essential tool for modern content management and distribution.

One of XML’s primary new media applications is **web content management**. Many content management systems (CMS) use XML to structure and store data, allowing easy updates and interoperability between different platforms. By providing a standardized format, XML ensures that digital content remains accessible and adaptable across various devices and screen sizes, making it easier to manage websites dynamically.

Another critical application is **RSS feeds**, which utilize XML to distribute news and updates in a structured format. Many websites, blogs, and news outlets rely on XML-based RSS feeds to deliver real-time content to users and aggregators, ensuring that information reaches audiences efficiently. This capability has transformed how people consume news and digital content by automating the process of content syndication.

XML is widely used in multimedia applications in addition to text-based media. SMIL (Synchronized Multimedia Integration Language) and SVG (Scalable Vector Graphics) leverage XML to create interactive and scalable media experiences. These formats allow developers to build animations, interactive graphics, and other rich media content while maintaining cross-platform compatibility.

Furthermore, XML is integral to data exchange in web services. Many APIs, including those that power social media, streaming platforms, and e-commerce sites, use XML to structure and transmit data between servers and clients. SOAP (Simple Object Access Protocol) and RESTful web services frequently employ XML to ensure seamless data communication across different systems.

XML is a foundational technology in new media, enabling structured content management, automated content distribution, rich multimedia experiences, and efficient data exchange. As digital media continues to evolve, XML remains a vital tool for ensuring interoperability, scalability, and accessibility in the ever-changing digital landscape.

![XML](images/XML.jpg)

[^vanderaalst25xml]: van der Aalst, Wil M.P. 2025. “Patterns and XPDL: A Critical Evaluation of the XML Process Definition Language.” _Department of Technology Management, Eindhoven University of Technology, The Netherlands_, February 2. PDF Document.  

[^feng24xml]: Feng, Dujuan. 2024. “New Media Advertising Information Search Method Based on XML Technology.” _International Journal of High Speed Electronics and Systems_, October 18, 2540033. https://doi.org/10.1142/S0129156425400336.  

[^wikipedia25xml]: Wikipedia. 2025. “XML.” _Wikipedia_, February 9, 2025. https://en.wikipedia.org/w/index.php?title=XML&oldid=1274745213.  





